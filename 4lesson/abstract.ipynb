{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59260837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "DATA = \".\\\\data\"\n",
    "punkt = re.compile(r\"[:.,;!?-]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91947480",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = None\n",
    "papers = []\n",
    "for root,dirs,files in os.walk(\".\\\\data\"):\n",
    "    paths = [os.path.join(DATA, flname) for flname in files]\n",
    "\n",
    "for path in paths:\n",
    "    with open(path,\"r\", encoding=\"utf-8\") as fh:\n",
    "        papers.extend(json.load(fh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9444ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts=[]\n",
    "for paper in papers: \n",
    "    paper1=paper['abstract']\n",
    "    abstracts.append(\n",
    "    { 'abstract': paper1.replace(\"\\n\",\" \").replace(\"Abstract:  \",\"\") ,\n",
    "      'len' : len(paper1)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f12cba10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'abstract': 'The design of novel devices with specific technical interests through modulating structural properties and bonding characteristics promotes the vigorous development of materials informatics. Herein, we propose a synergy strategy of component reconstruction by combining geometric configuration and bonding characteristics. With the synergy strategy, we designed a novel two-dimensional (2D) graphene-like borides, e.g. g-B3N5, which possesses counter-intuitive ultra-low thermal conductivity of 21.08 W/mK despite the small atomic mass. The ultra-low thermal conductivity is attributed to the synergy effect of electronics and geometry on thermal transport due to the combining reconstruction of g-BN and nitrogene. With the synergy effect, the dominant acoustic branches are strongly softened, and the scattering absorption and Umklapp process are simultaneously suppressed. Thus, the thermal conductivity is significantly lowered. To verify the component reconstruction strategy, we further constructed g-B3P5 and g-B3As5, and uncovered the ultra-low thermal conductivity of 2.50 and 1.85 W/mK, respectively. The synergy effect and the designed ultra-low thermal conductivity materials with lightweight atomic mass cater to the demand for light development of momentum machinery and heat protection, such as aerospace vehicles, high-speed rail, automobiles.',\n",
       "  'len': 1370},\n",
       " {'abstract': \"LiteBIRD, the Lite (Light) satellite for the study of B-mode polarization and Inflation from cosmic background Radiation Detection, is a space mission for primordial cosmology and fundamental physics. The Japan Aerospace Exploration Agency (JAXA) selected LiteBIRD in May 2019 as a strategic large-class (L-class) mission, with an expected launch in the late 2020s using JAXA's H3 rocket. LiteBIRD is planned to orbit the Sun-Earth Lagrangian point L2, where it will map the cosmic microwave background (CMB) polarization over the entire sky for three years, with three telescopes in 15 frequency bands between 34 and 448 GHz, to achieve an unprecedented total sensitivity of 2.2$\\\\mu$K-arcmin, with a typical angular resolution of 0.5$^\\\\circ$ at 100 GHz. The primary scientific objective of LiteBIRD is to search for the signal from cosmic inflation, either making a discovery or ruling out well-motivated inflationary models. The measurements of LiteBIRD will also provide us with insight into the quantum nature of gravity and other new physics beyond the standard models of particle physics and cosmology. We provide an overview of the LiteBIRD project, including scientific objectives, mission and system requirements, operation concept, spacecraft and payload module design, expected scientific outcomes, potential design extensions and synergies with other projects.\",\n",
       "  'len': 1383},\n",
       " {'abstract': 'Inverse problems exist in a wide variety of physical domains from aerospace engineering to medical imaging. The goal is to infer the underlying state from a set of observations. When the forward model that produced the observations is nonlinear and stochastic, solving the inverse problem is very challenging. Neural networks are an appealing solution for solving inverse problems as they can be trained from noisy data and once trained are computationally efficient to run. However, inverse model neural networks do not have guarantees of correctness built-in, which makes them unreliable for use in safety and accuracy-critical contexts. In this work we introduce a method for verifying the correctness of inverse model neural networks. Our approach is to overapproximate a nonlinear, stochastic forward model with piecewise linear constraints and encode both the overapproximate forward model and the neural network inverse model as a mixed-integer program. We demonstrate this verification procedure on a real-world airplane fuel gauge case study. The ability to verify and consequently trust inverse model neural networks allows their use in a wide variety of contexts, from aerospace to medicine.',\n",
       "  'len': 1213},\n",
       " {'abstract': 'Precipitate-matrix interactions govern the mechanical behavior of precipitate strengthened Al-based alloys. These alloys find a wide range of applications ranging from aerospace to automobile and naval industries due to their low cost and high strength to weight ratio. Structures made from Al-based alloys undergo complex loading conditions such as high strain rate impact, which involves high pressures. Here we use diamond anvil cells to study the behavior of Al-based Al7075 alloy under quasi-hydrostatic and non-hydrostatic pressure up to ~53 GPa. In situ X-ray diffraction (XRD) and pre- and post-compression transmission electron microscopy (TEM) imaging are used to analyze microstructural changes and estimate high pressure strength. We find a bulk modulus of 75.2 +- 1.9 GPa using quasi-hydrostatic pressure XRD measurements. XRD showed that non-hydrostatic pressure leads to a significant increase in defect density and peak broadening with pressure cycling. XRD mapping under non-hydrostatic pressure revealed that the region with the highest local pressure had the greatest increase in defect nucleation, whereas the region with the largest local pressure gradient underwent texturing and had larger grains. TEM analysis showed that pressure cycling led to the nucleation and growth of many precipitates. The significant increase in defect and precipitate density leads to an increase in strength for Al7075 alloy at high pressures.',\n",
       "  'len': 1456},\n",
       " {'abstract': 'Planetary rover systems need to perform terrain segmentation to identify drivable areas as well as identify specific types of soil for sample collection. The latest Martian terrain segmentation methods rely on supervised learning which is very data hungry and difficult to train where only a small number of labeled samples are available. Moreover, the semantic classes are defined differently for different applications (e.g., rover traversal vs. geological) and as a result the network has to be trained from scratch each time, which is an inefficient use of resources. This research proposes a semi-supervised learning framework for Mars terrain segmentation where a deep segmentation network trained in an unsupervised manner on unlabeled images is transferred to the task of terrain segmentation trained on few labeled images. The network incorporates a backbone module which is trained using a contrastive loss function and an output atrous convolution module which is trained using a pixel-wise cross-entropy loss function. Evaluation results using the metric of segmentation accuracy show that the proposed method with contrastive pretraining outperforms plain supervised learning by 2%-10%. Moreover, the proposed model is able to achieve a segmentation accuracy of 91.1% using only 161 training images (1% of the original dataset) compared to 81.9% with plain supervised learning.',\n",
       "  'len': 1401},\n",
       " {'abstract': \"The Extreme-ultraviolet Stellar Characterization for Atmospheric Physics and Evolution (ESCAPE) mission is an astrophysics Small Explorer employing ultraviolet spectroscopy (EUV: 80 - 825 Å and FUV: 1280 - 1650 Å) to explore the high-energy radiation environment in the habitable zones around nearby stars. ESCAPE provides the first comprehensive study of the stellar EUV and coronal mass ejection environments which directly impact the habitability of rocky exoplanets. In a 20 month science mission, ESCAPE will provide the essential stellar characterization to identify exoplanetary systems most conducive to habitability and provide a roadmap for NASA's future life-finder missions. ESCAPE accomplishes this goal with roughly two-order-of-magnitude gains in EUV efficiency over previous missions. ESCAPE employs a grazing incidence telescope that feeds an EUV and FUV spectrograph. The ESCAPE science instrument builds on previous ultraviolet and X-ray instrumentation, grazing incidence optical systems, and photon-counting ultraviolet detectors used on NASA astrophysics, heliophysics, and planetary science missions. The ESCAPE spacecraft bus is the versatile and high-heritage Ball Aerospace BCP Small spacecraft. Data archives will be housed at the Mikulski Archive for Space Telescopes (MAST).\",\n",
       "  'len': 1314},\n",
       " {'abstract': 'As one of the important methods for surface modification of materials and life extension of key components, cemented carbide brazing coatings are widely used in agricultural machinery, oil drilling, aerospace and other fields, it has also attracted the attention of scholars in the field of surface modification at home and abroad. Based on the research reports of recent 20 years at home and abroad, the present research situation of cemented carbide brazed coating additive manufacturing technology are reviewed firstly. The research progress in preparation and performance control of cemented carbide/iron-base, cemented carbide/copper-base, cemented carbide/nickel-base, cemented carbide/silver-base heterogeneous brazing coatings are reviewed in detail. Then the practical applications of cemented carbide heterogenous brazing coatings in the fields of contact soil agricultural machinery parts life extension, aviation parts repair, surface function strengthening and so on are reviewed. In this review, the limitation of the research of cemented carbide heterogenous braze coating technology is discussed. The deficiencies in the research and development of cemented carbide heterogenous braze coating technology are summarized including cemented carbide heterogeneous additive materials and technology need to be expanded, the structure of cemented carbide heterogeneous brazing coatings need to be optimized, the mechanism of interface defects in cemented carbide heterogeneous brazing coatings need to be clarified. Finally, the future development direction of braze coating technology is prospected, too.',\n",
       "  'len': 1626},\n",
       " {'abstract': 'This chapter focuses on the evolution of Human-Centered Design (HCD) in aerospace systems over the last forty years. Human Factors and Ergonomics first shifted from the study of physical and medical issues to cognitive issues circa the 1980s. The advent of computers brought with it the development of human-computer interaction (HCI), which then expanded into the field of digital interaction design and User Experience (UX). We ended up with the concept of interactive cockpits, not because pilots interacted with mechanical things, but because they interacted using pointing devices on computer displays. Since the early 2000s, complexity and organizational issues gained prominence to the point that complex systems design and management found itself center stage, with the spotlight on the role of the human element and organizational setups. Today, Human Systems Integration (HSI) is no longer only a single-agent problem, but a multi-agent research field. Systems are systems of systems, considered as representations of people and machines. They are made of statically and dynamically articulated structures and functions. When they are at work, they are living organisms that generate emerging functions and structures that need to be considered in evolution (i.e., in their constant redesign). This chapter will more specifically, focus on human factors such as human-centered systemic representations, life critical systems, organizational issues, complexity management, modeling and simulation, flexibility, tangibility and autonomy. The discussion will be based on several examples in civil aviation and air combat, as well as aerospace.',\n",
       "  'len': 1661},\n",
       " {'abstract': 'This study presents a policy optimisation framework for structured nonlinear control of continuous-time (deterministic) dynamic systems. The proposed approach prescribes a structure for the controller based on relevant scientific knowledge (such as Lyapunov stability theory or domain experiences) while considering the tunable elements inside the given structure as the point of parametrisation with neural networks. To optimise a cost represented as a function of the neural network weights, the proposed approach utilises the continuous-time policy gradient method based on adjoint sensitivity analysis as a means for correct and performant computation of cost gradient. This enables combining the stability, robustness, and physical interpretability of an analytically-derived structure for the feedback controller with the representational flexibility and optimised resulting performance provided by machine learning techniques. Such a hybrid paradigm for fixed-structure control synthesis is particularly useful for optimising adaptive nonlinear controllers to achieve improved performance in online operation, an area where the existing theory prevails the design of structure while lacking clear analytical understandings about tuning of the gains and the uncertainty model basis functions that govern the performance characteristics. Numerical experiments on aerospace applications illustrate the utility of the structured nonlinear controller optimisation framework.',\n",
       "  'len': 1487},\n",
       " {'abstract': \"Digital images contain a lot of redundancies, therefore, compressions are applied to reduce the image size without the loss of reasonable image quality. The same become more prominent in the case of videos that contains image sequences and higher compression ratios are achieved in low throughput networks. Assessment of the quality of images in such scenarios becomes of particular interest. Subjective evaluation in most of the scenarios becomes infeasible so objective evaluation is preferred. Among the three objective quality measures, full-reference and reduced-reference methods require an original image in some form to calculate the quality score which is not feasible in scenarios such as broadcasting or IP video. Therefore, a non-reference quality metric is proposed to assess the quality of digital images which calculates luminance and multiscale gradient statistics along with mean subtracted contrast normalized products as features to train a Feedforward Neural Network with Scaled Conjugate Gradient. The trained network has provided good regression and R2 measures and further testing on LIVE Image Quality Assessment database release-2 has shown promising results. Pearson, Kendall, and Spearman's correlation are calculated between predicted and actual quality scores and their results are comparable to the state-of-the-art systems. Moreover, the proposed metric is computationally faster than its counterparts and can be used for the quality assessment of image sequences.\",\n",
       "  'len': 1506},\n",
       " {'abstract': 'In this study, the effects of prepreg out time on the mechanical and fracture properties of Discontinuous Fiber Composites (DFCs) are investigated. Carbon fiber prepregs are aged at 0x, 1x, 2x, and 3x the out life in an environmental chamber at constant temperature and humidity. Degree of cure is measured via Differential Scanning Calorimetry (DSC) while tension, compression, and shear tests are performed to investigate the effects that aging has on these mechanical properties. For the first time, Mode I intra-laminar fracture and its size effect are also investigated by means of fracture tests on geometrically-scaled Single Edge Notch Tension (SENT) specimens. From the tension, compression, and shear experiments it is seen that the out time has no effect on the elastic moduli. However, the strength increases with increasing age of the specimens for all the loading conditions. The percent increase compared to the non-aged material ranges from 15% to 33%. This is likely caused by plasticization of the matrix with age, allowing for higher energy absorption. More complex trends are reported for the SENT specimens for all the sizes. It is found that the fracture energy and characteristic length initially decrease with age, and then finally increase for the longest out time. This trend is owed to two factors with countering effects on the fracture behavior: 1) the increase of the average number of platelets with increasing aging due to increase in resin viscosity, and 2) the plasticization of the matrix with aging. The results from this study suggest that Discontinuous Fiber Composites (DFCs) made from reused materials can have equal, if not better, performance than non-aged DFCs. The experimental data presented in this work can be used as a baseline to design DFC composite components made from repurposed prepreg scrap and waste.',\n",
       "  'len': 1867},\n",
       " {'abstract': 'Computer experiments can emulate the physical systems, help computational investigations, and yield analytic solutions. They have been widely employed with many engineering applications (e.g., aerospace, automotive, energy systems. Conventional Bayesian optimization did not incorporate the nested structures in computer experiments. This paper proposes a novel nested Bayesian optimization for complex computer experiments with multi-step or hierarchical characteristics. We prove the theoretical properties of nested outputs given two cases: Gaussian or non-Gaussian. The closed forms of nested expected improvement are derived. We also propose the computational algorithms for nested Bayesian optimization. Three numerical studies show that the proposed nested Bayesian optimization outperforms the five benchmark Bayesian optimization methods ignoring the intermediate outputs of the inner computer code. The case study shows that the nested Bayesian optimization can efficiently minimize the residual stress during composite structures assembly and avoid convergence to the local optimum.',\n",
       "  'len': 1104},\n",
       " {'abstract': 'We report a digital twin (DT) framework of electrical tomography (ET) to address the challenge of real-time quantitative multiphase flow imaging based on non-invasive and non-radioactive technologies. Multiphase flow is ubiquitous in nature, industry, and research. Accurate flow imaging is the key to understanding this complex phenomenon. Existing non-radioactive multiphase flow imaging methods based on electrical tomography are limited to providing qualitative images. The proposed DT framework, building upon a synergistic integration of 3D field coupling simulation, model-based deep learning, and edge computing, allows ET to dynamically learn the flow features in the virtual space and implement the model in the physical system, thus providing unprecedented resolution and accuracy. The DT framework is demonstrated on gas-liquid two-phase flow and electrical capacitance tomography (ECT). It can be readily extended to various tomography modalities, scenarios, and scales in biomedical, energy, and aerospace applications as an effective alternative to radioactive solutions for precise flow visualization and characterization.',\n",
       "  'len': 1149},\n",
       " {'abstract': 'Detecting anomalies in time-varying multivariate data is crucial in various industries for the predictive maintenance of equipment. Numerous machine learning (ML) algorithms have been proposed to support automated anomaly identification. However, a significant amount of human knowledge is still required to interpret, analyze, and calibrate the results of automated analysis. This paper investigates current practices used to detect and investigate anomalies in time series data in industrial contexts and identifies corresponding needs. Through iterative design and working with nine experts from two industry domains (aerospace and energy), we characterize six design elements required for a successful visualization system that supports effective detection, investigation, and annotation of time series anomalies. We summarize an ideal human-AI collaboration workflow that streamlines the process and supports efficient and collaborative analysis. We introduce MTV (Multivariate Time Series Visualization), a visual analytics system to support such workflow. The system incorporates a set of novel visualization and interaction designs to support multi-faceted time series exploration, efficient in-situ anomaly annotation, and insight communication. Two user studies, one with 6 spacecraft experts (with routine anomaly analysis tasks) and one with 25 general end-users (without such tasks), are conducted to demonstrate the effectiveness and usefulness of MTV.',\n",
       "  'len': 1477},\n",
       " {'abstract': 'An important issue during an engineering design process is to develop an understanding which design parameters have the most influence on the performance. Especially in the context of optimization approaches this knowledge is crucial in order to realize an efficient design process and achieve high-performing results. Information theory provides powerful tools to investigate these relationships because measures are model-free and thus also capture non-linear relationships, while requiring only minimal assumptions on the input data. We therefore propose to use recently introduced information-theoretic methods and estimation algorithms to find the most influential input parameters in optimization results. The proposed methods are in particular able to account for interactions between parameters, which are often neglected but may lead to redundant or synergistic contributions of multiple parameters. We demonstrate the application of these methods on optimization data from aerospace engineering, where we first identify the most relevant optimization parameters using a recently introduced information-theoretic feature-selection algorithm that accounts for interactions between parameters. Second, we use the novel partial information decomposition (PID) framework that allows to quantify redundant and synergistic contributions between selected parameters with respect to the optimization outcome to identify parameter interactions. We thus demonstrate the power of novel information-theoretic approaches in identifying relevant parameters in optimization runs and highlight how these methods avoid the selection of redundant parameters, while detecting interactions that result in synergistic contributions of multiple parameters.',\n",
       "  'len': 1754},\n",
       " {'abstract': 'Background: Each GECAM satellite payload contains 25 gamma-ray detectors (GRDs), which can detect gamma-rays and particles and can roughly localize the Gamma-Ray Bursts (GRBs). GRD was designed using lanthanum bromide (LaBr3) crystal as the sensitive material with the rear end coupled with silicon photomultiplier (SiPM) array for readout. Purpose: In aerospace engineering design of GRD, there are many key points to be studied. In this paper, we present the specific design scheme of GRD, the assembly and the performance test results of detectors. Methods: Based on Monte Carlo simulation and experimental test results, the specific schematic design and assembling process ofGRDwere optimized. After being fully assembled, theGRDswere conducted performance tests by using radioactive source and also conducted random vibration tests. Result and conclusion: The test results show that all satellite-borne GRDs have energy resolution <16% at 59.5 keV, meeting requirements of satellite in scientific performance. The random vibration test shows that GRD can maintain in a stable performance, which meets the requirement of spatial application.',\n",
       "  'len': 1156},\n",
       " {'abstract': \"[Context & motivation] Eliciting requirements that are detailed and logical enough to be amenable to formal verification is a difficult task. Multiple tools exist for requirements elicitation and some of these also support formalisation of requirements in a way that is useful for formal methods. [Question/problem] This paper reports on our experience of using the FRET alongside our industrial partner. The use case that we investigate is an aircraft engine controller. In this context, we evaluate the use of FRET to bridge the communication gap between formal methods experts and aerospace industry specialists. [Principal ideas/results] We describe our journey from ambiguous, natural-language requirements to concise, formalised FRET requirements. We include our analysis of the formalised requirements from the perspective of patterns, translation into other formal methods and the relationship between parent-child requirements in this set. We also provide insight into lessons learned throughout this process and identify future improvements to FRET. [Contribution] Previous experience reports have been published by the FRET team, but this is the first such report of an industrial use case that was written by researchers that have not been involved FRET's development.\",\n",
       "  'len': 1291},\n",
       " {'abstract': 'Gas turbine engines are complex machines that typically generate a vast amount of data, and require careful monitoring to allow for cost-effective preventative maintenance. In aerospace applications, returning all measured data to ground is prohibitively expensive, often causing useful, high value, data to be discarded. The ability to detect, prioritise, and return useful data in real-time is therefore vital. This paper proposes that system output measurements, described by a convolutional neural network model of normality, are prioritised in real-time for the attention of preventative maintenance decision makers. Due to the complexity of gas turbine engine time-varying behaviours, deriving accurate physical models is difficult, and often leads to models with low prediction accuracy and incompatibility with real-time execution. Data-driven modelling is a desirable alternative producing high accuracy, asset specific models without the need for derivation from first principles. We present a data-driven system for online detection and prioritisation of anomalous data. Biased data assessment deriving from novel operating conditions is avoided by uncertainty management integrated into the deep neural predictive model. Testing is performed on real and synthetic data, showing sensitivity to both real and synthetic faults. The system is capable of running in real-time on low-power embedded hardware and is currently in deployment on the Rolls-Royce Pearl 15 engine flight trials.',\n",
       "  'len': 1505},\n",
       " {'abstract': 'In recent years, increasing attentions are paid on object detection in remote sensing imagery. However, traditional optical detection is highly susceptible to illumination and weather anomaly. It is a challenge to effectively utilize the cross-modality information from multi-source remote sensing images, especially from optical and synthetic aperture radar images, to achieve all-day and all-weather detection with high accuracy and speed. Towards this end, a fast multi-source fusion detection framework is proposed in current paper. A novel distance-decay intersection over union is employed to encode the shape properties of the targets with scale invariance. Therefore, the same target in multi-source images can be paired accurately. Furthermore, the weighted Dempster-Shafer evidence theory is utilized to combine the optical and synthetic aperture radar detection, which overcomes the drawback in feature-level fusion that requires a large amount of paired data. In addition, the paired optical and synthetic aperture radar images for container ship Ever Given which ran aground in the Suez Canal are taken to demonstrate our fusion algorithm. To test the effectiveness of the proposed method, on self-built data set, the average precision of the proposed fusion detection framework outperform the optical detection by 20.13%.',\n",
       "  'len': 1346},\n",
       " {'abstract': 'This paper presents a technique for navigation of mobile robot with Deep Q-Network (DQN) combined with Gated Recurrent Unit (GRU). The DQN integrated with the GRU allows action skipping for improved navigation performance. This technique aims at efficient navigation of mobile robot such as autonomous parking robot. Framework for reinforcement learning can be applied to the DQN combined with the GRU in a real environment, which can be modeled by the Partially Observable Markov Decision Process (POMDP). By allowing action skipping, the ability of the DQN combined with the GRU in learning key-action can be improved. The proposed algorithm is applied to explore the feasibility of solution in real environment by the ROS-Gazebo simulator, and the simulation results show that the proposed algorithm achieves improved performance in navigation and collision avoidance as compared to the results obtained by DQN alone and DQN combined with GRU without allowing action skipping.',\n",
       "  'len': 990},\n",
       " {'abstract': 'Unmanned aerospace vehicles usually carry sensors (i.e., electro-optical and/or infrared imaging cameras) as their primary payload. These sensors are used for image processing, target tracking, surveillance, mapping, and providing high-resolution imagery for environmental surveys. It is crucial to obtain a steady image in all these applications. This is typically accomplished by using multi-axis gimbal systems. This paper concentrates on the modeling and control of a multi-axis gimbal system. A novel and fully outlined procedure is proposed to derive the nonlinear and highly coupled Equations of Motion of the two-axis gimbal system. Different from the existing literature, Forward Dynamics of the two-axis gimbal system is modeled using multi-body dynamics modeling techniques. In addition to the Forward Dynamics model, the Inverse Dynamics model is developed to estimate the complementary torques associated with the state and mechanism-dependent, complex disturbances acting on the system. A disturbance compensator based on multilayer perceptron (MLP) structure is implemented to cope with external and internal disturbances and parameter uncertainties through the torque input channel. Our initial simulations and experimental work show that the new NN (neural network)-based controller is performs better in the full operational range without requiring any tuning or adjustment when compared with well-known controllers such as cascaded PID, ADRC (Active Disturbance Rejection Control), Inverse Dynamics based controllers.',\n",
       "  'len': 1547},\n",
       " {'abstract': \"Polymer matrix composites exhibit remarkable lightweight and high strength properties that make them attractive for aerospace applications. Constituents' materials such as advanced polymers and fibers or fillers with their hierarchical structure embed these exceptional properties to the composite materials. This hierarchical structure in multiple length scales provides an opportunity for designing the composite materials for optimized properties. However, the high dimensional design space for the constituents' materials and architectures choice of the composites makes it a challenging design problem. To tackle this high dimensional design space, a systematic, efficient approach named mechanistic data science framework is proposed in this work to identify the governing mechanisms of materials systems from the limited available data and create a composite knowledge database. Our composite knowledge database comprises the knowledge of polymers at the nanoscale with nano reinforcement, the unidirectional structure at the microscale, and woven structure at mesoscale mechanisms that can be further used for the part scale composite design and analysis. The mechanistic data science framework presented in this work can be further extended to other materials systems that will provide the materials designer with the necessary tools to evaluate a new materials system design rapidly.\",\n",
       "  'len': 1404},\n",
       " {'abstract': 'The climate crisis we are facing calls for significant improvements in our understanding of natural phenomena, with clouds being identified as a dominant source of uncertainty. To this end, the emerging field of 3D computed cloud tomography (CCT) aims to more precisely characterize clouds by utilizing multi-dimensional imaging to reconstruct their outer and inner structure. In this paper, we propose a future Earth observation mission concept, driven by the needs of CCT, that operates constellation of NanoSats to provide multi-angular, spectrally-resolved, spatial and temporal scientific measurements of natural atmospheric phenomena. Our proposed mission, GEOSCAN, will on-board active steering capability to rapidly reconfigure networked swarm of autonomous Nanosats to track evolving phenomena of interest, on-demand, in real-time. We present the structure of the GEOSCAN constellation and discuss details of the mission concept from both science and engineering perspectives. On the science side, we outline the types of remote Earth observation measurements that GEOSCAN enables beyond the state-of-the-art, and how such measurements translate to improvements in CCT that can lead to reduction in uncertainty of the global climate models (GCMs). From the engineering side, we investigate feasibility of the concept starting from hardware components of the NanoSat that form the basis of the constellation. In particular, we focus on the active steering capability of the GEOSCAN with algorithmic approaches that enable coordination from new software. We identify technology gaps that need to be bridged and discuss other aspects of the mission that require in-depth analysis to further mature the concept.',\n",
       "  'len': 1727},\n",
       " {'abstract': 'Robotic and Autonomous Agricultural Technologies (RAAT) are increasingly available yet may fail to be adopted. This paper focusses specifically on cognitive factors that affect adoption including: inability to generate trust, loss of farming knowledge and reduced social cognition. It is recommended that agriculture develops its own framework for the performance and safety of RAAT drawing on human factors research in aerospace engineering including human inputs (individual variance in knowledge, skills, abilities, preferences, needs and traits), trust, situational awareness and cognitive load. The kinds of cognitive impacts depend on the RAATs level of autonomy, ie whether it has automatic, partial autonomy and autonomous functionality and stage of adoption, ie adoption, initial use or post-adoptive use. The more autonomous a system is, the less a human needs to know to operate it and the less the cognitive load, but it also means farmers have less situational awareness about on farm activities that in turn may affect strategic decision-making about their enterprise. Some cognitive factors may be hidden when RAAT is first adopted but play a greater role during prolonged or intense post-adoptive use. Systems with partial autonomy need intuitive user interfaces, engaging system information, and clear signaling to be trusted with low level tasks; and to compliment and augment high order decision-making on farm.',\n",
       "  'len': 1441},\n",
       " {'abstract': 'Onboard autonomy technologies such as planning and scheduling, identification of scientific targets, and content-based data summarization, will lead to exciting new space science missions. However, the challenge of operating missions with such onboard autonomous capabilities has not been studied to a level of detail sufficient for consideration in mission concepts. These autonomy capabilities will require changes to current operations processes, practices, and tools. We have developed a case study to assess the changes needed to enable operators and scientists to operate an autonomous spacecraft by facilitating a common model between the ground personnel and the onboard algorithms. We assess the new operations tools and workflows necessary to enable operators and scientists to convey their desired intent to the spacecraft, and to be able to reconstruct and explain the decisions made onboard and the state of the spacecraft. Mock-ups of these tools were used in a user study to understand the effectiveness of the processes and tools in enabling a shared framework of understanding, and in the ability of the operators and scientists to effectively achieve mission science objectives.',\n",
       "  'len': 1207},\n",
       " {'abstract': \"We introduce a self-attending task generative adversarial network (SATGAN) and apply it to the problem of augmenting synthetic high contrast scientific imagery of resident space objects with realistic noise patterns and sensor characteristics learned from collected data. Augmenting these synthetic data is challenging due to the highly localized nature of semantic content in the data that must be preserved. Real collected images are used to train a network what a given class of sensor's images should look like. The trained network then acts as a filter on noiseless context images and outputs realistic-looking fakes with semantic content unaltered. The architecture is inspired by conditional GANs but is modified to include a task network that preserves semantic information through augmentation. Additionally, the architecture is shown to reduce instances of hallucinatory objects or obfuscation of semantic content in context images representing space observation scenes.\",\n",
       "  'len': 991},\n",
       " {'abstract': 'Applications of free-flying robots range from entertainment purposes to aerospace applications. The control algorithm for such systems requires accurate estimation of their states based on sensor feedback. The objective of this paper is to design and verify a lightweight state estimation algorithm for a free-flying open kinematic chain that estimates the state of its center-of-mass and its posture. Instead of utilizing a nonlinear dynamics model, this research proposes a cascade structure of two Kalman filters (KF), which relies on the information from the ballistic motion of free-falling multibody systems together with feedback from an inertial measurement unit (IMU) and encoders. Multiple algorithms are verified in the simulation that mimics real-world circumstances with Simulink. Several uncertain physical parameters are varied, and the result shows that the proposed estimator outperforms EKF and UKF in terms of tracking performance and computational time.',\n",
       "  'len': 984},\n",
       " {'abstract': 'We investigate the acoustical properties of uncompressed and compressed open-celled aluminum metal foams fabricated using a directional solidification foaming process. We compressed the fabricated foams using a hydraulic press to different compression ratios and characterized the effect of compression on the cellular microstructure using microtomography and scanning electron microscopy. The static airflow resistances of the samples are measured and related to the observed microstructural changes. We measured the normal incidence acoustical properties using two- and four-microphone impedance tube methods and show that the compression substantially improves their sound absorption and transmission loss performance. We then stack individual disks with different compression ratios to create various stepwise relative density gradient configurations and show that stepwise gradients provide a significant improvement in properties as compared to the uncompressed sample. The effect of increasing and decreasing relative density gradients on the overall absorption and transmission loss behavior is characterized. Finally, we use an experimentally informed and validated transfer matrix method to predict the effect of various layer thicknesses and stacking sequences on the global acoustical properties. Our results show that open-celled metal foams with stepwise relative density gradients can be designed to provide tailored acoustic absorption performance while reducing the overall weight of the noise reduction package.',\n",
       "  'len': 1540},\n",
       " {'abstract': 'In an attempt to reduce jet noise, blended wing concept vehicle utilizes rectangular jet exhaust ports exiting from above the wing ahead of the trailing edge. In this study, we take another look at the rectangular exhaust port configuration with some notional modifications to the geometry of the trailing edge to determine if the emitted noise levels due to jet interactions can be reduced with respect to a baseline configuration. We consider various horizontal and vertical offsets of the jet exit with respect to a flat plate standing in for the aft wing surface. We then introduce a series of sinusoidal deformations to the trailing edge of the plate of varying amplitude and wave number. Our results show that the emitted sound levels due to the jet--surface interactions can be significantly altered by the proposed geometry modifications. While sound levels remained fairly consistent over many configurations, there were some that showed both increased and decreased sound levels in specific directions. We present results here for the simulated configurations which showed the greatest decrease in overall sound levels with respect to the baseline. These results provide strong indications that such geometry modifications can potentially be tailored to optimize for further reductions in sound levels.',\n",
       "  'len': 1323},\n",
       " {'abstract': \"This work aims to provide an engagement decision support tool for Beyond Visual Range (BVR) air combat in the context of Defensive Counter Air (DCA) missions. In BVR air combat, engagement decision refers to the choice of the moment the pilot engages a target by assuming an offensive stance and executing corresponding maneuvers. To model this decision, we use the Brazilian Air Force's Aerospace Simulation Environment (Ambiente de Simulação Aeroespacial - ASA in Portuguese), which generated 3,729 constructive simulations lasting 12 minutes each and a total of 10,316 engagements. We analyzed all samples by an operational metric called the DCA index, which represents, based on the experience of subject matter experts, the degree of success in this type of mission. This metric considers the distances of the aircraft of the same team and the opposite team, the point of Combat Air Patrol, and the number of missiles used. By defining the engagement status right before it starts and the average of the DCA index throughout the engagement, we create a supervised learning model to determine the quality of a new engagement. An algorithm based on decision trees, working with the XGBoost library, provides a regression model to predict the DCA index with a coefficient of determination close to 0.8 and a Root Mean Square Error of 0.05 that can furnish parameters to the BVR pilot to decide whether or not to engage. Thus, using data obtained through simulations, this work contributes by building a decision support system based on machine learning for BVR air combat.\",\n",
       "  'len': 1585},\n",
       " {'abstract': 'In-orbit satellite REIMEI, developed by the Japan Aerospace Exploration Agency, has been relying on off-the-shelf Li-ion batteries since its launch in 2005. The performance and durability of Li-ion batteries is impacted by various degradation mechanisms, one of which is the growth of the solid-electrolyte interphase (SEI). In this article, we analyse the REIMEI battery and parameterize a full-cell model with electrochemical cycling data, computer tomography images, and capacity fading experiments using image processing and surrogate optimization. We integrate a recent model for SEI growth into a full-cell model and simulate the degradation of batteries during cycling. To validate our model, we use experimental and in-flight data of the satellite batteries. Our combination of SEI growth model and microstructure-resolved 3D simulation shows, for the first time, experimentally observed inhomogeneities in the SEI thickness throughout the negative electrode for the degraded cells.',\n",
       "  'len': 1001},\n",
       " {'abstract': \"On 5th December 2020 at 17:28 UTC, the Japan Aerospace Exploration Agency's Hayabusa-2 sample return capsule came back to the Earth. It re-entered the atmosphere over South Australia, visible for 53 seconds as a fireball from near the Northern Territory border toward Woomera where it landed in the the Woomera military test range. A scientific observation campaign was planned to observe the optical, seismo-acoustic, radio and high energy particle phenomena associated with the entry of an interplanetary object. A multi-institutional collaboration between Australian and Japanese universities resulted in the deployment of 49 instruments, with a further 13 permanent observation sites. The campaign successfully recorded optical, seismo-acoustic and spectral data for this event which will allow an in depth analysis of the effects produced by interplanetary objects impacting the Earth's atmosphere. This will allow future comparison and insights to be made with natural meteoroid objects.\",\n",
       "  'len': 1004},\n",
       " {'abstract': 'The Stretched-FrOnt-Leg (SFOL) pulse is a high-accuracy distance measuring equipment (DME) pulse developed to support alternative positioning and navigation for aircraft during global navigation satellite system outages. To facilitate the use of the SFOL pulse, it is best to use legacy DMEs that are already deployed to transmit the SFOL pulse, rather than the current Gaussian pulse, through software changes only. When attempting to transmit the SFOL pulse in legacy DMEs, the greatest challenge is the pulse shape distortion caused by the pulse-shaping circuits and power amplifiers in the transmission unit such that the original SFOL pulse shape is no longer preserved. This letter proposes an inverse-learning-based DME digital predistortion method and presents successfully transmitted SFOL pulses from a testbed based on a commercial legacy DME that was designed to transmit Gaussian pulses.',\n",
       "  'len': 911},\n",
       " {'abstract': \"Verification of complex, safety-critical systems is a significant challenge. Manual testing and simulations are often used, but are only capable of exploring a subset of the system's reachable states. Formal methods are mathematically-based techniques for the specification and development of software, which can provide proofs of properties and exhaustive checks over a system's state space. In this paper, we present a formal requirements-driven methodology, applied to a model of an aircraft engine controller that has been provided by our industrial partner. Our methodology begins by formalising the controller's natural-language requirements using the (pre-existing) Formal Requirements Elicitation Tool (FRET), iteratively, in consultation with our industry partner. Once formalised, FRET can automatically translate the requirements to enable their verification alongside a Simulink model of the aircraft engine controller; the requirements can also guide formal verification using other approaches. These two parallel streams in our methodology seek to combine the results from formal requirements elicitation, classical verification approaches, and runtime verification; to support the verification of aerospace systems modelled in Simulink, from the requirements phase through to execution. Our methodology harnesses the power of formal methods in a way that complements existing verification techniques, and supports the traceability of requirements throughout the verification process. This methodology streamlines the process of developing verifiable aircraft engine controllers, by ensuring that the requirements are formalised up-front and useable during development. In this paper we give an overview of (FRET), describe our methodology and work to-date on the formalisation and verification of the requirements, and outline future work using our methodology.\",\n",
       "  'len': 1887},\n",
       " {'abstract': 'This work presents a concept for the localisation of Lamb waves using a Passive Phased Array (PPA). A Warped Frequency Transformation (WFT) is applied to the acquired signals using numerically determined phase velocity information to compensate for signal dispersion. Whilst powerful, uncertainty between material properties cannot completely remove dispersion and hence the close intra-element spacing of the array is leveraged to allow for the assumption that each acquired signal is a scaled, translated, and noised copy of its adjacent counterparts. Following this, a recursive signal-averaging method using artificial time-locking to denoise the acquired signals by assuming the presence of non-correlated, zero mean noise is applied. Unlike the application of bandpass filters, the signal-averaging method does not remove potentially useful frequency components. The proposed methodology is compared against a bandpass filtered approach through a parametric study. A further discussion is made regarding applications and future developments of this technique.',\n",
       "  'len': 1076},\n",
       " {'abstract': 'Hypersonic flight, generally defined as the region in which the speed of a vehicle exceeds Mach 5 and in which thermal loads become dominant, has seen more attention over the past several decades due to the potential military applications of such vehicles. Vehicles capable of flight at these speeds, whilst seemingly a novel prospect, have been in development for more than 70 years. The nature of flight in this environment as well as the new challenges it introduces have led to relatively high failure rates. This paper presents a review of hypersonic vehicle flights that have resulted in failure from conception to modern day with the purpose of identifying trends to aid and guide the development of future vehicles. The collected data is used to formulate a failure taxonomy to accurately identify and classify past hypersonic vehicle failures as well as potential future ones. The trends and features of the categorical data collected are explored using an ex-post facto study.',\n",
       "  'len': 997},\n",
       " {'abstract': 'Porous carbon ablators offer cost-effective thermal protection for aerospace vehicles during re-entry into planetary atmospheres. However, the exploration of more distant planets requires the development of ablators that are able to withstand stronger thermal radiation conditions. Here, we report the development of bio-inspired porous carbon insulators with pore sizes that are deliberately tuned to enhance heat-shielding performance by increasing scattering of high-temperature thermal radiation. Pore size intervals that promote scattering are first estimated using an established model for the radiative contribution to the thermal conductivity of porous insulators. On the basis of this theoretical analysis, we identify a polymer additive that enables the formation of pores in the desired size range through the polymerization-induced phase separation of a mixture of phenolic resin and ethylene glycol. Optical and electron microscopy, porosimetry and mechanical tests are used to characterize the structure and properties of porous insulators prepared with different resin formulations. Insulators with pore sizes in the optimal scattering range reduce laser-induced damage of the porous structures by up to 42%, thus offering a promising and simple route for the fabrication of carbon ablators for enhanced thermal protection at high temperatures.',\n",
       "  'len': 1370},\n",
       " {'abstract': 'As one of the main governing equations in kinetic theory, the Boltzmann equation is widely utilized in aerospace, microscopic flow, etc. Its high-resolution simulation is crucial in these related areas. However, due to the high dimensionality of the Boltzmann equation, high-resolution simulations are often difficult to achieve numerically. The moment method which Grad first proposed in 1949 [12] is among popular numerical methods to achieve efficient high-resolution simulations. We can derive the governing equations in the moment method by taking moments on both sides of the Boltzmann equation, which effectively reduces the dimensionality of the problem. However, one of the main challenges is that it leads to an unclosed moment system, and closure is needed to obtain a closed moment system. It is truly an art in designing closures for moment systems and has been a significant research field in kinetic theory. Other than the traditional human designs of closures, the machine learning-based approach has attracted much attention lately [13, 19]. In this work, we propose a machine learning-based method to derive a moment closure model for the Boltzmann-BGK equation. In particular, the closure relation is approximated by a carefully designed deep neural network that possesses desirable physical invariances, i.e., the Galilean invariance, reflecting invariance, and scaling invariance, inherited from the original Boltzmann-BGK equation and playing an important role in the correct simulation of the Boltzmann equation. Numerical simulations on the smooth and discontinuous initial condition problems, Sod shock tube problem, and the shock structure problems demonstrate satisfactory numerical performances of the proposed invariance preserving neural closure method.',\n",
       "  'len': 1794},\n",
       " {'abstract': \"Recently, aerodynamics syllabi have changed in high schools, pilot ground training, and even undergraduate physics. In contrast, there has been no change in the basic theory taught to aeronautical or aerospace engineers. What has changed is technology, both experimentally and computationally. The internet and social media have also empowered citizen science such that the deficiencies in the legacy physics education around flight and lift are well known. The long-standing equal transit time (ETT) theory to explain lift has been proven false. If incorrect, why was it ever taught? Through a historical analysis of relevant fluid and aerodynamics literature, this study attempts to explain why ETT theory is part of our collectively lower-level cognitive understanding of lift and flight. It was found that in 1744 D'Alembert himself assumed this to be a feature of moving fluids, and while this initial intuition (ETT 1.0) was incorrect, the property of ETT (ETT 2.0) was derived in 1752 when applying Newton's laws of motion to fluids. This incorrect result was independently confirmed in 1757 by Euler! The conclusion is that an over simplified treatment of fluids predicts ETT, along with no lift and drag. This then leads to the open question, can ETT be taught at an appropriately low level as an explanation for lift?\",\n",
       "  'len': 1338},\n",
       " {'abstract': 'The knowledge that an intelligent and autonomous mobile robot has and is able to acquire of itself and the environment, namely the situation, limits its reasoning, decision-making, and execution skills to efficiently and safely perform complex missions. Situational awareness is a basic capability of humans that has been deeply studied in fields like Psychology, Military, Aerospace, Education, etc., but it has barely been considered in robotics, which has focused on ideas such as sensing, perception, sensor fusion, state estimation, localization and mapping, spatial AI, etc. In our research, we connected the broad multidisciplinary existing knowledge on situational awareness with its counterpart in mobile robotics. In this paper, we survey the state-of-the-art robotics algorithms, we analyze the situational awareness aspects that have been covered by them, and we discuss their missing points. We found out that the existing robotics algorithms are still missing manifold important aspects of situational awareness. As a consequence, we conclude that these missing features are limiting the performance of robotic situational awareness, and further research is needed to overcome this challenge. We see this as an opportunity, and provide our vision for future research on robotic situational awareness.',\n",
       "  'len': 1325},\n",
       " {'abstract': 'This paper is devoted to experimental characterisation of linear thermal expansion coefficient (LTEC) and mechanical characteristics of the laser deposited Cu-Fe system multilayer functionally graded (FG) structures and binary Cu-Fe alloys, fabricated from the tin, aluminium, and chromium bronze with 89-99 wt.% of copper and stainless steel (SS) AISI 316L with 1:1 and 3:1 bronze-to-steel ratio. The best tensile mechanical strength of as-built parts is demonstrated by the aluminium bronze-stainless steel 1:1 alloy and reaches 876.4 MPa along with low elasticity modulus (11.2 GPa) and 1.684 1/K LTEC. Contrarily, the worst values of the mechanical characteristics are exhibited by parts created from the chromium bronze and SS, which failed at 294.0-463.3 MPa ultimate stress, showed the highest elasticity modulus (up to 42.4 GPa) and comparatively high average LTEC (up to 1.878 1/K). The aluminium bronze-stainless steel binary and FG alloys are discussed in the light of prospective application as the part of gradient materials, created by additive manufacturing (AM) technologies via the gradient path method and the alternating layers technique, with expected possibility of application in aerospace, nuclear, and electronic industry due to advantageous combination of the antifrictionality, heat conductivity, and oxidation resistance of the bronze, and the high mechanical strength, corrosion and creep resistance of the stainless steel.',\n",
       "  'len': 1462},\n",
       " {'abstract': \"Plans for establishing a long-term human presence on the Moon will require substantial increases in robot autonomy and multi-robot coordination to support establishing a lunar outpost. To achieve these objectives, algorithm design choices for the software developments need to be tested and validated for expected scenarios such as autonomous in-situ resource utilization (ISRU), localization in challenging environments, and multi-robot coordination. However, real-world experiments are extremely challenging and limited for extraterrestrial environment. Also, realistic simulation demonstrations in these environments are still rare and demanded for initial algorithm testing capabilities. To help some of these needs, the NASA Centennial Challenges program established the Space Robotics Challenge Phase 2 (SRC2) which consist of virtual robotic systems in a realistic lunar simulation environment, where a group of mobile robots were tasked with reporting volatile locations within a global map, excavating and transporting these resources, and detecting and localizing a target of interest. The main goal of this article is to share our team's experiences on the design trade-offs to perform autonomous robotic operations in a virtual lunar environment and to share strategies to complete the mission requirements posed by NASA SRC2 competition during the qualification round. Of the 114 teams that registered for participation in the NASA SRC2, team Mountaineers finished as one of only six teams to receive the top qualification round prize.\",\n",
       "  'len': 1559},\n",
       " {'abstract': 'The vulnerabilities of global navigation satellite systems (GNSSs) to radio frequency jamming and spoofing have attracted significant research attention. In particular, the large-scale jamming incidents that occurred in South Korea substantiate the practical importance of implementing a complementary navigation system. This letter briefly summarizes the efforts of South Korea to deploy an enhanced long-range navigation (eLoran) system, which is a terrestrial low-frequency radio navigation system that can complement GNSSs. After four years of research and development, the Korean eLoran testbed system has been recently deployed and is operational since June 1, 2021. Although its initial performance at sea is satisfactory, navigation through a narrow waterway is still challenging because a complete survey of the additional secondary factor (ASF), which is the largest source of error for eLoran, is practically difficult in a narrow waterway. This letter proposes an alternative way to survey the ASF in a narrow waterway and improve the ASF map generation methods. Moreover, the performance of the proposed approach was validated experimentally.',\n",
       "  'len': 1166},\n",
       " {'abstract': 'Reinforcement Learning (RL) is emerging as tool for tackling complex control and decision-making problems. However, in high-risk environments such as healthcare, manufacturing, automotive or aerospace, it is often challenging to bridge the gap between an apparently optimal policy learnt by an agent and its real-world deployment, due to the uncertainties and risk associated with it. Broadly speaking RL agents face two kinds of uncertainty, 1. aleatoric uncertainty, which reflects randomness or noise in the dynamics of the world, and 2. epistemic uncertainty, which reflects the bounded knowledge of the agent due to model limitations and finite amount of information/data the agent has acquired about the world. These two types of uncertainty carry fundamentally different implications for the evaluation of performance and the level of risk or trust. Yet these aleatoric and epistemic uncertainties are generally confounded as standard and even distributional RL is agnostic to this difference. Here we propose how a distributional approach (UA-DQN) can be recast to render uncertainties by decomposing the net effects of each uncertainty. We demonstrate the operation of this method in grid world examples to build intuition and then show a proof of concept application for an RL agent operating as a clinical decision support system in critical care',\n",
       "  'len': 1368},\n",
       " {'abstract': \"Gravitational wave (GW) detection in space probes GW spectrum that is inaccessible from the Earth. In addition to LISA project led by European Space Agency, and the DECIGO detector proposed by the Japan Aerospace Exploration Agency, two Chinese space-based GW observatories -- TianQin and Taiji -- are planned to be launched in the 2030s. TianQin has a unique concept in its design with a geocentric orbit. Taiji's design is similar to LISA, but is more ambitious with longer arm distance. Both facilities are complementary to LISA, considering that TianQin is sensitive to higher frequencies and Taiji probes similar frequencies but with higher sensitivity. In this Perspective we explain the concepts for both facilities and introduce the development milestones of TianQin and Taiji projects in testing extraordinary technologies to pave the way for future space-based GW detections. Considering that LISA, TianQin and Taiji have similar scientific goals, all are scheduled to be launched around the 2030s and will operate concurrently, we discuss possible collaborations among them to improve GW source localization and characterization.\",\n",
       "  'len': 1151},\n",
       " {'abstract': 'Time-Sensitive Networking (TSN) extends IEEE 802.1 Ethernet for safety-critical and real-time applications in several areas, e.g., automotive, aerospace or industrial automation. However, many of these systems also have stringent security requirements, and security attacks may impair safety. Given a TSN-based distributed architecture, a set of applications with tasks and messages, as well as a set of security and redundancy requirements, we are interested to synthesize a system configuration such that the real-time, safety and security requirements are upheld. We use the Timed Efficient Stream Loss-Tolerant Authentication (TESLA) low-resource multicast authentication protocol to guarantee the security requirements, and redundant disjunct message routes to tolerate link failures. We consider that tasks are dispatched using a static cyclic schedule table and that the messages use the time-sensitive traffic class in TSN, which relies on schedule tables (called Gate Control Lists, GCLs) in the network switches. A configuration consists of the schedule tables for tasks as well as the disjoint routes and GCLs for messages. We propose a Constraint Programming-based formulation which can be used to find an optimal solution with respect to our cost function. Additionally, we propose a Simulated Annealing based metaheuristic, which can find good solution for large test cases. We evaluate both approaches on several test cases.',\n",
       "  'len': 1450},\n",
       " {'abstract': \"We present a framework for bi-level trajectory optimization in which a system's dynamics are encoded as the solution to a constrained optimization problem and smooth gradients of this lower-level problem are passed to an upper-level trajectory optimizer. This optimization-based dynamics representation enables constraint handling, additional variables, and non-smooth behavior to be abstracted away from the upper-level optimizer, and allows classical unconstrained optimizers to synthesize trajectories for more complex systems. We provide a path-following method for efficient evaluation of constrained dynamics and utilize the implicit-function theorem to compute smooth gradients of this representation. We demonstrate the framework by modeling systems from locomotion, aerospace, and manipulation domains including: acrobot with joint limits, cart-pole subject to Coulomb friction, Raibert hopper, rocket landing with thrust limits, and planar-push task with optimization-based dynamics and then optimize trajectories using iterative LQR.\",\n",
       "  'len': 1055},\n",
       " {'abstract': 'Epoxy resins are thermosetting polymers with an extensive set of applications such as anticorrosive coatings, adhesives, matrices for fibre reinforced composites and elements of electronic systems for automotive, aerospace and construction industries.',\n",
       "  'len': 262},\n",
       " {'abstract': 'In recent decades, alternative propulsion systems have been investigated and have attracted great interest in the space community. Most of these alternative propulsion systems need propellants. One alternative propulsion system that does not require propellants and is not often discussed is electrodynamic tethers (EDTs). This paper speculates the potential applications of EDTs ranging from radiation belt remediation to space station re-boosting.',\n",
       "  'len': 460},\n",
       " {'abstract': \"Thermoset resin-based composite materials are widely used in the aerospace industry, mainly due to their high stiffness-to-weight and strength-to-weight ratios. A major issue with the use of thermoset resins in fiber composites is the process-induced residual stresses that are formed from resin chemical shrinkage during the curing process. These residual stresses within the composite material ultimately result in reduced durability and residual deformations of the final product. Polybenzoxazine (PBZ) polymer resins have demonstrated near-zero volumetric shrinkage during the curing process. Although the low shrinkage of PBZ is promising in terms of reduced process-induced residual stresses, little is known about the physical causes. In this work, Molecular Dynamics (MD) simulations are performed with a reactive force field to predict physical properties (gelation point, evolution of network, mass density, volumetric shrinkage) and mechanical properties (Bulk modulus, Shear modulus, Young's modulus, Poisson's ratio, Yield strength) as a function of crosslinking density. The MD modeling procedure is validated herein using experimental measurements of the modeled PBZ resin. The results of this study are used to provide a physical understanding of the zero-shrinkage phenomenon of PBZ. This information is also a critical input to future process modeling efforts for PBZ composites.\",\n",
       "  'len': 1408},\n",
       " {'abstract': 'In-field test of processor-based devices is a must when considering safety-critical systems (e.g., in robotics, aerospace, and automotive applications). During in-field testing, different solutions can be adopted, depending on the specific constraints of each scenario. In the last years, Self-Test Libraries (STLs) developed by IP or semiconductor companies became widely adopted. Given the strict constraints of in-field test, the size and time duration of a STL is a crucial parameter. This work introduces a novel approach to compress functional test programs belonging to an STL. The proposed approach is based on analyzing (via logic simulation) the interaction between the micro-architectural operation performed by each instruction and its capacity to propagate fault effects on any observable output, reducing the required fault simulations to only one. The proposed compaction strategy was validated by resorting to a RISC-V processor and several test programs stemming from diverse generation strategies. Results showed that the proposed compaction approach can reduce the length of test programs by up to 93.9% and their duration by up to 95%, with minimal effect on fault coverage.',\n",
       "  'len': 1205},\n",
       " {'abstract': 'The Gravitational wave high-energy Electromagnetic Counterpart All-sky Monitor (GECAM) satellite consists of two small satellites. Each GECAM payload contains 25 gamma ray detectors (GRD) and 8 charged particle detectors (CPD). GRD is the main detector which can detect gamma-rays and particles and localize the Gamma-Ray Bursts (GRB),while CPD is used to help GRD to discriminate gamma-ray bursts and charged particle bursts. The GRD makes use of lanthanum bromide (LaBr3) crystal readout by SiPM. As the all available SiPM devices belong to commercial grade, quality assurance tests need to be performed in accordance with the aerospace specifications. In this paper, we present the results of quality assurance tests, especially a detailed mechanism analysis of failed devices during the development of GECAM. This paper also summarizes the application experience of commercial-grade SiPM devices in aerospace payloads, and provides suggestions for forthcoming SiPM space applications.',\n",
       "  'len': 999},\n",
       " {'abstract': \"Active structures have the ability to change their shape, properties, and functionality as a response to changing operational conditions, which makes them more versatile than their static counterparts. However, most active structures currently lack the capability to achieve multiple, different target states with a single input actuation or require a tedious material programming step. Furthermore, the systematic design and fabrication of active structures is still a challenge as many structures are designed by hand in a trial and error process and thus are limited by engineers' knowledge and experience. In this work, a computational design and fabrication framework is proposed to generate structures with multiple target states for one input actuation that don't require a separate training step. A material dithering scheme based on multi-material 3D printing is combined with locally applied copper coil heating elements and sequential heating patterns to control the thermo-mechanical properties of the structures and switch between the different deformation modes. A novel topology optimization approach based on power diagrams is used to encode the different target states in the structure while ensuring the fabricability of the structures and the compatibility with the drop-in heating elements. The versatility of the proposed framework is demonstrated for four different example structures from engineering and computer graphics. The numerical and experimental results show that the optimization framework can produce structures that show the desired motion, but experimental accuracy is limited by current fabrication methods. The generality of the proposed method makes it suitable for the development of structures for applications in many different fields from aerospace to robotics to animated fabrication in computer graphics.\",\n",
       "  'len': 1860},\n",
       " {'abstract': 'We describe ACE0, a lightweight platform for evaluating the suitability and viability of AI methods for behaviour discovery in multiagent simulations. Specifically, ACE0 was designed to explore AI methods for multi-agent simulations used in operations research studies related to new technologies such as autonomous aircraft. Simulation environments used in production are often high-fidelity, complex, require significant domain knowledge and as a result have high R&D costs. Minimal and lightweight simulation environments can help researchers and engineers evaluate the viability of new AI technologies for behaviour discovery in a more agile and potentially cost effective manner. In this paper we describe the motivation for the development of ACE0.We provide a technical overview of the system architecture, describe a case study of behaviour discovery in the aerospace domain, and provide a qualitative evaluation of the system. The evaluation includes a brief description of collaborative research projects with academic partners, exploring different AI behaviour discovery methods.',\n",
       "  'len': 1101},\n",
       " {'abstract': 'A reinforcement learning environment with adversary agents is proposed in this work for pursuit-evasion game in the presence of fog of war, which is of both scientific significance and practical importance in aerospace applications. One of the most popular learning environments, StarCraft, is adopted here and the associated mini-games are analyzed to identify the current limitation for training adversary agents. The key contribution includes the analysis of the potential performance of an agent by incorporating control and differential game theory into the specific reinforcement learning environment, and the development of an adversary agents challenge (SAAC) environment by extending the current StarCraft mini-games. The subsequent study showcases the use of this learning environment and the effectiveness of an adversary agent for evasion units. Overall, the proposed SAAC environment should benefit pursuit-evasion studies with rapidly-emerging reinforcement learning technologies. Last but not least, the corresponding tutorial code can be found at GitHub.',\n",
       "  'len': 1081},\n",
       " {'abstract': 'Autonomous landing is a capability that is essential to achieve the full potential of multi-rotor drones in many social and industrial applications. The implementation and testing of this capability on physical platforms is risky and resource-intensive; hence, in order to ensure both a sound design process and a safe deployment, simulations are required before implementing a physical prototype. This paper presents the development of a monocular visual system, using a software-in-the-loop methodology, that autonomously and efficiently lands a quadcopter drone on a predefined landing pad, thus reducing the risks of the physical testing stage. In addition to ensuring that the autonomous landing system as a whole fulfils the design requirements using a Gazebo-based simulation, our approach provides a tool for safe parameter tuning and design testing prior to physical implementation. Finally, the proposed monocular vision-only approach to landing pad tracking made it possible to effectively implement the system in an F450 quadcopter drone with the standard computational capabilities of an Odroid XU4 embedded processor.',\n",
       "  'len': 1142},\n",
       " {'abstract': 'This work considers the seven-coefficient polynomials proposed by the National Aeronautics and Space Administration (NASA) to facilitate obtaining a normalized value for three thermodynamic standard-state specific properties of ideal gases or condenser matters over an interval of temperature. These properties are the heat capacity at constant pressure, the absolute enthalpy (sensible enthalpy plus heat contents due to chemical or physical changes), and the entropy. In the open literature, one can find several databases for the polynomial coefficients with variation in the number of species included or the range of temperature covered, and this raises a question of whether the choice of a database to use has an important impact on these evaluated thermodynamic properties. Addressing this point, we compare and assess three databases for the NASA 7-coefficient polynomials, over a selected range of temperature from 300 K to 3500 K, and for selected six common gaseous species encountered in combustion or industrial processes, which are molecular oxygen (O2), molecular nitrogen (N2), molecular hydrogen (H2), methane (CH4), carbon dioxide (CO2), and water vapor (H2O). Our comparisons suggest that despite the difference in the values of coefficients, there is no significant difference in their predictions. However, the latest (7th edition) database of Prof. Alexander Burcat hosted at Eötvös Loránd University (ELTE) in Budapest, Hungary showed superior features when contrasted to other two databases (one accompanying the simulation package OpenFOAM 6, and another provided by the natural-gas reaction mechanism GRI-MECH 3.0).',\n",
       "  'len': 1653},\n",
       " {'abstract': 'This paper describes work carried out within the European Union (EU)-Russia Buterfli project to look at the control of transition-causing \"target\" stationary cross flow vortices, by the use of distributed plasma actuation to generate sub-dominant \"killer\" modes. The objective is to use the \"killer\" modes to control the \"target\" modes through a non-linear stabilizing mechanism. The numerical modelling and results are compared to experimental studies performed at the TsAGI T124 tunnel for a swept plate subject to a favorable pressure gradient flow. A mathematical model for the actuator developed at TsAGI was implemented in a linearized Navier Stokes (LNS) solver and used to model and hence predict \"killer\" mode amplitudes at a measurement plane in the experiment. The LNS analysis shows good agreement with experiment, and the results are used as input for non-linear PSE analysis to predict the effect of these modes on crossflow transition. Whilst the numerical model indicates a delay in transition, experimental results indicated an advance in transition rather than delay. This was determined to be due to actuator induced unsteadiness arising in the experiment, resulting in the generation of travelling crossflow disturbances which tended to obscure and thus dominate the plasma stabilized stationary disturbances.',\n",
       "  'len': 1340},\n",
       " {'abstract': \"As the aerospace industry becomes increasingly demanding for stronger lightweight materials, the ultra-strong carbon nanotube (CNT) composites with highly aligned CNT network structures could be the answer. In this work, a novel methodology applying topological data analysis (TDA) to the scanning electron microscope (SEM) images was developed to detect CNT orientation. The CNT bundle extensions in certain directions were summarized algebraically and expressed as visible barcodes. The barcodes were then calculated and converted into the total spread function $V(X,\\\\theta)$, from which the alignment fraction and the preferred direction could be determined. For validation purposes, the random CNT sheets were mechanically stretched at various strain ratios ranging from $0-40\\\\%$, and quantitative TDA analysis was conducted based on the SEM images taken at random positions. The results showed high consistency ($R^2=0.975$) compared to the Herman's orientation factors derived from the polarized Raman spectroscopy and wide-angle X-ray scattering analysis. Additionally, the TDA method presented great robustness with varying SEM acceleration voltages and magnifications, which might alter the scope in alignment detection. With potential applications in nanofiber systems, this study offers a rapid and simple way to quantify CNT alignment, which plays a crucial role in transferring the CNT properties into engineering products.\",\n",
       "  'len': 1447},\n",
       " {'abstract': 'The capability to simulate a two-way coupled interaction between a rarefied gas and an arbitrary-shaped colloidal particle is important for many practical applications, such as aerospace engineering, lung drug deliver and semiconductor manufacturing. By means of numerical simulations based on the Direct Simulation Monte Carlo (DSMC) method, we investigate the influence of the orientation of the particle and rarefaction on the drag and lift coefficients, in the case of prolate and oblate ellipsoidal particles immersed in a uniform ambient flow. This is done by modelling the solid particles using a cut-cell algorithm embedded within our DSMC solver. In this approach, the surface of the particle is described by its analytical expression and the microscopic gas-solid interactions are computed exactly using a ray-tracing technique. The measured drag and lift coefficients are used to extend the correlations available in the continuum regime to the rarefied regime, focusing on the transitional and free-molecular regimes. The functional forms for the correlations for the ellipsoidal particles are chosen as a generalisation from the spherical case. We show that the fits over the data from numerical simulations can be extended to regimes outside the simulated range of $Kn$ by testing the obtained predictive model on values of $Kn$ that where not included in the fitting process, allowing to achieve an higher precision when compared with existing predictive models from literature. Finally, we underline the importance of this work in providing new correlations for non-spherical particles that can be used for point-particle Euler-Lagrangian simulations to address the problem of contamination from finite-size particles in high-tech mechanical systems.',\n",
       "  'len': 1777},\n",
       " {'abstract': \"The Ni-base superalloy 718 is the most widely used material for turbomachinery in the aerospace industry and land-based turbines. Although the relationship between processing and the resulting properties is well known, an understanding of the specific deformation mechanisms activated across its application temperature range is required to create more mechanistically accurate property models. Direct atomic-scale imaging observations with high angle annular dark-field scanning transmission electron microscopy, complemented by phase-field modeling informed by generalized stacking fault surface calculations using density functional theory, were employed to understand the shear process of ${\\\\gamma}''$ and ${\\\\gamma}'/{\\\\gamma}''$ co-precipitates after 1 \\\\% macroscopic strain at lower temperature (ambient and $427 °C$). Experimentally, intrinsic stacking faults were observed in the ${\\\\gamma}''$, whereas the ${\\\\gamma}'$ was found to exhibit anti-phase boundaries or superlattice intrinsic stacking faults. Additionally, the atomically flat ${\\\\gamma}'/{\\\\gamma}''$ interfaces in the co-precipitates were found to exhibit offsets after shearing, which can be used as tracers for the deformation events. Phase-field modeling shows that the developing fault-structure is dependent on the direction of the Burgers vector of the $a/2 \\\\langle110\\\\rangle$ matrix dislocation (or dislocation group) due to the lower crystal symmetry of the ${\\\\gamma''}$ phase. The interplay between ${\\\\gamma}'$ and ${\\\\gamma}''$ phases results in unique deformation pathways of the co-precipitate and increases the shear resistance. Consistent with the experimental observations, the simulation results indicate that complex shearing mechanisms are active in the low-temperature deformation regime and that multiple $a/2 \\\\langle110\\\\rangle$ dislocations of non-parallel Burgers vectors may be active on the same slip plane.\",\n",
       "  'len': 1909},\n",
       " {'abstract': \"The increase in the application of the satellite has skyrocketed the number of satellites, especially in the low earth orbit. The major concern today is after the end of life, these satellites become debris which negatively affects the space environment. As per the international guidelines of the European Space Agency, it is mandatory to deorbit the satellite within 25 years of the end of life. StudSat1, which was successfully launched on 12th July 2010, is the first Pico satellite developed in India by undergraduate students from seven different engineering colleges across South India. Now, the team is developing StudSat2, which is India's first twin satellite mission having two nanosatellites whose overall mass is less than 10kg. This paper is aimed to design the propulsion system, cold gas thruster, to deorbit StudSat2 from its original orbit i.e. 600 km to lower orbit i.e. 400km. The propulsion system mainly consists of a storage tank, pipes, Convergent Divergent nozzle, and electronic actuators. The paper also gives information about the components of cold gas thruster, which have been designed in the CATIA V5, and the structural and flow analysis of the same has been done in ANSYS. The concept of Hohmann transfer has been used to deorbit the satellite and STK has been used to simulate it.\",\n",
       "  'len': 1326},\n",
       " {'abstract': 'In many inference problems, the evaluation of complex and costly models is often required. In this context, Bayesian methods have become very popular in several fields over the last years, in order to obtain parameter inversion, model selection or uncertainty quantification. Bayesian inference requires the approximation of complicated integrals involving (often costly) posterior distributions. Generally, this approximation is obtained by means of Monte Carlo (MC) methods. In order to reduce the computational cost of the corresponding technique, surrogate models (also called emulators) are often employed. Another alternative approach is the so-called Approximate Bayesian Computation (ABC) scheme. ABC does not require the evaluation of the costly model but the ability to simulate artificial data according to that model. Moreover, in ABC, the choice of a suitable distance between real and artificial data is also required. In this work, we introduce a novel approach where the expensive model is evaluated only in some well-chosen samples. The selection of these nodes is based on the so-called compressed Monte Carlo (CMC) scheme. We provide theoretical results supporting the novel algorithms and give empirical evidence of the performance of the proposed method in several numerical experiments. Two of them are real-world applications in astronomy and satellite remote sensing.',\n",
       "  'len': 1402},\n",
       " {'abstract': 'One of the possible ways to face the challenge of reducing the environmental impact of aviation, without limiting the growth of air transport, is the introduction of more efficient, radically different aircraft architectures. Among these, the box-wing one represents a promising solution, at least in the case of its application to short-to-medium haul aircraft, which, according to the achievement of the H2020 project \"PARSIFAL\", would bring to a 20% reduction in terms of emitted CO2 per passenger-kilometre. The present paper faces the problem of estimating the structural mass of such a disruptive configuration in the early stages of the design, underlining the limitations in this capability of the approaches available by literature and proposing a DoE-based approach to define surrogate models suitable for such purpose. A test case from the project \"PARSIFAL\" is used for the first conception of the approach, starting from the Finite Element Model parametrization, then followed by the construction of a database of FEM results, hence introducing the regression models and implementing them in an optimization framework. Results achieved are investigated in order to validate both the wing sizing and the optimization procedure. Finally, an additional test case resulting from the application of the box-wing layout to the regional aircraft category within the Italian research project \"PROSIB\", is briefly presented to further assess the capabilities of the proposed approach.',\n",
       "  'len': 1499},\n",
       " {'abstract': \"Neural networks are often utilised in critical domain applications (e.g.~self-driving cars, financial markets, and aerospace engineering), even though they exhibit overconfident predictions for ambiguous inputs. This deficiency demonstrates a fundamental flaw indicating that neural networks often overfit on spurious correlations. To address this problem in this work we present two novel objectives that improve the ability of a network to detect out-of-distribution samples and therefore avoid overconfident predictions for ambiguous inputs. We empirically demonstrate that our methods outperform the baseline and perform better than the majority of existing approaches, while performing competitively those that they don't outperform. Additionally, we empirically demonstrate the robustness of our approach against common corruptions and demonstrate the importance of regularisation and auxiliary information in out-of-distribution detection.\",\n",
       "  'len': 957},\n",
       " {'abstract': 'Everted tubes have often been modeled as inflated beams to determine transverse and axial buckling conditions. This paper seeks to validate the assumption that an everted tube can be modeled in this way. The tip deflections of everted and uneverted beams under transverse cantilever loads are compared with a tip deflection model that was first developed for aerospace applications. LDPE and silicone coated nylon beams were tested; everted and uneverted beams showed similar tip deflection. The literature model best fit the tip deflection of LDPE tubes with an average tip deflection error of 6 mm, while the nylon tubes had an average tip deflection error of 16.4 mm. Everted beams of both materials buckled at 83% of the theoretical buckling condition while straight beams collapsed at 109% of the theoretical buckling condition. The curvature of everted beams was estimated from a tip load and a known displacement showing relative errors of 14.2% and 17.3% for LDPE and nylon beams respectively. This paper shows a numerical method for determining inflated beam deflection. It also provides an iterative method for computing static tip pose and applied wall forces in a known environment.',\n",
       "  'len': 1205},\n",
       " {'abstract': \"The integration of Information and Communication Technology (ICT) tools into mechanical devices found in aviation industry has raised security concerns. The more integrated the system, the more vulnerable due to the inherent vulnerabilities found in ICT tools and software that drives the system. The security concerns have become more heightened as the concept of electronic-enabled aircraft and smart airports get refined and implemented underway. In line with the above, this paper undertakes a review of cyber-security incidence in the aviation sector over the last 20 years. The essence is to understand the common threat actors, their motivations, the type of attacks, aviation infrastructure that is commonly attacked and then match these so as to provide insight on the current state of the cyber-security in the aviation sector. The review showed that the industry's threats come mainly from Advance Persistent Threat (APT) groups that work in collaboration with some state actors to steal intellectual property and intelligence, in order to advance their domestic aerospace capabilities as well as possibly monitor, infiltrate and subvert other nations' capabilities. The segment of the aviation industry commonly attacked is the Information Technology infrastructure, and the prominent type of attacks is malicious hacking activities that aim at gaining unauthorised access using known malicious password cracking techniques such as Brute force attacks, Dictionary attacks and so on. The review further analysed the different attack surfaces that exist in aviation industry, threat dynamics, and use these dynamics to predict future trends of cyberattacks in the industry. The aim is to provide information for the cybersecurity professionals and aviation stakeholders for proactive actions in protecting these critical infrastructures against cyberincidence for an optimal customer service oriented industry.\",\n",
       "  'len': 1931},\n",
       " {'abstract': 'Time-equispaced inertial measurements are practically used as inputs for motion determination. Polynomial interpolation is a common technique of recovering the gyroscope signal but is subject to a fundamentally numerical stability problem due to the Runge effect on equispaced samples. This paper reviews the theoretical results of Runge phenomenon in related areas and proposes a straightforward borrowing-and-cutting (BAC) strategy to depress it. It employs the neighboring samples for higher-order polynomial interpolation but only uses the middle polynomial segment in the actual time interval. The BAC strategy has been incorporated into attitude computation by functional iteration, leading to accuracy benefit of several orders of magnitude under the classical coning motion. It would potentially bring significant benefits to the inertial navigation computation under sustained dynamic motions.',\n",
       "  'len': 913},\n",
       " {'abstract': 'This work is concerned with the numerical simulation of plasma arc interaction with aerospace substrates under conditions akin to lightning strike and in particular with the accurate calculation of radiative heat losses. These are important because they have a direct effect on the calculation of thermal and pressure loads on the substrates, which can lead to material damage under certain conditions. Direct numerical solution of the radiation transport equation (RTE) in mesoscale simulations is not viable due to its computational cost, so for practical applications reduced models are usually employed. To this end, four approximations for solving the RTE are considered in this work, ranging from a simple local thermodynamical behaviour consideration, to a more complex spectral absorption dependent on the arc geometry. Their performance is initially tested on a one-dimensional cylindrical arc, before implementing them in a multi-dimensional magnetohydrodynamics code. Results indicate that inclusion of spectral absorption is necessary in order to obtain consistent results. However, the approaches accounting for the arc geometry require repeated solution of the computationally intensive Helmholtz equations, making them prohibitive for multi-dimensional simulations. As an alternative, a method using the net emission coefficient is employed, which provides a balance between computational efficiency and accuracy, as shown by comparisons against experimental measurements for a plasma arc attaching to an aluminium substrate.',\n",
       "  'len': 1551},\n",
       " {'abstract': 'Aerospace production volumes have increased over time and robotic solutions have been progressively introduced in the aeronautic assembly lines to achieve high-quality standards, high production rates, flexibility and cost reduction. Robotic workcells are sometimes characterized by robots mounted on slides to increase the robot workspace. The slide introduces an additional degree of freedom, making the system kinematically redundant, but this feature is rarely used to enhance performances. The paper proposes a new concept in trajectory planning, that exploits the redundancy to satisfy additional requirements. A dynamic programming technique is adopted, which computes optimized trajectories, minimizing or maximizing the performance indices of interest. The use case is defined on the LABOR (Lean robotized AssemBly and cOntrol of composite aeRostructures) project which adopts two cooperating six-axis robots mounted on linear axes to perform assembly operations on fuselage panels. Considering the needs of this workcell, unnecessary robot movements are minimized to increase safety, the mechanical stiffness is maximized to increase stability during the drilling operations, collisions are avoided, while joint limits and the available planning time are respected. Experiments are performed in a simulation environment, where the optimal trajectories are executed, highlighting the resulting performances and improvements with respect to non-optimized solutions.',\n",
       "  'len': 1484},\n",
       " {'abstract': \"Many unconventional descent mechanisms are evolved in nature to maximize the dispersion of seeds to increase the population of floral species. The induced autorotation produces lift through asymmetrical weight distribution, increasing the fall duration and giving the seed extra time to get drifted away by the wind. The proposed bio-inspired concept was used to produce novel modern pods for various aerospace applications that require free-falling or controlled velocity descent in planetary or interplanetary missions without relying on traditional techniques such as propulsion-based descent and the use of parachutes. We provide an explanation for the design procedure and the functioning of a mono blade auto-rotating wing. An element-based computational method based on Glauert's blade element momentum theory (BEMT) model was employed to estimate the geometry by maximizing the coefficient of power through MATLAB's optimization toolbox using the Sequential quadratic programming (SQP) solver. The dynamic model was developed for the single-wing design through the MATLAB Simulink 6-DOF toolbox to carry out a free-flight simulation of the wing to verify its global stability.\",\n",
       "  'len': 1195},\n",
       " {'abstract': 'Aerospace technologies are crucial for modern civilization; space-based infrastructure underpins weather forecasting, communications, terrestrial navigation and logistics, planetary observations, solar monitoring, and other indispensable capabilities. Extraplanetary exploration -- including orbital surveys and (more recently) roving, flying, or submersible unmanned vehicles -- is also a key scientific and technological frontier, believed by many to be paramount to the long-term survival and prosperity of humanity. All of these aerospace applications require reliable control of the craft and the ability to record high-precision measurements of physical quantities. Magnetometers deliver on both of these aspects, and have been vital to the success of numerous missions. In this review paper, we provide an introduction to the relevant instruments and their applications. We consider past and present magnetometers, their proven aerospace applications, and emerging uses. We then look to the future, reviewing recent progress in magnetometer technology. We particularly focus on magnetometers that use optical readout, including atomic magnetometers, magnetometers based on quantum defects in diamond, and optomechanical magnetometers. These optical magnetometers offer a combination of field sensitivity, size, weight, and power consumption that allows them to reach performance regimes that are inaccessible with existing techniques. This promises to enable new applications in areas ranging from unmanned vehicles to navigation and exploration.',\n",
       "  'len': 1564},\n",
       " {'abstract': 'In recent years, the growth of Machine Learning (ML) algorithms has raised the number of studies including their applicability in a variety of different scenarios. Among all, one of the hardest ones is the aerospace, due to its peculiar physical requirements. In this context, a feasibility study and a first prototype for an Artificial Intelligence (AI) model to be deployed on board satellites are presented in this work. As a case study, the detection of volcanic eruptions has been investigated as a method to swiftly produce alerts and allow immediate interventions. Two Convolutional Neural Networks (CNNs) have been proposed and designed, showing how to efficiently implement them for identifying the eruptions and at the same time adapting their complexity in order to fit on board requirements.',\n",
       "  'len': 814},\n",
       " {'abstract': \"In this paper, we present a user-friendly planetary rover's control system for low latency surface telerobotic. Thanks to the proposed system, an operator can comfortably give commands through the control base station to a rover using commercially available off-the-shelf (COTS) joysticks or by command sequencing with interactive monitoring on the sensed map of the environment. During operations, high situational awareness is made possible thanks to 3D map visualization. The map of the environment is built on the on-board computer by processing the rover's camera images with a visual Simultaneous Localization and Mapping (SLAM) algorithm. It is transmitted via Wi-Fi and displayed on the control base station screen in near real-time. The navigation stack takes as input the visual SLAM data to build a cost map to find the minimum cost path. By interacting with the virtual map, the rover exhibits properties of a Cyber Physical System (CPS) for its self-awareness capabilities. The software architecture is based on the Robot Operative System (ROS) middleware. The system design and the preliminary field test results are shown in the paper.\",\n",
       "  'len': 1161},\n",
       " {'abstract': 'BRITE-Constellation is devoted to high-precision optical photometric monitoring of bright stars, distributed all over the Milky Way, in red and/or blue passbands. Photometry from space avoids the turbulent and absorbing terrestrial atmosphere and allows for very long and continuous observing runs with high time resolution and thus provides the data necessary for understanding various processes inside stars (e.g., asteroseismology) and in their immediate environment. While the first astronomical observations from space focused on the spectral regions not accessible from the ground it soon became obvious around 1970 that avoiding the turbulent terrestrial atmosphere significantly improved the accuracy of photometry and satellites explicitly dedicated to high-quality photometry were launched. A perfect example is BRITE-Constellation, which is the result of a very successful cooperation between Austria, Canada and Poland. Research highlights for targets distributed nearly over the entire HRD are presented, but focus primarily on massive and hot stars.',\n",
       "  'len': 1074},\n",
       " {'abstract': 'The flexible capacitive pressure sensors are one of the most essential and famous devices with vast applications in automobile, aerospace, marine, healthcare, wearables, consumer, and portable electronics. The fabrication of pressure sensors in a cleanroom is expensive and time-consuming; however, the sensitivity, linearity, and other performance factors of those pressure sensors are exceptional. Moreover, sometimes we require sensors that are not expensive and can be fabricated rapidly where the other performance factors do not need to be highly remarkable. In this modern era, household materials and DIY (Do-it-yourself) techniques are quite helpful, highly utilized. They are recommended to fabricate low-cost sensors and healthcare devices for personalized medicine and low-cost consumer electronics. Different flexible capacitive pressure sensors are presented and experimentally characterized for acoustic and air-pressure monitoring in this thesis. The design criteria of a cantilever-based capacitive pressure sensor are discussed. The three different designs are analysed with aspect ratios of 1.5, 1.0, and 0.67. The sensor with an aspect ratio of 0.67 shows maximum sensitivity (mechanical and electrical), better response time, and the 1st and 2nd mode of resonant frequencies is comparatively less than the other two. The cantilever designs are susceptible to slight pressure; therefore, the diaphragm-based normal mode capacitive pressure sensor is introduced in the second chapter, which defines the design criteria of diaphragm shapes. The five different diaphragms analysed are circular, elliptical, pentagon, square, and rectangular shapes. The circular capacitive pressure sensor shows maximum sensitivity, however, maximum non-linear response.....',\n",
       "  'len': 1785},\n",
       " {'abstract': 'Semiconducting transition metal dichalcogenides (TMDs) are promising for flexible high-specific-power photovoltaics due to their ultrahigh optical absorption coefficients, desirable band gaps and self-passivated surfaces. However, challenges such as Fermi-level pinning at the metal contact-TMD interface and the inapplicability of traditional doping schemes have prevented most TMD solar cells from exceeding 2% power conversion efficiency (PCE). In addition, fabrication on flexible substrates tends to contaminate or damage TMD interfaces, further reducing performance. Here, we address these fundamental issues by employing: 1) transparent graphene contacts to mitigate Fermi-level pinning, 2) $\\\\rm{MoO}_\\\\it{x}$ capping for doping, passivation and anti-reflection, and 3) a clean, non-damaging direct transfer method to realize devices on lightweight flexible polyimide substrates. These lead to record PCE of 5.1% and record specific power of $\\\\rm{4.4\\\\ W\\\\,g^{-1}}$ for flexible TMD ($\\\\rm{WSe_2}$) solar cells, the latter on par with prevailing thin-film solar technologies cadmium telluride, copper indium gallium selenide, amorphous silicon and III-Vs. We further project that TMD solar cells could achieve specific power up to $\\\\rm{46\\\\ W\\\\,g^{-1}}$, creating unprecedented opportunities in a broad range of industries from aerospace to wearable and implantable electronics.',\n",
       "  'len': 1390},\n",
       " {'abstract': 'Integrated circuits (ICs) that can operate at high temperature have a wide variety of applications in the fields of automotive, aerospace, space exploration, and deep-well drilling. Conventional silicon-based complementary metal-oxide-semiconductor (CMOS) circuits cannot work at higher than 200 $^\\\\circ$C, leading to the use of wide bandgap semiconductor, especially silicon carbide (SiC). However, high-density defects at an oxide-SiC interface make it impossible to predict electrical characteristics of SiC CMOS logic gates in a wide temperature range and high supply voltage (typically ${\\\\geqq 15}$ V) is required to compensate their large logic threshold voltage shift. Here, we show that SiC complementary logic gates composed of p- and n-channel junction field-effect transistors (JFETs) operate at 300 $^\\\\circ$C with a supply voltage as low as 1.4 V. The logic threshold voltage shift of the complementary JFET (CJFET) inverter is 0.2 V from room temperature to 300 $^\\\\circ$C. Furthermore, temperature dependencies of the static and dynamic characteristics of the CJFET inverter are well explained by a simple analytical model of SiC JFETs. This allows us to perform electronic circuit simulation, leading to superior designability of complex circuits or memories based on SiC CJFET technology, which operate within a wide temperature range.',\n",
       "  'len': 1361},\n",
       " {'abstract': 'Direct simulation of physical processes on a kinetic level is prohibitively expensive in aerospace applications due to the extremely high dimension of the solution spaces. In this paper, we consider the moment system of the Boltzmann equation, which projects the kinetic physics onto the hydrodynamic scale. The unclosed moment system can be solved in conjunction with the entropy closure strategy. Using an entropy closure provides structural benefits to the physical system of partial differential equations. Usually computing such closure of the system spends the majority of the total computational cost, since one needs to solve an ill-conditioned constrained optimization problem. Therefore, we build a neural network surrogate model to close the moment system, which preserves the structural properties of the system by design, but reduces the computational cost significantly. Numerical experiments are conducted to illustrate the performance of the current method in comparison to the traditional closure.',\n",
       "  'len': 1025},\n",
       " {'abstract': 'This paper investigates applications of eye tracking in transport aircraft design evaluations. Piloted simulations were conducted for a complete flight profile including take off, cruise and landing flight scenario using the transport aircraft flight simulator at CSIR National Aerospace Laboratories. Thirty-one simulation experiments were carried out with three pilots and engineers while recording the ocular parameters and the flight data. Simulations were repeated for high workload conditions like flying with degraded visibility and during stall. Pilots visual scan behaviour and workload levels were analysed using ocular parameters; while comparing with the statistical deviations from the desired flight path. Conditions for fatigue were also recreated through long duration simulations and signatures for the same from the ocular parameters were assessed. Results from the study found correlation between the statistical inferences obtained from the ocular parameters with those obtained from the flight path deviations. The paper also demonstrates an evaluators console that assists the designers or evaluators for better understanding of pilots attentional resource allocation.',\n",
       "  'len': 1201},\n",
       " {'abstract': 'In this article, the effect of geometrical parameters and flow conditions on the performance of a swirl atomizer is studied. Dimensional analysis and experimental investigations are utilized to define significant terms. The PDA system used for the measurements was able to supply information about the size, concentration, and particle velocity at each measurement location. The orifice diameter, the spiral cone angle, and also the flow Reynolds number, which is defined based on the injector orifice diameter, play an important role in spray quality, and their significance is summarized in a correlation. In order to achieve the appropriate combination of design variables that satisfy the design constraints, a GA-based program was used in a reverse analysis process. Finally, the advantages of human inspection were employed to provide true best performers from a small group of final answers.',\n",
       "  'len': 909},\n",
       " {'abstract': 'High entropy alloys are finding significant scientific interest due to their exotic microstructures and exceptional properties resulting thereof. These alloys have excellent corrosion resistance and may find broad range of applications from bio-implants, aerospace components and nuclear industry. A critical performance metric that determines the application worthiness of the alloys is the resilience of stressed structural members in a corrosive environment. This study reports the results from a novel experimental setup to quantify the corrosion rate under uniaxial tensile stress in a single phase fcc Al0.1CoCrFeNi high entropy alloy rods. Under a uniform uniaxial applied stress of 600 MPa, the corrosion current density was observed to increase by three orders of magnitude and ~150 mV drop in corrosion potential. The mechanism of accelerated corrosion is identified as surface passivation layer breakdown, pit initiation on un-passivated surface and rapid pit-propagation along the loading direction.',\n",
       "  'len': 1022},\n",
       " {'abstract': 'A Cyber-Physical System (CPS) comprises physical as well as software subsystems. Simulation-based approaches are typically used to support design and Verification and Validation (V&V) of CPSs in several domains such as: aerospace, defence, automotive, smart grid and healthcare. Accordingly, many simulation-based tools are available, and this poses huge interoperability challenges. To overcome them, in 2010 the Functional Mock-up Interface (FMI) was proposed as an open standard to support both Model Exchange (ME) and Co-Simulation (CS). Models adhering to such a standard are called Functional Mock-up Units (FMUs). FMUs play an essential role in defining complex CPSs through, e.g., the SSP standard. Simulation-based V&V of CPSs typically requires exploring different scenarios (i.e., exogenous CPS input sequences), many of them showing a shared prefix. Accordingly, the simulator state at the end of a shared prefix is saved and then restored and used as a start state for the simulation of the next scenario. In this context, an important FMI feature is the capability to save and restore the internal FMU state on demand. Unfortunately, the implementation of this feature is not mandatory and it is available only within some commercial software. This motivates developing such a feature for open-source CPS simulation environments. In this paper, we focus on JModelica, an open-source modelling and simulation environment for CPSs defined in the Modelica language. We describe how we have endowed JModelica with our open-source implementation of the FMI 2.0 functions to save and restore internal states of FMUs for ME. Furthermore, we present results evaluating, through 934 benchmark models, correctness and efficiency of our extended JModelica. Our results show that simulation-based V&V is, on average, 22 times faster with our get/set functionality than without it.',\n",
       "  'len': 1893},\n",
       " {'abstract': 'Motivated by the goal of learning controllers for complex systems whose dynamics change over time, we consider the problem of designing control laws for systems that switch among a finite set of unknown discrete-time linear subsystems under unknown switching signals. To this end, we propose a method that uses data to directly design a control mechanism without any explicit identification step. Our approach is online, meaning that the data are collected over time while the system is evolving in closed-loop, and are directly used to iteratively update the controller. A major benefit of the proposed online implementation is therefore the ability of the controller to automatically adjust to changes in the operating mode of the system. We show that the proposed control mechanism guarantees stability of the closed-loop switched linear system provided that the switching is slow enough. Effectiveness of the proposed design technique is illustrated for two aerospace applications.',\n",
       "  'len': 996},\n",
       " {'abstract': 'Active learning is a subfield of machine learning that focuses on improving the data collection efficiency of expensive-to-evaluate systems. Especially, active learning integrated surrogate modeling has shown remarkable performance in computationally demanding engineering systems. However, the existence of heterogeneity in underlying systems may adversely affect the performance of active learning. In order to improve the learning efficiency under this regime, we propose the partitioned active learning that seeks the most informative design points for partitioned Gaussian process modeling of heterogeneous systems. The proposed active learning consists of two systematic subsequent steps: the global searching scheme accelerates the exploration of active learning by investigating the most uncertain design space, and the local searching exploits the circumscribed information induced by the local GP. We also propose Cholesky update driven numerical remedies for our active learning to address the computational complexity challenge. The proposed method is applied to numerical simulations and two real-world case studies about (i) the cost-efficient automatic fuselage shape control in aerospace manufacturing; and (ii) the optimal design of tribocorrosion-resistant alloys in materials science. The results show that our approach outperforms benchmark methods with respect to prediction accuracy and computational efficiency.',\n",
       "  'len': 1445},\n",
       " {'abstract': 'Ultrawide bandgap semiconductor technologies offer potentially revolutionary advances in the rapidly developing areas of quantum communication, short wavelength optics, smart energy conversion and biomedical interfaces. These strongly demanding technologies can be partly constructed using conventional devices but new hybrid architectures are needed to overpass current performances and add functionalities. Here, we propose a new concept based on the specific properties of a diamond pn junction combined with both an electric and optical control of the depletion region. Using this junction as a gate in a junction field effect transistor, we report a proof of concept of a non volatile diamond photo-switch. A diamond pn junction made with nitrogen deep donors in the n-side is demonstrated to be optically activated thanks to visible light. The n-type diamond gate is almost devoid of free carriers in the dark and thus insulating. Illuminating the device renders the standard electrical gate control of the transistor efficient. Without illumination, the device is frozen, keeping a permanent memory of the current state. This new way of operating the device opens numerous possibilities to store and transfer information or energy with applications in the field of electrical aircraft or aerospace electronics, power electronics, bio-electronics and quantum communication.',\n",
       "  'len': 1390},\n",
       " {'abstract': 'Conventional Sliding mode control and observation techniques are widely used in aerospace applications, including aircrafts, UAVs, launch vehicles, missile interceptors, and hypersonic missiles. This work is dedicated to creating a MATLAB-based sliding mode controller design and simulation software toolbox that aims to support aerospace vehicle applications. An architecture of the aerospace sliding mode control toolbox (SMC Aero) using the relative degree approach is proposed. The SMC Aero libraries include 1st order sliding mode control (1-SMC), second order sliding mode control (2-SMC), higher order sliding mode (HOSM) control (either fixed gain or adaptive), as well as higher order sliding mode differentiators. The efficacy of the SMC Aero toolbox is confirmed in two case studies: controlling and simulating resource prospector lander (RPL) soft landing on the Moon and launch vehicle (LV) attitude control in ascent mode.',\n",
       "  'len': 947},\n",
       " {'abstract': 'Space agencies have been incessantly working to propose a sustainable architecture for human Mars mission. But, before proceeding to a giant leap and accomplishing those mission intent, it is significant to know the extent of possibility to expand terrestrial species on the vast red planet. Decades of scientific exploration and experimentation through planetary landers and rovers showed uncertain and unsatisfactory results to determine the possibility of life on Mars. Consequently, the technological limitation has impeded to perform in-situ experimentation and analysis on the surface. Therefore, we require soil samples through Mars sample return vehicles for superior analysis in our ground-based laboratories or on-orbit analysis aboard International Space Station from the aspect of planetary protection policy. Sampling analysis either in ground or orbit will report the presence of fundamental constituents to harbor life. To effectuate this intent, we have proposed a unique sample return architecture integrated with large solar Montgolfier. Here, we deploy parachute and retro propulsion thrusters to deliver sample return vehicles on to the surface and systematic ascend with the aid of solar Montgolfier to propel the MSRV out of Mars atmosphere. Subsequently, the MSRV effectuates orbital rendezvous with orbiter for the refueling process and safe return. The proposed concept is cheap, robust and simple as compared to the current state of MSR architectures ultimately minimizing the technological necessities. It also detains backup plans that were found to be nowhere presented in any of sample return strategies. Further, we have extended our discussion to comprehensively analyze entire MSR architecture with our concept for mission feasibility.',\n",
       "  'len': 1779},\n",
       " {'abstract': 'A systematic study was performed to measure the effective thermal conductivity of ceramic particle beds, a promising heat transfer and thermal energy storage media for concentrating solar power (CSP). The thermal conductivity of the ceramic particle beds was measured using a transient hot-wire (THW) method within a temperature range of room temperature to 700 oC, the target operating temperature of the next-generation CSP systems. Two different types of ceramic particles were examined: (1) CARBOBEAD HSP 40/70 and (2) CARBOBEAD CP 40/100 with the average particle sizes of ~ 400 {\\\\mu}m and ~280 {\\\\mu}m, respectively, and thermal conductivities ranging from ~0.25 W m-1 K-1 to ~0.50 W m-1 K-1 from 20 oC to 700 oC in both air and N2 gas. The gaseous pressure dependence of the thermal conductivity of the ceramic particle beds was also studied in the N2 environment to differentiate the contributions from gas conduction, solid conduction, and radiation. Calculations using the Zehner, Bauer, and Schlünder (ZBS) model showed good agreements with the measurements. Based on the model, it is concluded that the effective thermal conductivity of the packed particle beds is dominated by the gas conduction while the solid conduction and radiation contributes to about 20% of the effective thermal conductivity at high temperature.',\n",
       "  'len': 1343},\n",
       " {'abstract': \"The Wide-field Infrared Transient Explorer (WINTER) is a 1x1 degree infrared survey telescope under development at MIT and Caltech, and slated for commissioning at Palomar Observatory in 2021. WINTER is a seeing-limited infrared time-domain survey and has two main science goals: (1) the discovery of IR kilonovae and r-process materials from binary neutron star mergers and (2) the study of general IR transients, including supernovae, tidal disruption events, and transiting exoplanets around low mass stars. We plan to meet these science goals with technologies that are relatively new to astrophysical research: hybridized InGaAs sensors as an alternative to traditional, but expensive, HgCdTe arrays and an IR-optimized 1-meter COTS telescope. To mitigate risk, optimize development efforts, and ensure that WINTER meets its science objectives, we use model-based systems engineering (MBSE) techniques commonly featured in aerospace engineering projects. Even as ground-based instrumentation projects grow in complexity, they do not often have the budget for a full-time systems engineer. We present one example of systems engineering for the ground-based WINTER project, featuring software tools that allow students or staff to learn the fundamentals of MBSE and capture the results in a formalized software interface. We focus on the top-level science requirements with a detailed example of how the goal of detecting kilonovae flows down to WINTER's optical design. In particular, we discuss new methods for tolerance simulations, eliminating stray light, and maximizing image quality of a fly's-eye design that slices the telescope's focus onto 6 non-buttable, IR detectors. We also include a discussion of safety constraints for a robotic telescope.\",\n",
       "  'len': 1770},\n",
       " {'abstract': 'In situ thermal transport measurement of flowing fluid could be useful for the characterization and diagnosis of practical thermal systems such as fluid heat exchangers and thermal energy storage systems. Despite abundant reports on the ex-situ thermal conductivity measurement of stagnant fluids, a suitable technique for the thermal conductivity measurement of flowing fluid has been rarely reported. This paper presents the thermal conductivity measurement of flowing fluid within a pipe using a non-contact modulated photothermal radiometry (MPR) technique, where the surface of the pipe is heated by an intensity-modulated laser and the heat diffuses into the fluid with suitable modulation frequency. We design a tube section with small wall thickness suitable for the MPR measurements to maximize the sensitivity of the thermal response to the fluid properties while minimizing the lateral heat spreading effect. Intrinsic thermal conductivity of different fluids was obtained within a proper range of frequency and flow velocity where the forced convection effect is negligible. The forced convection effect became prominent at high flowing velocity and at low modulation frequency, leading to overestimated thermal conductivity of fluid. It is found that the intrinsic thermal conductivity could be obtained when the flow velocity is less than 100 mm/sec and ReD1/2Pr1/3 < 100 for DI water and Xceltherm oil under the specified experimental conditions, where Re_D is the Reynolds number and Pr is the Prandtl number.',\n",
       "  'len': 1536},\n",
       " {'abstract': 'The orbital observatory Spectrum-Roentgen-Gamma (SRG), equipped with the grazing-incidence X-ray telescopes Mikhail Pavlinsky ART-XC and eROSITA, was launched by Roscosmos to the Lagrange L2 point of the Sun-Earth system on July 13, 2019. The launch was carried out from the Baikonur Cosmodrome by a Proton-M rocket with a DM-03 upper stage. The German telescope eROSITA was installed on SRG under an agreement between Roskosmos and the DLR, the German Aerospace Agency. In December 2019, SRG started to perform its main scientific task: scanning the celestial sphere to obtain X-ray maps of the entire sky in several energy ranges (from 0.2 to 8 keV with eROSITA, and from 4 to 30 keV with ART-XC). By mid-June 2021, the third six-month all-sky survey had been completed. Over a period of four years, it is planned to obtain eight independent maps of the entire sky in each of the energy ranges. The sum of these maps will provide high sensitivity and reveal more than three million quasars and over one hundred thousand massive galaxy clusters and galaxy groups. The availability of eight sky maps will enable monitoring of long-term variability (every six months) of a huge number of extragalactic and Galactic X-ray sources, including hundreds of thousands of stars with hot coronae. The rotation of the satellite around the axis directed toward the Sun with a period of four hours enables tracking the faster variability of bright X-ray sources during one day every half year. The chosen strategy of scanning the sky leads to the formation of deep survey zones near both ecliptic poles. The paper presents sky maps obtained by the telescopes on board SRG during the first survey of the entire sky and a number of results of deep observations performed during the flight to the L2 point in the frame of the performance verification program.(Abriged)',\n",
       "  'len': 1864},\n",
       " {'abstract': 'A concept of drone launched short range rockets (DLSRR) is presented. A drone or an aircraft rises DLSRR to a release altitude of up to 20 $km$. At the release altitude, the drone or an aircraft is moving at a velocity of up to 700 $m/s$ and a steep angle of up to 68$^o$ to the horizontal. After DLSRRs are released, their motors start firing. DLSRRs use slow burning motors to gain altitude and velocity. At the apogee of their flight DLSRRs release projectiles which fly to the target and strike it at high impact velocity. The projectiles reach a target at ranges of up to 442 $km$ and impact velocities up to 1.88 $km/s$. We show that a rocket launched at high altitude and high initial velocity does not need expensive thermal protection to survive ascent. Delivery of munitions to target by DLSRRs should be much less expensive than delivery by a conventional rocket. %% Even though delivery of munitions by bomber aircraft is even less expensive, a bomber needs to fly close to the target, while a DLSRR carrier releases the rockets from a distance of at least 200 $km$ from the target. %% All parameters of DLSRRs, and their trajectories are calculated based on theoretical (mechanical and thermodynamical) analysis and on several MatLab programs.',\n",
       "  'len': 1267},\n",
       " {'abstract': 'For manufacturing of aerospace composites, several parts may be processed simultaneously using convective heating in an autoclave. Due to uncertainties including tool placement, convective Boundary Conditions (BCs) vary in each run. As a result, temperature histories in some of the parts may not conform to process specifications due to under-curing or over-heating. Thermochemical analysis using Finite Element (FE) simulations are typically conducted prior to fabrication based on assumed range of BCs. This, however, introduces unnecessary constraints on the design. To monitor the process, thermocouples (TCs) are placed under tools near critical locations. The TC data may be used to back-calculate BCs using trial-and-error FE analysis. However, since the inverse heat transfer problem is ill-posed, many solutions are obtained for given TC data. In this study, a novel machine learning (ML) framework is presented capable of optimizing air temperature cycle in real-time based on TC data from multiple parts, for active control of manufacturing. The framework consists of two recurrent Neural Networks (NN) for inverse modeling of the ill-posed curing problem at the speed of 300 simulations/second, and a classification NN for multi-objective optimization of the air temperature at the speed of 35,000 simulations/second. A virtual demonstration of the framework for process optimization of three composite parts with data from three TCs is presented.',\n",
       "  'len': 1471},\n",
       " {'abstract': 'An idealized 1:2 scale demonstrator and a numerical parameter optimization algorithm are proposed to closely reproduce the deformation shape and, thus, spatial strain directions of a real aerodynamically loaded civil aircraft spoiler using only four concentrated loads. Cost-efficient experimental studies on demonstrators of increasing complexity are required to transfer knowledge from coupons to full-scale structures and to build up confidence in novel structural health monitoring (SHM) technologies. Especially for testing novel sensor systems that depend on or are affected by mechanical strains, e.g., strain-based SHM methods, it is essential that the considered lab-scale structures reflect the strain states of the real structure at operational loading conditions. Finite element simulations with detailed models were performed for static strength analysis and for comparison to experimental measurements. The simulated and measured deformations and spatial strain directions of the idealized demonstrator correlated well with the numerical results of the real aircraft spoiler. Thus, using the developed idealized demonstrator, strain-based SHM systems can be tested under conditions that reflect operational aerodynamic pressure loads, while the test effort and costs are significantly reduced. Furthermore, the presented loading optimization algorithm can be easily adapted to mimic other pressure loads in plate-like structures to reproduce specific structural conditions.',\n",
       "  'len': 1498},\n",
       " {'abstract': 'This paper describes a technique for the autonomous mission planning of unmanned aerial system swarms. Given a swarm operating in a known area, a central command system generates measurements from the swarm. If those measurements indicate changes to the mission situation such as target movement, the swarm planning is updated to reflect the new situation and guidance updates are broadcast to the swarm. The primary algorithms featured in this work are A* pathfinding and the Generalized Labeled Multi-Bernoulli multi-target tracking method.',\n",
       "  'len': 553},\n",
       " {'abstract': 'Hayabusa2 is the Japanese Asteroid Return Mission and targeted the carbonaceous asteroid Ryugu, conducted by the Japan Aerospace Exploration Agency (JAXA). The goal of this mission was to conduct proximity operations including remote sensing observations, material sampling, and a Small Carry-On Impact experiment, as well as sample analyses. As of September 2020, the spacecraft is on the way back to Earth with samples from Ryugu with no critical issues after the successful departure in November 2019. Here, we propose an extended mission in which the spacecraft will rendezvous with a small asteroid with ~30 m - ~40 m in diameter that is rotating at a spin period of ~10 min after an additional ~10-year cruise phase. We introduce that two scenarios are suitable for the extended mission. In the first scenario, the spacecraft will perform swing-by maneuvers at Venus once and Earth twice to arrive at asteroid 2001 AV43. In the second scenario, it will perform swing-by maneuvers at Earth twice to reach asteroid 1998 KY26. In both scenarios, the mission will continue until the early 2030s. JAXA recently released the decision that the spacecraft will rendezvous with 1998 KY26. This paper focuses on our scientific assessments of the two scenarios but leaves the decision process to go to 1998 KY26 for future reports. Rendezvous operations will be planned to detail the physical properties and surrounding environments of the target, one of the smallest elements of small planetary bodies. By achieving the planned operations, the mission will provide critical hints on the violent histories of collisions and accumulations of small bodies in the solar system. Furthermore, the established scientific knowledge and techniques will advance key technologies for planetary defense.',\n",
       "  'len': 1798},\n",
       " {'abstract': 'Fault tolerance is increasingly being use to design Dependable Digital Systems (DDS), which refers to the capability of a system to keep performing its intended functions in existence of faults. DDS are typically used in Safety-critical system (SCS) such as medical (I&C) devices, Nuclear power Plants (I&C) devices and Aerospace (I&C) systems, the failure in these systems can cause harm to environment, death, injury to people. Different fault tolerance techniques were developed to overcome these issues and that has led to increase the reliability and dependability of applications on Field Programmable Gate Arrays (FPGAs). In this paper, multiple related works are present dealing with different types of faults and fault tolerance methods in FPGA based systems. Furthermore, a comparison between the evaluation metrics of previous works of Fault Tolerant (FT) techniques like hardware redundancy overhead, time delay, reliability, and performance are also present.',\n",
       "  'len': 982},\n",
       " {'abstract': \"The Constellation Project's planned return to the moon requires numerous landings at the same site. Since the top few centimeters are loosely packed regolith, plume impingement from the Lander ejects the granular material at high velocities. Much work is needed to understand the physics of plume impingement during landing in order to protect hardware surrounding the landing sites. While mostly qualitative in nature, the Apollo Lunar Module landing videos can provide a wealth of quantitative information using modern photogrammetry techniques. The authors have used the digitized videos to quantify plume impingement effects of the landing exhaust on the lunar surface. The dust ejection angle from the plume is estimated at 1-3 degrees. The lofted particle density is estimated at 10^8 - 10^13 particles/m^3. Additionally, evidence for ejection of large 10-15 cm sized objects and a dependence of ejection angle on thrust are presented. Further work is ongoing to continue quantitative analysis of the landing videos.\",\n",
       "  'len': 1033},\n",
       " {'abstract': 'Experiments, analyses, and simulations have shown that the engine exhaust plume of a Mars lander large enough for human spaceflight will create a deep crater in the martian soil, blowing ejecta to approximately 1 km distance, damaging the bottom of the lander with high-momentum rock impacts, and possibly tilting the lander as the excavated hole collapses to become a broad residual crater upon engine cutoff. Because of this, we deem that we will not have adequate safety margins to land humans on Mars unless we robotically stabilize the soil to form in situ landing pads prior to the mission. It will take a significant amount of time working in a harsh off-planet environment to develop and certify the new technologies and procedures for in situ landing pad construction. The only place to reasonably accomplish this is on the Moon.',\n",
       "  'len': 849},\n",
       " {'abstract': 'For metrology, geodesy and gravimetry in space, satellite based instruments and measurement techniques are used and the orbits of the satellites as well as possible deviations between nearby ones are of central interest. The measurement of this deviation itself gives insight into the underlying structure of the spacetime geometry, which is curved and therefore described by the theory of general relativity (GR). In the context of GR, the deviation of nearby geodesics can be described by the Jacobi equation that is a result of linearizing the geodesic equation around a known reference geodesic with respect to the deviation vector and the relative velocity. We review the derivation of this Jacobi equation and restrict ourselves to the simple case of the spacetime outside a spherically symmetric mass distribution and circular reference geodesics to find solutions by projecting the Jacobi equation on a parallel propagated tetrad as done by Fuchs. Using his results, we construct solutions of the Jacobi equation for different physical initial scenarios inspired by satellite gravimetry missions and give a set of parameter together with their precise impact on satellite orbit deviation. We further consider the Newtonian analog and construct the full solution, that exhibits a similar structure, within this theory.',\n",
       "  'len': 1336},\n",
       " {'abstract': \"The Miniature X-ray Solar Spectrometer (MinXSS) is a 3-Unit (3U) CubeSat developed at the Laboratory for Atmospheric and Space Physics (LASP) at the University of Colorado, Boulder (CU). Over 40 students contributed to the project with professional mentorship and technical contributions from professors in the Aerospace Engineering Sciences Department at CU and from LASP scientists and engineers. The scientific objective of MinXSS is to study processes in the dynamic Sun, from quiet-Sun to solar flares, and to further understand how these changes in the Sun influence the Earth's atmosphere by providing unique spectral measurements of solar soft x-rays (SXRs). The enabling technology providing the advanced solar SXR spectral measurements is the Amptek X123, a commercial-off-the-shelf (COTS) silicon drift detector (SDD). The Amptek X123 has a low mass (~324 g after modification), modest power consumption (~2.50 W), and small volume (6.86 cm x 9.91 cm x 2.54 cm), making it ideal for a CubeSat. This paper provides an overview of the MinXSS mission: the science objectives, project history, subsystems, and lessons learned that can be useful for the small-satellite community.\",\n",
       "  'len': 1197},\n",
       " {'abstract': 'Bismaleimide (BMI) resins are a new breed of thermosetting resins used mainly for high temperature applications and have major usage in aerospace. FTIR studies have shown the signatures of imide, CNC stretching, malemide and N-H stretching. These BMI polymer coatings were deposited on aluminum and mild steel substrates by sprinkling powers followed by baking. Thermo gravimetric analysis and Differential scanning calorimetric studies showed the degradation temperature of these polymers around 370oC. Aluminum coatings were deposited on BMI previously deposited on Al and mild steel to make a metal-BMI-metal trilayer. These trilayers can solve the problem charging of the aircraft bodies at high altitudes. Atomic force microscopy was done to determine the morphology of the surface. Roughness and thickness measurements of the BMI coatings were carried out by surface profilometer. Vickers microhardness tests showed an increase in hardness of the metal-BMI-metal trilayer. FTIR spectrum showed signature of imides, CNC stretching, maleimide and N-H stretching in BMI. We observed that peak broadens at which shows the release of the stress during thermal treatment of the coating. The coating is subject to variable APPJ conditions which improve the properties at high temperature.',\n",
       "  'len': 1298},\n",
       " {'abstract': 'The vertical stabilizer is the key aerodynamic surface that provides an aircraft with its directional stability characteristic while ailerons and rudder are the primary control surfaces that give pilots control authority of the yawing and banking maneuvers. Losing the vertical stabilizer will, therefore, result in the consequential loss of lateral/directional stability and control, which is likely to cause a fatal crash. In this paper, we construct a scenario of a damaged aircraft model which has no physical rudder control surface, and then a strategy based on differential thrust is proposed to be utilized as a control input to act as a \"virtual\" rudder to help maintain stability and control of the damaged aircraft. The $H_{\\\\infty}$ loop-shaping approach based robust control system design is implemented to achieve a stable and robust flight envelope, which is aimed to provide a safe landing. Investigation results demonstrate successful application of such robust differential thrust methodology as the damaged aircraft can achieve stability within feasible control limits. Finally, the robustness analysis results conclude that the stability and performance of the damaged aircraft in the presence of uncertainty remain within desirable limits, and demonstrate not only a robust, but a safe flight mission through the proposed $H_{\\\\infty}$ loop-shaping robust differential thrust control methodology.',\n",
       "  'len': 1425},\n",
       " {'abstract': 'Spacecraft attitude control using only magnetic torques is a time-varying system. Many designs were proposed using LQR and H-infinity formulations. The existence of the solutions depends on the controllability of the linear time-varying systems which has not been established. In this paper, we will derive the conditions of the controllability for this linear time-varying systems.',\n",
       "  'len': 393},\n",
       " {'abstract': 'Computational mathematics plays an increasingly important role in computational fluid dynamics (CFD). The aeronautics and aerospace re- search community is working on next generation of CFD capacity that is accurate, automatic, and fast. A key component of the next generation of CFD is a greatly enhanced capacity for mesh generation and adaptivity of the mesh according to solution and geometry. In this paper, we propose a new method that generates triangular meshes on domains of curved boundary. The method deforms a Cartesian mesh that covers the domain to generate a mesh with prescribed boundary nodes. The deformation fields are generated by a system of divergence and curl equations which are solved effectively by the least square finite element method.',\n",
       "  'len': 775},\n",
       " {'abstract': 'Autonomous critical systems, such as satellites and space rovers, must be able to detect the occurrence of faults in order to ensure correct operation. This task is carried out by Fault Detection and Identification (FDI) components, that are embedded in those systems and are in charge of detecting faults in an automated and timely manner by reading data from sensors and triggering predefined alarms. The design of effective FDI components is an extremely hard problem, also due to the lack of a complete theoretical foundation, and of precise specification and validation techniques. In this paper, we present the first formal approach to the design of FDI components for discrete event systems, both in a synchronous and asynchronous setting. We propose a logical language for the specification of FDI requirements that accounts for a wide class of practical cases, and includes novel aspects such as maximality and trace-diagnosability. The language is equipped with a clear semantics based on temporal epistemic logic, and is proved to enjoy suitable properties. We discuss how to validate the requirements and how to verify that a given FDI component satisfies them. We propose an algorithm for the synthesis of correct-by-construction FDI components, and report on the applicability of the design approach on an industrial case-study coming from aerospace.',\n",
       "  'len': 1375},\n",
       " {'abstract': 'In this paper, a generalized differentiation-integration observer is presented based on sensors selection. The proposed differentiation-integration observer can estimate the multiple integrals and high-order derivatives of a signal, synchronously. The parameters selection rules are presented for the differentiation-integration observer. The theoretical results are confirmed by the frequency-domain analysis. The effectiveness of the proposed observer are verified through the numerical simulations on a quadrotor aircraft: i) through the differentiation-integration observer, the attitude angle and the uncertainties in attitude dynamics are estimated synchronously from the measurements of angular velocity; ii) a control law is designed based on the observers to drive the aircraft to track a reference trajectory.',\n",
       "  'len': 830},\n",
       " {'abstract': 'The widespread use of metallic structures in space technology brings risk of degradation which occurs under space conditions. New types of materials dedicated for space applications, that have been developed in the last decade, are in majority not well tested for different space mission scenarios. Very little is known how material degradation may affect the stability and functionality of space vehicles and devices during long term space missions. Our aim is to predict how the solar wind and electromagnetic radiation degrade metallic structures. Therefore both experimental and theoretical studies of material degradation under space conditions have been performed. The studies are accomplished at German Aerospace Center (DLR) in Bremen (Germany) and University of Zielona Góra (Poland). The paper presents the results of the theoretical part of those studies. It is proposed that metal bubbles filled with Hydrogen molecular gas, resulting from recombination of the metal free electrons and the solar protons, are formed on the irradiated surfaces. A thermodynamic model of bubble formation has been developed. We study the creation process of $\\\\rm{H_2}$-bubbles as function of, inter alia, the metal temperature, proton dose and energy. Our model has been verified by irradiation experiments completed at the DLR facility in Bremen. Consequences of the bubble formation are changes of the physical and thermo-optical properties of such degraded metals. We show that a high surface density of bubbles (up to $10^8$ $\\\\rm{cm^{-2}}$) with a typical bubble diameter of $\\\\sim 0.4$$\\\\rm{\\\\mu}$m will cause a significant increase of the metallic surface roughness. This may have serious consequences to any space mission. Changes in the thermo-optical properties of metallic foils are especially important for the solar sail propulsion technology, ...',\n",
       "  'len': 1860},\n",
       " {'abstract': 'This paper presents a novel model of large-size tilt-rotor aircraft, which can operate as a helicopter as well as being capable of transition to fixed-wing flight. Aerodynamics of the dynamic large-size tilt-rotors based on blade element method is analyzed during mode transition. For the large-size aircraft with turboshaft engines, the blade pitch angles of the rotors are regulated to vary according to the desired level of thrust, and the following expressions are formulated explicitly: rotor thrust and blade pitch angle, drag torque and blade pitch angle. A finite-time convergent observer based on Lyapunov function is developed to reconstruct the unknown variables and uncertainties during mode transitions. The merits of this design include the modeling of dynamic large-size tilt-rotor, ease of the uncertainties estimation during the tilting and the widely applications. Moreover, a switched logic controller based on the finite-time convergent observer is proposed to drive the aircraft to implement the mode transition with invariant flying height.',\n",
       "  'len': 1073},\n",
       " {'abstract': 'In this paper, colocated MIMO radar waveform design is considered by minimizing the integrated side-lobe level to obtain beam patterns with lower side-lobe levels than competing methods. First, a quadratic programming problem is formulated to design beam patterns by using the criteria for a minimal integrated side-lobe level. A theorem is derived that provides a closed-form analytical optimal solution that appears to be an extension of the Rayleigh quotient minimization for a possibly singular matrix in quadratic form. Such singularities are shown to occur in the problem of interest, but proofs for the optimum solution in these singular matrix cases could not be found in the literature. Next, an additional constraint is added to obtain beam patterns with desired 3 dB beamwidths, resulting in a nonconvex quadratically constrained quadratic program which is NP-hard. A semidefinite program and a Gaussian randomized semidefinite relaxation are used to determine feasible solutions arbitrarily close to the solution to the original problem. Theoretical and numerical analyses illustrate the impacts of changing the number of transmitters and orthogonal waveforms employed in the designs. Numerical comparisons are conducted to evaluate the proposed design approaches.',\n",
       "  'len': 1287},\n",
       " {'abstract': 'Multiple-input multiple-output (MIMO) radars offer higher resolution, better target detection, and more accurate target parameter estimation. Due to the sparsity of the targets in space-velocity domain, we can exploit Compressive Sensing (CS) to improve the performance of MIMO radars when the sampling rate is much less than the Nyquist rate. In distributed MIMO radars, block CS methods can be used instead of classical CS ones for more performance improvement, because the received signal in this group of MIMO radars is a block sparse signal in a basis. In this paper, two new methods are proposed to improve the performance of the block CS-based distributed MIMO radars. The first one is a new method for optimal energy allocation to the transmitters, and the other one is a new method for optimal design of the measurement matrix. These methods are based on the minimization of an upper bound of the sensing matrix block-coherence. Simulation results show an increase in the accuracy of multiple targets parameters estimation for both proposed methods.',\n",
       "  'len': 1069},\n",
       " {'abstract': 'The single sensor probability hypothesis density (PHD) and cardinalized probability hypothesis density (CPHD) filters have been developed in the literature using the random finite set framework. The existing multisensor extensions of these filters have limitations such as sensor order dependence, numerical instability or high computational requirements. In this paper we derive update equations for the multisensor CPHD filter. The multisensor PHD filter is derived as a special case. Exact implementation of the multisensor CPHD involves sums over all partitions of the measurements from different sensors and is thus intractable. We propose a computationally tractable approximation which combines a greedy measurement partitioning algorithm with the Gaussian mixture representation of the PHD. Our greedy approximation method allows the user to control the tradeoff between computational overhead and approximation accuracy.',\n",
       "  'len': 940},\n",
       " {'abstract': 'This paper evaluates methods of hierarchical skill analysis developed in aerospace to the problem of surgical skill assessment and modeling. The analysis employs tool motion data of Fundamental of Laparoscopic Skills (FLS) tasks collected from clinicians of various skill levels at three different clinical teaching hospitals in the United States. Outcomes are evaluated based on their ability to provide relevant information about the underlying processes across the entire system hierarchy including control, guidance and planning.',\n",
       "  'len': 544},\n",
       " {'abstract': 'A comprehensive review of the literature on manoeuvring target tracking for both uncluttered and cluttered measurements is presented. Various discrete-time dynamical models including non-random input, random-input and switching or hybrid system manoeuvre models are presented. The problem of manoeuvre detection is covered. Classical and current filtering methods for manoeuvre tracking are presented including multi-level process noise, input estimation, variable dimension filtering, two-stage filter, the interacting multiple model algorithm, and generalised pseudo-Bayesian algorithms. Various extensions of these algorithms to the case of cluttered measurements are also described and these include: joint manoeuvre and measurement association, probabilistic data association and multi-hypothesis tracking. Smoothing schemes, including IMM smoothing and batch expectation-maximisation using the Viterbi algorithm, are also described. The use of amplitude information for target measurement discrimination is discussed. It is noted that although many manoeuvre tacking techniques exist, the literature contains surprisingly few performance comparisons to guide the design engineer although a performance benchmark has recently been introduced.',\n",
       "  'len': 1258},\n",
       " {'abstract': \"This paper investigates the utilization of differential thrust to help a commercial aircraft with a damaged vertical stabilizer regain its lateral/directional stability. In the event of an aircraft losing its vertical stabilizer, the consequential loss of the lateral/directional stability and control is likely to cause a fatal crash. In this paper, an aircraft with a completely damaged vertical stabilizer is investigated, and a unique differential thrust based adaptive control approach is proposed to achieve a stable flight envelope. The propulsion dynamics of the aircraft is modeled as a system of differential equations with engine time constant and time delay terms to study the engine response time with respect to a differential thrust input. The proposed differential thrust control module is then presented to map the rudder input to differential thrust input. Model reference adaptive control based on the Lyapunov stability approach is implemented to test the ability of the damaged aircraft to track the model aircraft's (reference) response in an extreme scenario. Investigation results demonstrate successful application of such differential thrust approach to regain lateral/directional stability of a damaged aircraft with no vertical stabilizer. Finally, the conducted robustness and uncertainty analysis results conclude that the stability and performance of the damaged aircraft remain within desirable limits, and demonstrate a safe flight mission through the proposed adaptive control methodology.\",\n",
       "  'len': 1534},\n",
       " {'abstract': 'This paper contains an overview and summary on the achievements of the United Nations basic space science initiative in terms of donated and provided planetariums, astronomical telescopes, and space weather instruments, particularly operating in developing nations. This scientific equipment has been made available to respective host countries, particularly developing nations, through the series of twenty basic space science workshops, organized through the United Nations Programme on Space Applications since 1991. Organized by the United Nations, the European Space Agency (ESA), the National Aeronautics and Space Administration (NASA) of the United States of America, and the Japan Aerospace Exploration Agency (JAXA), the basic space science workshops were organized as a series of workshops that focused on basic space science (1991-2004), the International Heliophysical Year 2007 (2005-2009), and the International Space Weather Initiative (2010-2012) proposed by the Committee on the Peaceful Uses of Outer Space on the basis of discussions of its Scientific and Technical Subcommittee, as reflected in the reports of the Subcommittee.',\n",
       "  'len': 1159},\n",
       " {'abstract': 'We experimentally studied whispering-gallery modes(WGMs) and demonstrated resonance enhancement of optical forces evanescently exerted on dielectric microspheres. We showed that the resonant light pressure can be used for optical sorting of microparticles with extraordinary uniform resonant properties that is unachievable by conventional sorting techniques.',\n",
       "  'len': 370},\n",
       " {'abstract': 'This study presents a real-time guidance strategy for an unmanned aerial vehicles (UAVs) that can be used to enhance their flight endurance by utilizing {\\\\sl insitu} measurements of wind speeds and wind gradients. In these strategies, periodic adjustments are made in the airspeed and/or heading angle command, in level flights, for the UAV to minimize a projected power requirement. In this study, UAV dynamics are described by a three-dimensional dynamic point-mass model. A stochastic wind field model has been used to analyze the effect of the wind in the process. Onboard closed-loop trajectory tracking logics that follow airspeed vector commands are modeled using the method of feedback linearization. To evaluate the benefits of these strategies in enhancing UAV flight endurance, a reference strategy is introduced in which the UAV would follow the optimal airspeed command in a steady level flight under zero wind conditions. A performance measure is defined as the average power consumption with respect to no wind case. Different scenarios have been evaluated both over a specified time interval and over different initial heading angles of the UAV. A relative benefit criterion is then defined as the percentage improvement in the performance measure of a proposed strategy over that of the reference strategy. Extensive numerical simulations are conducted to show efficiency and applicability of the proposed algorithms. Results demonstrate possible power savings of the proposed real-time guidance strategies in level flights, by utilization of wind energy.',\n",
       "  'len': 1583},\n",
       " {'abstract': \"In order to avoid collisions with space debris, the near Earth orbit must be continuously scanned by either ground- or spaced-based facilities. For the low Earth orbit, radar telescopes are the workhorse for this task, especially due to their continuous availability. However, optical observation methods can deliver complementary information, especially towards high accuracy measurements. Passive-optical observations are inexpensive and can yield very precise information about the apparent position of the object in the sky via comparison with background stars. However, the object's distance from the observer is not readily accessible, which constitutes a major drawback of this approach for the precise calculation of the orbital elements. Two experimental methods have been devised to overcome this problem: Using two observatories a few kilometres apart, strictly simultaneous observations of the same object yield an accurate, instantaneous 3D position determination through measurement of the parallax. If only one observatory is available, a pulsed laser can be used in addition to the passive-optical channel to measure the distance to the object, in a similar fashion as used by the satellite laser ranging community. However, compared to conventional laser ranging, a stronger laser and more elaborate tracking algorithms are necessary. The two approaches can also be combined by illuminating the object with a pulsed laser from one observatory and measuring the return times at both observatories. These techniques are explored by German Aerospace Center in Stuttgart using its orbital debris research observatory, in cooperation with the Satellite Laser Ranging station in Graz and the Geodetic Observatory in Wettzell. This contribution will present some of the results and plans for further measurement campaigns.\",\n",
       "  'len': 1843},\n",
       " {'abstract': \"In the present paper we consider a 2D pantographic structure composed by two orthogonal families of Euler beams. Pantographic rectangular 'long' waveguides are considered in which imposed boundary displacements can induce the onset of traveling (possibly non-linear) waves. We performed numerical simulations concerning a set of dynamically interesting cases. The system undergoes large rotations which may involve geometrical non-linearities, possibly opening the path to appealing phenomena such as propagation of solitary waves. Boundary conditions dramatically influence the transmission of the considered waves at discontinuity surfaces. The theoretical study of this kind of objects looks critical, as the concept of pantographic 2D sheets seems to have promising possible applications in a number of fields, e.g. acoustic filters, vascular prostheses and aeronautic/aerospace panels.\",\n",
       "  'len': 901},\n",
       " {'abstract': \"The period between the creation of the cosmic microwave background at a redshift of ~1000 and the formation of the first stars and black holes that re-ionize the intergalactic medium at redshifts of 10-20 is currently unobservable. The baryonic component of the universe during this period is almost entirely neutral hydrogen, which falls into local regions of higher dark matter density. This seeds the formation of large-scale structures including the cosmic web that we see today in the filamentary distribution of galaxies and clusters of galaxies. The only detectable signal from these dark ages is the 21-cm spectral line of hydrogen, redshifted down to frequencies of approximately 10-100 MHz. Space-based observations of this signal will allow us to determine the formation epoch and physics of the first sources of ionizing radiation, and potentially detect evidence for the decay of dark matter particles. JPL is developing deployable low frequency antenna and receiver prototypes to enable both all-sky spectral measurements of neutral hydrogen and ultimately to map the spatial distribution of the signal as a function of redshift. Such observations must be done from space because of Earth's ionosphere and ubiquitous radio interference. A specific application of these technologies is the Dark Ages Radio Explorer (DARE) mission. This small Explorer class mission is designed to measure the sky-averaged hydrogen signal from the shielded region above the far side of the Moon. These data will complement ground-based radio observations of the final stages of intergalactic re-ionization at higher frequencies. DARE will also serve as a scientific percursor for space-based interferometry missions to image the distribution of hydrogen during the cosmic dark ages.\",\n",
       "  'len': 1788},\n",
       " {'abstract': 'In this paper, the optimization-based alignment (OBA) methods are investigated with main focus on the vector observations construction procedures for the strapdown inertial navigation system (SINS). The contributions of this study are twofold. First the OBA method is extended to be able to estimate the gyroscopes biases coupled with the attitude based on the construction process of the existing OBA methods. This extension transforms the initial alignment into an attitude estimation problem which can be solved using the nonlinear filtering algorithms. The second contribution is the comprehensive evaluation of the OBA methods and their extensions with different vector observations construction procedures in terms of convergent speed and steady-state estimate using field test data collected from different grades of SINS. This study is expected to facilitate the selection of appropriate OBA methods for different grade SINS.',\n",
       "  'len': 944},\n",
       " {'abstract': 'Space astronomy in the last 40 years has largely been done from spacecraft in low Earth orbit (LEO) for which the technology is proven and delivery mechanisms are readily available. However, new opportunities are arising with the surge in commercial aerospace missions. We describe here one such possibility: deploying a small instrument on the Moon. This can be accomplished by flying onboard the Indian entry to the Google Lunar X PRIZE competition, Team Indus mission, which is expected to deliver a nearly 30 kgs of payloads to the Moon, with a rover as its primary payload. We propose to mount a wide-field far-UV (130--180 nm) imaging telescope as a payload on the Team Indus lander. Our baseline operation is a fixed zenith pointing but with the option of a mechanism to allow observations of different attitudes. Pointing towards intermediate ecliptic latitude (50 deg or above) ensures that the Sun is at least 40 deg off the line of sight at all times. In this position, the telescope can cover higher galactic latitudes as well as parts of Galactic plane. The scientific objectives of such a prospective are delineated and discussed.',\n",
       "  'len': 1155},\n",
       " {'abstract': 'Railguns can convert large quantities of electrical energy into kinetic energy of the projectile. This was demon- strated by the 33 MJ muzzle energy shot performed in 2010 in the framework of the Office of Naval Research (ONR) electromag- netic railgun program. Since then, railguns are a prime candidate for future long range artillery systems. In this scenario, a heavy projectile (several kilograms) is accelerated to approx. 2.5 km/s muzzle velocity. While the primary interest for such a hypersonic projectile is the bombardment of targets being hundreds of kilometers away, they can also be used to counter airplane attacks or in other direct fire scenarios. In these cases, the large initial velocity significantly reduces the time to impact the target. In this study we investigate a scenario, where a future shipboard railgun installation delivers the same kinetic energy to a target as the explosive round of a contemporary European ship artillery system. At the same time the railgun outperforms the current artillery systems in range. For this scenario a first draft for the parameters of a railgun system were derived. For the flight-path of the projectile, trajectories for different launch angles were simulated and the aero-thermodynamic heating was estimated using engineering-tools developed within the German Aerospace Center (DLR). This enables the assessment of the feasibility of the different strike scenarios, as well as the identification of the limits of the technology. It is envisioned that this baseline design can be used as a helpful starting point for discussions of a possible electrical weaponization of future European warships.',\n",
       "  'len': 1674},\n",
       " {'abstract': 'We discuss two geosynchronous gravitational wave mission concepts, which we generically name gLISA. One relies on the science instrument hosting program onboard geostationary commercial satellites, while the other takes advantage of recent developments in the aerospace industry that result in dramatic satellite and launching vehicle cost reductions for a dedicated geosynchronous mission. To achieve the required level of disturbance free-fall onboard these large and heavy platforms we propose a \"two-stage\" drag-free system, which incorporates the Modular Gravitational Reference Sensor (MGRS) (developed at Stanford University) and does not rely on the use of micro-Newton thrusters. Although both mission concepts are characterized by different technical and programmatic challenges, individually they could be flown and operated at a cost significantly lower than those of previously envisioned gravitational wave missions. We estimate both mission concepts to cost less than 500M US$ each, and in the year 2015 we will perform at JPL a detailed selecting mission cost analysis.',\n",
       "  'len': 1096},\n",
       " {'abstract': 'In a typical MIMO radar scenario, transmit nodes transmit orthogonal waveforms, while each receive node performs matched filtering with the known set of transmit waveforms, and forwards the results to the fusion center. Based on the data it receives from multiple antennas, the fusion center formulates a matrix, which, in conjunction with standard array processing schemes, such as MUSIC, leads to target detection and parameter estimation. In MIMO radars with compressive sensing (MIMO-CS), the data matrix is formulated by each receive node forwarding a small number of compressively obtained samples. In this paper, it is shown that under certain conditions, in both sampling cases, the data matrix at the fusion center is low-rank, and thus can be recovered based on knowledge of a small subset of its entries via matrix completion (MC) techniques. Leveraging the low-rank property of that matrix, we propose a new MIMO radar approach, termed, MIMO-MC radar, in which each receive node either performs matched filtering with a small number of randomly selected dictionary waveforms or obtains sub-Nyquist samples of the received signal at random sampling instants, and forwards the results to a fusion center. Based on the received samples, and with knowledge of the sampling scheme, the fusion center partially fills the data matrix and subsequently applies MC techniques to estimate the full matrix. MIMO-MC radars share the advantages of the recently proposed MIMO-CS radars, i.e., high resolution with reduced amounts of data, but unlike MIMO-CS radars do not require grid discretization. The MIMO-MC radar concept is illustrated through a linear uniform array configuration, and its target estimation performance is demonstrated via simulations.',\n",
       "  'len': 1766},\n",
       " {'abstract': 'Due to the complexity and inconstancy of the space environment, accurate mathematical models for spacecraft rendezvous are difficult to obtain, which consequently complicates the control tasks. In this paper, a linearized time-variant plant model with external perturbations is adopted to approximate the real circumstance. To realize the robust stability with optimal performance cost, a partially independent control scheme is proposed, which consists of a robust anti-windup controller for the in-plane motion and a ${{H}_{\\\\infty}}$ controller for the out-of-plane motion. Finally, a rendezvous simulation is given to corroborate the practicality and advantages of the partially independent control scheme over a coupled control scheme.',\n",
       "  'len': 750},\n",
       " {'abstract': 'A key issue in developing pendular Fabry-Perot interferometers as very accurate displacement measurement devices, is the noise level. The Fabry-Perot pendulums are the most promising device to detect gravitational waves, and therefore the background and the internal noise should be accurately measured and reduced. In fact terminal masses generates additional internal noise mainly due to thermal fluctuations and vibrations. We propose to exploit the reflectivity change, that occurs in some special points, to monitor the pendulums free oscillations and possibly estimate the noise level. We find that in spite of long transients, it is an effective method for noise estimate. We also prove that to only retain the sequence of escapes, rather than the whole time dependent dynamics, entails the main characteristics of the phenomenon. Escape times could also be relevant for future gravitational wave detector developments.',\n",
       "  'len': 937},\n",
       " {'abstract': 'The ability to simulate a reentry vehicle plasma layer and the radio wave interaction with that layer, is crucial to the design of aerospace vehicles when the analysis of radio communication blackout is required. Results of aerothermal heating, plasma generation and electromagnetic wave propagation over a reentry vehicle are presented in this paper. Simulation of a magnetic window radio communication blackout mitigation method is successfully demonstrated.',\n",
       "  'len': 471},\n",
       " {'abstract': \"This volume contains the proceedings of the 2nd French Singaporean Workshop on Formal Methods and Applications (FSFMA'14). The workshop was held in Singapore on May 13th, 2014, as a satellite event of the 19th International Symposium on Formal Methods (FM'14). FSFMA aims at sharing research interests and launching collaborations in the area of formal methods and their applications. The scientific subject of the workshop covers (but is not limited to) areas such as formal specification, model checking, verification, program analysis/transformation, software engineering, and applications in major areas of computer science, including aeronautics and aerospace. The workshop brings together researchers and industry R&D experts from France, Singapore and other countries together to exchange their knowledge, discuss their research findings, and explore potential collaborations. This volume contains eight contributions: four invited talks and four regular papers.\",\n",
       "  'len': 980},\n",
       " {'abstract': \"The James Webb Space Telescope (JWST) Optical Simulation Testbed (JOST) is a tabletop workbench to study aspects of wavefront sensing and control for a segmented space telescope, including both commissioning and maintenance activities. JOST is complementary to existing optomechanical testbeds for JWST (e.g. the Ball Aerospace Testbed Telescope, TBT) given its compact scale and flexibility, ease of use, and colocation at the JWST Science & Operations Center. We have developed an optical design that reproduces the physics of JWST's three-mirror anastigmat using three aspheric lenses; it provides similar image quality as JWST (80% Strehl ratio) over a field equivalent to a NIRCam module, but at HeNe wavelength. A segmented deformable mirror stands in for the segmented primary mirror and allows control of the 18 segments in piston, tip, and tilt, while the secondary can be controlled in tip, tilt and x, y, z position. This will be sufficient to model many commissioning activities, to investigate field dependence and multiple field point sensing & control, to evaluate alternate sensing algorithms, and develop contingency plans. Testbed data will also be usable for cross-checking of the WFS&C Software Subsystem, and for staff training and development during JWST's five- to ten-year mission.\",\n",
       "  'len': 1316},\n",
       " {'abstract': 'Coating Brownian noise is the dominant noise term, in a frequency band from a few tens to a few hundreds Hz, for all Earth-bound detectors of gravitational waves. Minimizing such noise is mandatory to increase the visibility distance of these instruments, and eventually reach their quantum limit. Several strategies are possible to achieve this goal. Layer thickness optimization is the simplest option, yielding a sensible noise reduction with limited technological challenges. Experimental results confirm the accuracy of the underlying theory, and the robustness of the design.',\n",
       "  'len': 592},\n",
       " {'abstract': 'We are developing a silica-aerogel-based cosmic dust collector for use in the Tanpopo experiment to be conducted on the International Space Station. The mass production of simple two-layer hydrophobic aerogels was undertaken in a contamination-controlled environment, yielding more than 100 undamaged products. The collector, comprising an aerogel tile and holder panel, was designed to resist launch vibration and to conform to an exposure attachment. To this end, a box-framing aerogel with inner and outer densities of 0.01 and 0.03 g/cm$^3$, respectively, was fabricated. The aerogel mounted in the panel passed random vibration tests at the levels of the acceptance and qualification tests for launch. It also withstood the pressure changes expected in the airlock on the International Space Station.',\n",
       "  'len': 816},\n",
       " {'abstract': 'Common ISAR radar images and signals can be reconstructed from much fewer samples than the sampling theorem requires since they are usually sparse. Unavailable randomly positioned samples can result from heavily corrupted parts of the signal. Since these samples can be omitted and declared as unavailable, the application of the compressive sensing methods in the recovery of heavily corrupted signal and radar images is possible. A\\\\ simple direct method for the recovery of unavailable signal samples and the calculation of the restored ISAR image is reviewed. An analysis of the noise influence is performed. For fast maneuvering ISAR targets the sparsity property is lost since the ISAR image is blurred. A nonparametric quadratic time-frequency representations based method is used to restore the ISAR image sparsity. However, the linear relation between the signal and the sparsity domain transformation is lost. A recently proposed gradient recovery algorithm is adapted for this kind of analysis. It does not require the linear relation of the signal and its sparsity domain transformation in the process of unavailable data recovery. The presented methods and results are tested on several numerical examples proving the expected accuracy and improvements.',\n",
       "  'len': 1276},\n",
       " {'abstract': \"Metals are the most common materials used in space technology. Metal structures, while used in space, are subjected to the full spectrum of the electromagnetic radiation together with particle irradiation. Hence, they undergo degradation. Future space missions are planned to proceed in the interplanetary space, where the protons of the solar wind play a very destructive role on metallic surfaces. Unfortunately, their real degradation behavior is to a great extent unknown. Our aim is to predict materials' behavior in such a destructive environment. Therefore both, theoretical and experimental studies are performed at the German Aerospace Center (DLR) in Bremen, Germany. Here, we report the theoretical results of those studies. We examine the process of H2-bubble formation on metallic surfaces. H2-bubbles are metal caps filled with Hydrogen molecular gas resulting from recombination processes of the metal free electrons and the solar protons. A thermodynamic model of the bubble growth is presented. Our model predicts e.g. the velocity of that growth and the reflectivity of foils populated by bubbles. Formation of bubbles irreversibly changes the surface quality of irradiated metals. Thin metallic films are especially sensitive for such degradation processes. They are used e.g. in the solar sail propulsion technology. The efficiency of that technology depends on the thermo-optical properties of the sail materials. Therefore, bubble formation processes have to be taken into account for the planning of long-term solar sail missions.\",\n",
       "  'len': 1564},\n",
       " {'abstract': 'Hybrid aerial-terrestrial communication networks based on Low Altitude Platforms (LAPs) are expected to optimally meet the urgent communication needs of emergency relief and recovery operations for tackling large scale natural disasters. The energy-efficient operation of such networks is important given the fact that the entire network infrastructure, including the battery operated ground terminals, exhibits requirements to operate under power-constrained situations. In this paper, we discuss the design and evaluation of an adaptive cooperative scheme intended to extend the survivability of the battery operated aerial-terrestrial communication links. We propose and evaluate a real-time adaptive cooperative transmission strategy for dynamic selection between direct and cooperative links based on the channel conditions for improved energy efficiency. We show that the cooperation between mobile terrestrial terminals on the ground could improve the energy efficiency in the uplink depending on the temporal behavior of the terrestrial and the aerial uplink channels. The corresponding delay in having cooperative (relay-based) communications with relay selection is also addressed. The simulation analysis corroborates that the adaptive transmission technique improves the overall energy efficiency of the network whilst maintaining low latency enabling real time applications.',\n",
       "  'len': 1398},\n",
       " {'abstract': 'Cyber-Physical Systems (CPS) have the promise of presenting the next evolution in computing with potential applications that include aerospace, transportation, robotics, and various automation systems. These applications motivate advances in the different sub-fields of CPS (e.g. mobile computing and communication, control, and vision). However, deploying and testing complete CPSs is known to be a complex and expensive task. In this paper, we present the design, implementation, and evaluation of Up and Away (UnA): a testbed for Cyber-Physical Systems that use UAVs as their physical component. UnA aims at abstracting the control of physical components of the system to reduce the complexity of UAV oriented Cyber-Physical Systems experiments. In addition, UnA provides an API to allow for converting CPS simulations into physical experiments using a few simple steps. We present a case study bringing a mobile-camera-based surveillance system simulation to life using UnA.',\n",
       "  'len': 989},\n",
       " {'abstract': 'Web-based Automated Process Control systems are a new type of applications that use the Internet to control industrial processes with the access to the real-time data. Supervisory control and data acquisition (SCADA) networks contain computers and applications that perform key functions in providing essential services and commodities (e.g., electricity, natural gas, gasoline, water, waste treatment, transportation) to all Americans. As such, they are part of the nation s critical infrastructure and require protection from a variety of threats that exist in cyber space today.',\n",
       "  'len': 592},\n",
       " {'abstract': 'When 60 de-interlaced fields per second digital uncompressed video is streamed to a computer, some video fields are lost and not able to be stored on a computer s hard drive successfully. Additionally, this problem amplifies once multiple video sources are deployed. If it is possible to stream digital uncompressed video without dropped video fields, then a sophisticated computer analysis of the transmitted via IEEE 1394a connection video is possible. Such process is used in biomechanics when it is important to analyze athletes performance via streaming digital uncompressed video to a computer and then analyzing it. If a loss of video fields occurs, then a quality analysis of video is not possible.',\n",
       "  'len': 717},\n",
       " {'abstract': 'When there is a possibility to wirelessly stream video over a network, a sophisticated computer analysis of the transmitted video is possible. Such process is used in biomechanics when it is important to analyze athletes performance via streaming digital uncompressed video to a computer and then analyzing it using specific software such as Arial Performance Analysis Systems or Dartfish. This manuscript presents some approaches and challenges in streaming video as well as some applications of Information Technology in biomechanics. An example of how scientists from Indiana State University approached the wireless transmission of video is also introduced.',\n",
       "  'len': 672},\n",
       " {'abstract': 'Unified Modeling Language (UML) is currently accepted as the standard for modeling (object-oriented) software, and its use is increasing in the aerospace industry. Verification and Validation of complex software developed according to UML is not trivial due to complexity of the software itself, and the several different UML models/diagrams that can be used to model behavior and structure of the software. This paper presents an approach to transform up to three different UML behavioral diagrams (sequence, behavioral state machines, and activity) into a single Transition System to support Model Checking of software developed in accordance with UML. In our approach, properties are formalized based on use case descriptions. The transformation is done for the NuSMV model checker, but we see the possibility in using other model checkers, such as SPIN. The main contribution of our work is the transformation of a non-formal language (UML) to a formal language (language of the NuSMV model checker) towards a greater adoption in practice of formal methods in software development.',\n",
       "  'len': 1096},\n",
       " {'abstract': 'Supercritical fluids near the critical point are characterized by liquid-like densities and gas-like transport properties. These features are purposely exploited in different contexts ranging from natural products extraction/fractionation to aerospace propulsion. Large part of studies concerns this last context, focusing on the dynamics of supercritical fluids at high Mach number where compressibility and thermodynamics strictly interact. Despite the widespread use also at low Mach number, the turbulent mixing properties of slightly supercritical fluids have still not investigated in detail in this regime. This topic is addressed here by dealing with Direct Numerical Simulations (DNS) of a coaxial jet of a slightly supercritical Van der Waals fluid. Since acoustic effects are irrelevant in the Low Mach number conditions found in many industrial applications, the numerical model is based on a suitable low-Mach number expansion of the governing equation. According to experimental observations, the weakly supercritical regime is characterized by the formation of finger-like structures-- the so-called ligaments --in the shear layers separating the two streams. The mechanism of ligament formation at vanishing Mach number is extracted from the simulations and a detailed statistical characterization is provided. Ligaments always form whenever a high density contrast occurs, independently of real or perfect gas behaviors. The difference between real and perfect gas conditions is found in the ligament small-scale structure. More intense density gradients and thinner interfaces characterize the near critical fluid in comparison with the smoother behavior of the perfect gas. A phenomenological interpretation is here provided on the basis of the real gas thermodynamics properties.',\n",
       "  'len': 1810},\n",
       " {'abstract': 'Recently, the spinning tethered system is regarded as a typical and fundamental space structure attracting great interest of the aerospace engineers, and has been discussed primarily for specific space missions in past decades, including on-orbit capture and propellantless orbit transfer etc. The present work studies the dynamical behaviours of a fast spinning tethered binary system under central gravitational field, and derives principles of the basic laws of orbital maneuver. Considering the characteristics of coupled librational and orbital motions, an averaging method is introduced to deal with the slow-fast system equation, thus a definite equivalent model is derived. The general orbit motion is completely determined analytically, including the orbit geometry, periodicity, conversations and moving region etc. Since the possibility of orbit control using tether reaction has been proved by previous studies, special attention is paid to the transportation mode of angular momentum and mechanical energy between the orbit and libration. The effect of tether length change on the orbit shape is verified both in the averaged model and original model. The results show the orbit angular momentum and mechanical energy can be controlled independently, and the operating principles of tether reactions are derived for special modification of orbit shape.',\n",
       "  'len': 1376},\n",
       " {'abstract': 'This paper presents flow simulation results of the EUROLIFT DLR-F11 multi-element wing configuration, obtained with a highly scalable finite element solver, PHASTA. This work was accomplished as a part of the 2nd high lift prediction workshop. In-house meshes were constructed with increasing mesh density for analysis. A solution adaptive approach was used as an alternative and its effectiveness was studied by comparing its results with the ones obtained with other meshes. Comparisons between the numerical solution obtained with unsteady RANS turbulence model and available experimental results are provided for verification and discussion. Based on the observations, future direction for adaptive research and simulations with higher fidelity turbulence models is outlined.',\n",
       "  'len': 790},\n",
       " {'abstract': 'Multi-element wings are popular in the aerospace community due to their high lift performance. Turbulent flow simulations of these configurations require very fine mesh spacings especially near the walls, thereby making use of a boundary layer mesh necessary. However, it is difficult to accurately determine the required mesh resolution a priori to the simulations. In this paper we use an anisotropic adaptive meshing approach including adaptive control of elements in the boundary layers and study its effectiveness for two multi-element wing configurations. The results are compared with experimental data as well as nested refinements to show the efficiency of adaptivity driven by error indicators, where superior resolution in wakes and near the tip region through adaptivity are highlighted.',\n",
       "  'len': 810},\n",
       " {'abstract': 'The \"Airships: A New Horizon for Science\" study at the Keck Institute for Space Studies investigated the potential of a variety of airships currently operable or under development to serve as observatories and science instrumentation platforms for a range of space, atmospheric, and Earth science. The participants represent a diverse cross-section of the aerospace sector, NASA, and academia. Over the last two decades, there has been wide interest in developing a high altitude, stratospheric lighter-than-air (LTA) airship that could maneuver and remain in a desired geographic position (i.e., \"station-keeping\") for weeks, months or even years. Our study found considerable scientific value in both low altitude (< 40 kft) and high altitude (> 60 kft) airships across a wide spectrum of space, atmospheric, and Earth science programs. Over the course of the study period, we identified stratospheric tethered aerostats as a viable alternative to airships where station-keeping was valued over maneuverability. By opening up the sky and Earth\\'s stratospheric horizon in affordable ways with long-term flexibility, airships allow us to push technology and science forward in a project-rich environment that complements existing space observatories as well as aircraft and high-altitude balloon missions.',\n",
       "  'len': 1316},\n",
       " {'abstract': 'This paper serves as a review of our recent new DNS study on physics of late boundary layer transition. This includes mechanism of the large coherent vortex structure formation, small length scale generation and flow randomization. The widely spread concept vortex breakdown to turbulence,which was considered as the last stage of flow transition, is not observed and is found theoretically incorrect. The classical theory on boundary layer transition is challenged and we proposed a new theory with five steps, i.e. receptivity, linear instability, large vortex formation, small length scale generation, loss of symmetry and randomization to turbulence. We have also proposed a new theory about turbulence generation. The new theory shows that all small length scales (turbulence) are generated by shear layer instability which is produced by large vortex structure with multiple level vortex rings, multiple level sweeps and ejections, and multiple level negative and positive spikes near the laminar sub-layers.Therefore, turbulence is not generated by vortex breakdown but rather positive and negative spikes and consequent high shear layers. Shear layer instability is considered as the mother of turbulence. This new theory may give a universal mechanism for turbulence generation and sustenance which shows that the energy is brought by large vortex structure through multiple level sweeps.',\n",
       "  'len': 1408},\n",
       " {'abstract': 'This paper is devoted to the investigation of the origin and mechanism of randomization in late boundary layer transition over a flat plate without pressure gradient. The flow randomization is a crucial phase before flow transition to the turbulent state. According to existing literatures, the randomization was caused by the big background noises and non-periodic spanwise boundary conditions. It was assumed that the large ring structure is affected by background noises first, and then the change of large ring structure affects the small length scales quickly, which directly leads to randomization and formation of turbulence. However, by careful analysis of our high order DNS results, we believe that the internal instability of multiple ring cycles structure is the main reason. What we observed is that randomization begins when the third cycle overlaps the first and second cycles. A significant asymmetric phenomenon is originated from the second cycle in the middle of both streamwise and spanwise directions. More technically, a visible asymmetric phenomenon in the middle vortex ring cycle starts at time step t=16.25T and x=838.9{\\\\delta}in where the top and bottom level rings are still completely symmetric. The non-symmetric structure of middle level ring affects the small length scale in boundary layer bottom quickly. The randomization phenomenon spreads to top level through ejections. Finally, the whole flow domain becomes randomized. A hypothesis of C- and K-types shift is given as a possible mechanism of flow randomization.',\n",
       "  'len': 1562},\n",
       " {'abstract': 'The small vortex generation is a key issue of the mechanism for late flow transition and turbulence generation. It was widely accepted that small length vortices were generated by large vortex breakdown. According to our recent DNS, we find that the hairpin vortex structure is very stable and never breaks down to small pieces. On the other hand, we recognize that there are strong positive spikes besides the ring neck in the spanwise direction. The strongly positive spikes are caused by second sweeps which are generated by perfectly circular and perpendicularly standing vortex rings. The second sweep brings energy from the invisid region downdraft to the bottom of the boundary layers, which generates high shear layers around the positive spikes.Since the high shear layer is not stable, all small length scales (turbulence) are generated around high shear layers especially near the wall surface (bottom of boundary layers). This happens near the ring neck in the streamwise direction and besides the original vortex legs in the spanwise direction.The small length scales then rise up from the wall surface and travel to the upper boundary layer.',\n",
       "  'len': 1166},\n",
       " {'abstract': 'In this paper, the implicitly implemented LES method and fifth order bandwidth optimized WENO scheme are used to make comprehensive studies on the separation topology of the MVG controlled flow at M=2.5 and Re{\\\\theta}=5760. Experiments are also made to verify the prediction of the computation. Analyses are conducted on three categories of the topology: the surface separation, cross-section separation and the three dimensional structure of the vortices. A complete description about the separation topology and a series of new findings are obtained. Among them, a pair of spiral point is first predicted by the computation and verified by the experiment. A corresponding new vortex model with 7 vortex tubes is presented also.',\n",
       "  'len': 740},\n",
       " {'abstract': 'In this study, we investigate the interaction between vortex rings behind MVG and the oblique shocks in the MVG controlled ramp flow at M=2.5 and Req=5760. Implicit large eddy simulation (ILES) method is used by solving the unfiltered form of the Navier-Stokes equations with the 5th order Bandwidth-optimized WENO scheme. The fully developed inflow is given by a series of turbulent profiles obtained from previous DNS simulation. It shows that the ring structure does not break down and keeps its topology after penetrating the strong shock wave and the oblique shocks is influenced a lot by the induced flow field from rings. The bump of the 3D shock wave surface is discovered and its mechanism is explained.',\n",
       "  'len': 723},\n",
       " {'abstract': 'The goal of this work is to develop a new universal high order subroutine for shock boundary layer interaction. First, an effective shock/discontinuity detector has been developed.The detector has two steps.The first step is to check the ratio of the truncation errors on the coarse and fine grids and the second step is to check the local ratio of the left and right slopes. The currently popular shock/discontinuity detectors can detect shock, but mistake high frequency waves and critical points as shock and then damp the physically important high frequency waves.Preliminary results show the new shock/discontinuity detector is very delicate and can detect all shocks including strong, weak and oblique shocks or discontinuity in function and the first, second, and third order derivatives without artificial constants, but never mistake high frequency waves and critical points, expansion waves as shock. This will overcome the bottle neck problem with numerical simulation for the shock-boundary layer interaction, shock-acoustic interaction, image process, porous media flow, multiple phase flow and anywhere the high frequency waves are important, but discontinuity exists and is mixed with high frequency waves. After detecting the shock we can then use one side high order scheme for shocks and high order central compact scheme for the rest if the shock is appropriately located. Then a high order universal subroutine for finite difference method is developed which can be used for any finite difference code for accurate numerical derivatives.',\n",
       "  'len': 1568},\n",
       " {'abstract': 'The compact scheme has high order accuracy and high resolution, but cannot be used to capture the shock. WENO is a great scheme for shock capturing, but is too dissipative for turbulence and small length scales. We developed a modified upwinding compact scheme which uses an effective shock detector to block compact scheme to cross the shock and a control function to mix the flux with WENO scheme near the shock. The new scheme makes the original compact scheme able to capture the shock sharply and, more important, keep high order accuracy and high resolution in the smooth area, which is particularly important for shock boundary layer and shock acoustic interactions. This work is a continuation to modify the control function for the modified up-winding compact scheme (MUCS). Numerical results show the scheme is successful for 2-D Euler.',\n",
       "  'len': 857},\n",
       " {'abstract': 'This paper illustrates the mechanism of U-shaped vortex formation which is found both by experiment and DNS. The main goal of this paper is to explain how the U-shaped vortex is formed and further develops. According to the results obtained by our direct numerical simulation with high order accuracy, the U-shaped vortex is part of the coherent vortex structure and is actually the tertiary streamwise vortices induced by the secondary vortices. The new finding is quite different from existing theories which describe that the U-shaped vortex is newly formed as the head of young turbulence spot and finally break down to small pieces. In addition, we find that the U-shaped vortex has the same vorticity sign as the original {\\\\lambda}-shaped vortex tube legs and serves as a second neck to supply vorticity to the ringlike vortex when the original vortex tube is stretched and multiple rings are generated.',\n",
       "  'len': 920},\n",
       " {'abstract': 'The mechanism of randomization in late boundary layer transition is a key issue of late boundary layer transition and turbulence theory. We studied the mechanism carefully by high order DNS. The randomization was originally considered as a result of large background noise and non-periodic spanwise boundary conditions. It was addressed that the large ring structure is affected by background noises first and then the change of large ring structure affects the small length scale quickly, which directly leads to randomization and formation of turbulence. However, what we observed is that the loss of symmetry starts from the middle level rings while the top and bottom rings are still symmetric. The nonsymmetric structure of second level rings will influence the small length scales at the boundary layer bottom quickly. The symmetry loss at the bottom of the boundary layer is quickly spread to up level through ejections. This will lead to randomization of the whole flow field. Therefore, the internal instability of multiple level ring structure, especially the middle ring cycles, is the main reason for flow randomization, but not the background noise. A hypothesis is given that the loss of symmetry may be caused by the shift from C-type transition to K-type transition or reverses.',\n",
       "  'len': 1305},\n",
       " {'abstract': 'This paper investigates adaptive zero reaction motion control for free-floating space manipulators with uncertain kinematics and dynamics. The challenge in deriving the adaptive reaction null-space (RNS) based control scheme is that it is difficult to obtain a linear expression, which is the basis of the adaptive control. The main contribution of this paper is that we skillfully obtain such a linear expression, based on which, an adaptive version of the RNS-based controller (referred to as the adaptive zero reaction motion controller in the sequel) is developed at the velocity level, taking into account both the kinematic and dynamic uncertainties. It is shown that the proposed controller achieves both the spacecraft attitude regulation and end-effector trajectory tracking. The performance of the proposed adaptive controller is shown by numerical simulations with a planar 3-DOF (degree-of-freedom) space manipulator.',\n",
       "  'len': 940},\n",
       " {'abstract': 'In this paper, a reduced-rank scheme with joint iterative optimization is presented for direction of arrival estimation. A rank-reduction matrix and an auxiliary reduced-rank parameter vector are jointly optimized to calculate the output power with respect to each scanning angle. Subspace algorithms to estimate the rank-reduction matrix and the auxiliary vector are proposed. Simulations are performed to show that the proposed algorithms achieve an enhanced performance over existing algorithms in the studied scenarios.',\n",
       "  'len': 534},\n",
       " {'abstract': 'Evolving a software process model without a retrospective and, in consequence, without an understanding of the process evolution, can lead to severe problems for the software development organization, e.g., inefficient performance as a consequence of the arbitrary introduction of changes or difficulty in demonstrating compliance to a given standard. Capturing information on the rationale behind changes can provide a means for better understanding process evolution. This article presents the results of an exploratory study with the goal of understanding the nature of process changes in a given context. It presents the most important issues that motivated process engineers changing important aerospace software process standards during an industrial project. The study is part of research work intended to incrementally define a systematic mechanism for process evolution supported by rationale information.',\n",
       "  'len': 925},\n",
       " {'abstract': 'Aligning the activities of an organization with its business goals is a challenging task that is critical for success. Alignment in a multi-organizational setting requires the integration of different internal or external organizational units. The anticipated benefits of multi-organizational alignment consist of clarified contributions and increased transparency of the involved organizational units. The GQM+Strategies approach provides mechanisms for explicitly linking goals and strategies within an organization and is based on goal-oriented measurement. This paper presents the process and first-hand experience of applying GQM+Strategies in a multi-organizational setting from the aerospace industry. Additionally, the resulting GQM+Strategies grid is sketched and selected parts are discussed. Finally, the results are reflected on and an overview of future work is given.',\n",
       "  'len': 892},\n",
       " {'abstract': 'In the most common space solar power (SSP) system architectures, solar energy harvested by large satellites in geostationary orbit is transmitted to Earth via microwave radiation. Currently, only limited information about the interactions of microwave beams with energy densities of several tens to hundreds of W/m$^2$ with the different layers of the atmosphere is available. Governmental bodies will likely require detailed investigations of safety and atmospheric effects of microwave power beams before issuing launch licenses for SSP satellite systems. This paper proposes to collect representative and comprehensive data of the interaction of power beams with the atmosphere by extending the infrastructure of the High Frequency Active Auroral Research Program (HAARP) facility in Alaska, USA. Estimates of the transmission infrastructure performance as well as measurement devices and scientific capabilities of possible upgrade scenarios will be discussed. The proposed upgrade of the HAARP facility is expected to deliver a wealth of data and information which could serve as a decision base for governmental launch licensing of SSP satellites, and which can be used in addition to deepen public acceptance of SSP as a large-scale renewable energy source. Copyright 2014 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.',\n",
       "  'len': 1667},\n",
       " {'abstract': \"The X-Ray Telescope (XRT) onboard the Hinode satellite, launched 23 September 2006 by the Japanese Aerospace Exploration Agency (JAXA) is a joint mission between Japan, the United States, and the United Kingdom to study the solar corona. In particular XRT was designed to study solar plasmas with temperatures between 1 and 10 MK with $\\\\approx1''$ pixels ($\\\\approx2''$ resolution). Prior to analysis, the data product from this instrument must be properly calibrated and data values quantified in order to assess accurately the information contained within. We present here the standard methods of calibration for these data. The calibration is performed on an empirical basis which uses the least complicated correction that accurately describes the data while suppressing spurious features. By analyzing the uncertainties remaining in the data after calibration, we conclude that the procedure is successful, as the remaining uncertainty after calibration is dominated by photon noise. This calibration software is available in the Solar Soft software library.\",\n",
       "  'len': 1073},\n",
       " {'abstract': 'Helical structures, almost ubiquitous in biological systems, have inspired the design and manufacturing of helical devices with applications in nanoelecromechanical systems (NEMS), morphing structures, optoelectronics, micro-robotics and drug delivery devices. Meanwhile, multi-stable structures, represented by the Venus flytrap and slap bracelet, have attracted increasing attention due to their applications in making artificial muscles, bio-inspired robots, deployable aerospace components and energy harvesting devices. Here we show that the mechanical anisotropy pertinent to helical deformation, together with geometric nonlinearity associated with multi-stability, can lead to novel selection principle of the geometric shape and multi-stability in spontaneous helical ribbons. Simple table-top experiments were also performed to illustrate the working principle. Our work will promote understanding of spontaneous curling, twisting, wrinkling of thin objects and their instabilities, and serve as a tool in developing functional structures and devices with tunable, morphing geometries and smart actuation mechanism that can be applied in a spectrum of areas.',\n",
       "  'len': 1179},\n",
       " {'abstract': 'This paper presents knowledge-aided space-time adaptive processing (KA-STAP) algorithms that exploit the low-rank dominant clutter and the array geometry properties (LRGP) for airborne radar applications. The core idea is to exploit the fact that the clutter subspace is only determined by the space-time steering vectors, {red}{where the Gram-Schmidt orthogonalization approach is employed to compute the clutter subspace. Specifically, for a side-looking uniformly spaced linear array, the} algorithm firstly selects a group of linearly independent space-time steering vectors using LRGP that can represent the clutter subspace. By performing the Gram-Schmidt orthogonalization procedure, the orthogonal bases of the clutter subspace are obtained, followed by two approaches to compute the STAP filter weights. To overcome the performance degradation caused by the non-ideal effects, a KA-STAP algorithm that combines the covariance matrix taper (CMT) is proposed. For practical applications, a reduced-dimension version of the proposed KA-STAP algorithm is also developed. The simulation results illustrate the effectiveness of our proposed algorithms, and show that the proposed algorithms converge rapidly and provide a SINR improvement over existing methods when using a very small number of snapshots.',\n",
       "  'len': 1319},\n",
       " {'abstract': 'The paper focuses on the calibration of serial industrial robots using partial pose measurements. In contrast to other works, the developed advanced robot calibration technique is suitable for geometrical and elastostatic calibration. The main attention is paid to the model parameters identification accuracy. To reduce the impact of measurement errors, it is proposed to use directly position measurements of several points instead of computing orientation of the end-effector. The proposed approach allows us to avoid the problem of non-homogeneity of the least-square objective, which arises in the classical identification technique with the full-pose information. The developed technique does not require any normalization and can be efficiently applied both for geometric and elastostatic identification. The advantages of a new approach are confirmed by comparison analysis that deals with the efficiency evaluation of different identification strategies. The obtained results have been successfully applied to the elastostatic parameters identification of the industrial robot employed in a machining work-cell for aerospace industry.',\n",
       "  'len': 1154},\n",
       " {'abstract': \"Posynomials are nonnegative combinations of monomials with possibly fractional and both positive and negative exponents. Posynomial models are widely used in various engineering design endeavors, such as circuits, aerospace and structural design, mainly due to the fact that design problems cast in terms of posynomial objectives and constraints can be solved efficiently by means of a convex optimization technique known as geometric programming (GP). However, while quite a vast literature exists on GP-based design, very few contributions can yet be found on the problem of identifying posynomial models from experimental data. Posynomial identification amounts to determining not only the coefficients of the combination, but also the exponents in the monomials, which renders the identification problem numerically hard. In this draft, we propose an approach to the identification of multivariate posynomial models, based on the expansion on a given large-scale basis of monomials. The model is then identified by seeking coefficients of the combination that minimize a mixed objective, composed by a term representing the fitting error and a term inducing sparsity in the representation, which results in a problem formulation of the ``square-root LASSO'' type, with nonnegativity constraints on the variables. We propose to solve the problem via a sequential coordinate-descent scheme, which is suitable for large-scale implementations.\",\n",
       "  'len': 1454},\n",
       " {'abstract': 'Purpose: This paper presents a full fourth-order model of the gravity gradient torque of spacecraft around asteroids by taking into consideration of the inertia integrals of the spacecraft up to the fourth order, which is an improvement of the previous fourth-order model of the gravity gradient torque. Design, methodology and approach: The fourth-order gravitational potential of the spacecraft is derived based on Taylor expansion. Then the expression of the gravity gradient torque in terms of gravitational potential derivatives is derived. By using the formulation of the gravitational potential, explicit formulations of the full fourth-order gravity gradient torque are obtained. Then a numerical simulation is carried out to verify our model. Findings: We find that our model is more sound and precise than the previous fourth-order model due to the consideration of higher-order inertia integrals of the spacecraft. Numerical simulation results show that the motion of the previous fourth-order model is quite different from the exact motion, while our full fourth-order model fits the exact motion very well. Our full fourth-order model is precise enough for high-precision attitude dynamics and control around asteroids. Practical implications: This high-precision model is of importance for the future asteroids missions for scientific explorations and near-Earth objects mitigation. Originality and value: In comparison with the previous model, a gravity gradient torque model around asteroids that is more sound and precise is established. This model is valuable for high-precision attitude dynamics and control around asteroids.',\n",
       "  'len': 1655},\n",
       " {'abstract': 'This paper is concerned with performance analysis for data association, in a target tracking environment. Effects of misassociation are considered in a simple (linear) multiscan framework so as to provide closed-form expressions of the probability of correct association. In this paper, we focus on the development of explicit approximations of this probability. Via rigorous calculations the effect of dimensioning parameters (number of scans, false measurement positions or densities) is analyzed, for various modelings of the false measurements. Remarkably, it is possible to derive very simple expressions of the probability of correct association which are independent of the scenario kinematic parameters.',\n",
       "  'len': 722},\n",
       " {'abstract': \"All spacecraft attitude estimation methods are based on Wahba's optimization problem. This problem can be reduced to finding the largest eigenvalue and the corresponding eigenvector for Davenport's $K$-matrix. Several iterative algorithms, such as QUEST and FOMA, were proposed, aiming at reducing the computational cost. But their computational time is unpredictable because the iteration number is not fixed and the solution is not accurate in theory. Recently, an analytical solution, ESOQ was suggested. The advantages of analytical solutions are that their computational time is fixed and the solution should be accurate in theory if there is no numerical error. In this paper, we propose a different analytical solution to the Wahba's problem. We use simple and easy to be verified examples to show that this method is numerically more stable than ESOQ, potentially faster than QUEST and FOMA. We also use extensive simulation test to support this claim.\",\n",
       "  'len': 971},\n",
       " {'abstract': 'BRITE-Constellation (where BRITE stands for BRIght Target Explorer) is an international nanosatellite mission to monitor photometrically, in two colours, brightness and temperature variations of stars brighter than V = 4. The current mission design consists of three pairs of 7 kg nanosats from Austria, Canada and Poland carrying optical telescopes and CCDs. One instrument in each pair is equipped with a blue filter; the other, a red filter. The first two nanosats are UNIBRITE, designed and built by University of Toronto Institute for Aerospace Studies - Space Flight Laboratory and its twin, BRITE-Austria, built by the Technical University Graz with support of UTIAS-SFL. They were launched on 25 February 2013 by the Indian Space Agency under contract to the Canadian Space Agency into a low-Earth dusk-dawn polar orbit.',\n",
       "  'len': 839},\n",
       " {'abstract': \"The European Proximity Operation Simulator (EPOS) of the DLR-German Aerospace Center is a robotics-based simulator that aims at validating and verifying a satellite docking phase. The generic concept features a robotics tracking system working in closed loop with a force/torque feedback signal. Inherent delays in the tracking system combined with typical high stiffness at contact challenge the stability of the closed-loop system. The proposed concept of operations is hybrid: the feedback signal is a superposition of a measured value and of a virtual value that can be tuned in order to guarantee a desired behavior. This paper is concerned with an analytical study of the system's closed-loop stability, and with an experimental validation of the hybrid concept of operations in one dimension (1D). The robotics simulator is modeled as a second-order loop-delay system and closed-form expressions for the critical delay and associated frequency are derived as a function of the satellites' mass and the contact dynamics stiffness and damping parameters. A numerical illustration sheds light on the impact of the parameters on the stability regions. A first-order Pade approximation provides additional means of stability investigation. Experiments were performed and tests results are described for varying values of the mass and the damping coefficients. The empirical determination of instability is based on the coefficient of restitution and on the observed energy. There is a very good agreement between the critical damping values predicted by the analysis and observed during the tests...\",\n",
       "  'len': 1612},\n",
       " {'abstract': 'This paper presents a solution procedure of search parameter optimization for minimum load ensuring desired one-off and cumulative probabilities of detection in a multifunction phased array radar. The key approach is to convert this nonlinear optimization on four search parameters into a scalar optimization on signal-to-noise ratio by a semi-analytic process based on subproblem decomposition. The efficacy of the proposed solution approach is verified with theoretical analysis and numerical case studies.',\n",
       "  'len': 519},\n",
       " {'abstract': 'PRISMA is a demonstration mission for formation-flying and on-orbit-servicing critical technologies that involves two spacecraft launched in low Earth orbit in June 2010 and still in operation. Funded by the Swedish National Space Board, PRISMA mission has been developed by OHB Sweden with important contributions from the German Aerospace Centre (DLR/GSOC), the French Space Agency (CNES), and the Technical University of Denmark (DTU). The paper focuses on the last CNES experiment achieved in September 2012 that was devoted to the preparation of future astrometry missions illustrated by the NEAT and microNEAT mission concepts. The experiment consisted in performing the type of formation maneuvers required to point the two-satellite axis to a celestial target and maintain it fixed during the observation period. Achieving inertial pointing for a LEO formation represented a new challenge given the numerous constraints from propellant usage to star tracker blinding. The paper presents the experiment objectives in relation with the NEAT/microNEAT mission concept, describes its main design features along with the guidance and control algorithms evolutions and discusses the results in terms of performances achieved during the two rehearsals',\n",
       "  'len': 1263},\n",
       " {'abstract': 'Temperature-dependent dielectric permittivity of lead-free (LixNa1-x)NbO3 for nominal x = 0.04-0.20, prepared by solid state reaction followed by sintering, was studied to resolve often debated issue pertaining to exactness of morphotropic phase boundary (MPB) location along with structural aspects and phase stability in the system near MPB. Interestingly, a diffuse phase transition has been observed in the dielectric permittivity peak arising from the disorder induced in A-site and structural frustration in the perovskite cell due to Li substitution. A partial phase diagram has been proposed based on temperature-dependent dielectric permittivity studies. The room temperature piezoelectric and ferroelectric properties were investigated and the ceramics with x = 0.12 showed relatively good electrical properties (d33 = 28 pC/N, kp = 13.8%, Qm = 440, Pr = 12.5 {micro}C/cm2, EC = 43.2 kV/cm, Tm = 340 oC). These parameter values make this material suitable for piezoelectric resonator and filter applications. Moreover, a high dielectric permittivity (= 2703) with broad diffuse peak near transition temperature along with low dielectric loss (< 4%) in a wide temperature range (50-250 oC) found in this material may also have a potential application in high-temperature multilayer capacitors in automotive and aerospace related industries.',\n",
       "  'len': 1360},\n",
       " {'abstract': 'The posterior Cramér-Rao lower bound (PCRLB) derived in Tichavský et al., 1998, provides a bound on the mean square error (MSE) obtained with any non-linear state filter. Computing the PCRLB involves solving complex, multi-dimensional expectations, which do not lend themselves to an easy analytical solution. Furthermore, any attempt to approximate it using numerical or simulation based approaches require a priori access to the true states, which may not be available, except in simulations or in carefully designed experiments. To allow recursive approximation of the PCRLB when the states are hidden or unmeasured, a new approach based on sequential Monte-Carlo (SMC) or particle filters (PF) is proposed. The approach uses SMC methods to estimate the hidden states using a sequence of the available sensor measurements. The developed method is general and can be used to approximate the PCRLB in non-linear systems with non-Gaussian state and sensor noise. The efficacy of the developed method is illustrated on two simulation examples, including a practical problem of ballistic target tracking at re-entry phase.',\n",
       "  'len': 1131},\n",
       " {'abstract': 'We consider the asymptotic behaviour of the effective thermal conductivity of a two-phase composite obtained by introducing into an infinite homogeneous matrix a periodic set of inclusions of a different material and of size proportional to a positive parameter \\\\epsilon. We are interested in the case of imperfect thermal contact at the two-phase interface. Under suitable assumptions, we show that the effective thermal conductivity can be continued real analytically in the parameter \\\\epsilon around the degenerate value \\\\epsilon=0, in correspondence of which the inclusions collapse to points. The results presented here are obtained by means of an approach based on functional analysis and potential theory and are also part of a forthcoming paper by the authors.',\n",
       "  'len': 779},\n",
       " {'abstract': 'We describe a non-commutative generalization of the complex Fourier-Mellin transform to Clifford algebra valued signal functions over the domain $\\\\R^{p,q}$ taking values in Cl(p,q), p+q=2. Keywords: algebra, Fourier transforms; Logic, set theory, and algebra, Fourier analysis, Integral transforms',\n",
       "  'len': 308},\n",
       " {'abstract': 'When (robotic) Automated Fibre Placement (AFP) is used to manufacture aerospace components with complex three dimensional geometries, gaps between fibre tows can occur. This paper is the first to explore the interaction under compressive load of these tow gaps with impact damage. Two coupons with different distributions of tow-gaps were impacted. Results indicated that the area of delamination is smaller for an impact directly over a tow gap where the tow gap is situated close to the non-impact face. Subsequent Compression After Impact (CAI) testing demonstrated that both the formation of sublaminate buckles and subsequent growth of delaminations is inhibited by the presence of a tow gap near the non-impact face. Non-destructive testing techniques and a computationally efficient infinite Strip model are used to analyse the damage resistance and damage tolerance of the coupons. A new validation of the Strip model is also presented.',\n",
       "  'len': 955},\n",
       " {'abstract': 'The Random Hypersurface Model (RHM) is introduced that allows for estimating a shape approximation of an extended object in addition to its kinematic state. An RHM represents the spatial extent by means of randomly scaled versions of the shape boundary. In doing so, the shape parameters and the measurements are related via a measurement equation that serves as the basis for a Gaussian state estimator. Specific estimators are derived for elliptic and star-convex shapes.',\n",
       "  'len': 484},\n",
       " {'abstract': 'The present thesis deals with the non-modal linear analysis of 3D perturbations in wall flows. In the first part,a solution to the Orr-Sommerfeld and Squire IVP, in the form of orthogonal functions expansion, is researched. The Galerkin method is successfully implemented to numerically compute approximate solutions for bounded flows. The Chandrasekhar functions revealed to ensure a fifth order of accuracy. The focus of the subsequent analysis is on the transient behavior of the perturbation frequency and phase velocity. The results confirm recent observations about a jump in the temporal evolution of the frequency of the wall-normal velocity signal, considered as the end of an Early Transient. After this jump, the wave frequency for Plane Couette flow experiences a periodic modulation about the asymptotic value, which is motivated and investigated in detail. A new result is the presence of a second frequency jump for the wall-normal vorticity. This fact, together with the possibility for different values of the signals asymptotic frequency, shows the existence of an Intermediate Transient. Moreover, a connection between the frequency jumps and the establishing of a self-similarity condition in time for both the velocity and vorticity profiles is found and investigated for both Plane Couette flow and Plane Poiseuille flow. Eventually, through superposition of waves with limited wavenumber range, a wave packet is reconstructed for Plane Couette flow and Blasius boundary-layer flow . The linear spot evolution revealed to have many common features with the early stages of a turbulent spot, particularly the streaky structure and the spot shape.',\n",
       "  'len': 1678},\n",
       " {'abstract': 'Target motion analysis with wideband passive sonar has received much attention. Maximum likelihood probabilistic data-association (ML-PDA) represents an asymptotically efficient estimator for deterministic target motion, and is especially well-suited for low-observable targets; the results presented here apply to situations with higher signal to noise ratio as well, including of course the situation of a deterministic target observed via clean measurements without false alarms or missed detections. Here we study the inverse problem, namely, how to identify the observing platform (following a two-leg motion model) from the results of the target estimation process, i.e. the estimated target state and the Fisher information matrix, quantities we assume an eavesdropper might intercept. We tackle the problem and we present observability properties, with supporting simulation results.',\n",
       "  'len': 902},\n",
       " {'abstract': \"Context. Primitive asteroids contain complex organic material and ices relevant to the origin of life on Earth. These types of asteroids are the target of several-sample return missions to be launched in the next years. 1999 JU3 is the target of the Japanese Aerospace Exploration Agency's Hayabusa 2 mission. Aims. 1999 JU3 has been previously identified as a C-class asteroid. Spectroscopic observations at longer wavelengths will help to constrain its composition. Methods. We obtained spectroscopy of 1999 JU3 from 0.85 to 2.2 microns, with the 3.6 m Telescopio Nazionale Galileo using the low resolution mode of the Near Infrared Camera Spectrograph. Results. We present a near-infrared spectrum of 1999 JU3 from 0.85 to 2.2microns that is consistent with previously published spectra and with its C-type classification. Conclusions. Our spectrum confirms the primitive nature of 1999 JU3 and its interest as target of the sample-return mission Hayabusa 2.\",\n",
       "  'len': 972},\n",
       " {'abstract': \"The General Antiparticle Spectrometer (GAPS) experiment is a novel approach for the detection of cosmic ray antiparticles. A prototype GAPS experiment (pGAPS) was successfully flown on a high-altitude balloon in June of 2012. The goals of the pGAPS experiment were: to test the operation of lithium drifted silicon (Si(Li)) detectors at balloon altitudes, to validate the thermal model and cooling concept needed for engineering of a full-size GAPS instrument, and to characterize cosmic ray and X-ray backgrounds. The instrument was launched from the Japan Aerospace Exploration Agency's (JAXA) Taiki Aerospace Research Field in Hokkaido, Japan. The flight lasted a total of 6 hours, with over 3 hours at float altitude (~33 km). Over one million cosmic ray triggers were recorded and all flight goals were met or exceeded.\",\n",
       "  'len': 835},\n",
       " {'abstract': \"The Stratospheric Observatory For Infrared Astronomy, or SOFIA, is the largest flying observatory ever built,consisting of a 2.7-meter diameter telescope embedded in a modified Boeing 747-SP aircraft. SOFIA is a joint project between NASA and the German Aerospace Center Deutsches Zentrum fur Luft und-Raumfahrt (DLR). By flying at altitudes up to 45000 feet, the observatory gets above 99.9 percent of the infrared-absorbing water vapor in the Earth's atmosphere. This opens up an almost uninterrupted wavelength range from 0.3-1600 microns that is in large part obscured from ground based observatories. Since its 'Initial Science Flight' in December 2010, SOFIA has flown several dozen science flights, and has observed a wide array of objects from Solar System bodies, to stellar nurseries, to distant galaxies. This paper reviews a few of the exciting new science results from these first flights which were made by three instruments: the mid-infrared camera FORCAST, the far-infrared heterodyne spectrometer GREAT, and the optical occultation photometer HIPO.\",\n",
       "  'len': 1076},\n",
       " {'abstract': 'A reduced-rank framework with set-membership filtering (SMF) techniques is presented for adaptive beamforming problems encountered in radar systems. We develop and analyze stochastic gradient (SG) and recursive least squares (RLS)-type adaptive algorithms, which achieve an enhanced convergence and tracking performance with low computational cost as compared to existing techniques. Simulations show that the proposed algorithms have a superior performance to prior methods, while the complexity is lower.',\n",
       "  'len': 517},\n",
       " {'abstract': \"Recent literature has shown that the control of False Discovery Rate (FDR) for distributed detection in wireless sensor networks (WSNs) can provide substantial improvement in detection performance over conventional design methodologies. In this paper, we further investigate system design issues in FDR based distributed detection. We demonstrate that improved system design may be achieved by employing the Kolmogorov-Smirnov distance metric instead of the deflection coefficient, as originally proposed in Ray&VarshneyAES11. We also analyze the performance of FDR based distributed detection in the presence of Byzantines. Byzantines are malicious sensors which send falsified information to the Fusion Center (FC) to deteriorate system performance. We provide analytical and simulation results on the global detection probability as a function of the fraction of Byzantines in the network. It is observed that the detection performance degrades considerably when the fraction of Byzantines is large. Hence, we propose an adaptive algorithm at the FC which learns the Byzantines' behavior over time and changes the FDR parameter to overcome the loss in detection performance. Detailed simulation results are provided to demonstrate the robustness of the proposed adaptive algorithm to Byzantine attacks in WSNs.\",\n",
       "  'len': 1324},\n",
       " {'abstract': 'The mechanical response of solids depends on temperature because the way atoms and molecules respond collectively to deformation is affected at various levels by thermal motion. This is a fundamental problem of solid state science and plays a crucial role in metallurgy, aerospace engineering, energy. In glasses the vanishing of rigidity upon increasing temperature is the reverse process of the glass transition. It remains poorly understood due to the disorder leading to nontrivial (nonaffine) components in the atomic displacements. Our theory explains the basic mechanism of the melting transition of amorphous (disordered) solids in terms of the lattice energy lost to this nonaffine motion, compared to which thermal vibrations turn out to play only a negligible role. It predicts the square-root vanishing of the shear modulus $G\\\\sim\\\\sqrt{T_{c}-T}$ at criticality observed in the most recent numerical simulation study. The theory is also in good agreement with classic data on melting of amorphous polymers (for which no alternative theory can be found in the literature) and offers new opportunities in materials science.',\n",
       "  'len': 1143},\n",
       " {'abstract': 'Data association, the problem of reasoning over correspondence between targets and measurements, is a fundamental problem in tracking. This paper presents a graphical model formulation of data association and applies an approximate inference method, belief propagation (BP), to obtain estimates of marginal association probabilities. We prove that BP is guaranteed to converge, and bound the number of iterations necessary. Experiments reveal a favourable comparison to prior methods in terms of accuracy and computational complexity.',\n",
       "  'len': 545},\n",
       " {'abstract': \"Taking into account the drivers' state is a major challenge for designing new advanced driver assistance systems. In this paper we present a driver assistance system strongly coupled to the user. DAARIA 1 stands for Driver Assistance by Augmented Reality for Intelligent Automobile. It is an augmented reality interface powered by several sensors. The detection has two goals: one is the position of obstacles and the quantification of the danger represented by them. The other is the driver's behavior. A suitable visualization metaphor allows the driver to perceive at any time the location of the relevant hazards while keeping his eyes on the road. First results show that our method could be applied to a vehicle but also to aerospace, fluvial or maritime navigation.\",\n",
       "  'len': 783},\n",
       " {'abstract': 'The Spectral Airglow Temperature Imager is a ground-based spectral instrument for spatial registration of airglow emissions. The basic aim of the instrument development is the investigation of gravity waves based on the spatial characteristics of the temperature field at the altitude of mesopause and its evolution in the time. The temperature retrieval is based on matching measured and preliminary calculated synthetic spectra. Possibilities are presented for generalization of the basic regression equation which connects the measured and the synthetic spectra. A linear change of the background for the entire filter transmittance interval was presumed. Numerical experiments by Monte-Karlo simulation were conducted. The presented results show a bigger stability of the proposed approach in comparison with the traditional one, without considering the linear background.',\n",
       "  'len': 887},\n",
       " {'abstract': 'A new dynamical parameter, the f-indicator, is introduced and used in order to distinguish between regular and chaotic motion in galactic Hamiltonian systems. Two kinds of galactic potentials are used: (i) a global potential, which describes the whole galaxy and (ii) a local potential, which is made up of perturbed harmonic oscillators and describes motion near an equilibrium point. The new indicator is based on the energies of the separable system along the x, y and z axis. Comparison between the outcomes obtained using the new dynamical parameter and other methods, such as the maximum Lyapunov Characteristic Exponent (L.C.E), or the S(c) dynamical spectrum, shows that the new dynamical indicator gives fast and reliable results concerning the regular or chaotic character of the orbits. The new indicator was tested in several Hamiltonian systems of two (2D) degrees and three (3D) degrees of freedom.',\n",
       "  'len': 923},\n",
       " {'abstract': 'In recent years, wireless sensor network becomes popular both in civil and military jobs. However, security is one of the significant challenges for sensor network because of their deployment in open and unprotected environment. As cryptographic mechanism is not enough to protect sensor network from external attacks, intrusion detection system (IDS) needs to be introduced. In this paper we propose a policy based IDS for hierarchical architecture that fits the current demands and restrictions of wireless ad hoc sensor network. In this proposed IDS architecture we followed clustering mechanism to build four level hierarchical network which enhance network scalability to large geographical area and use both anomaly and misuse detection techniques for intrusion detection that concentrates on power saving of sensor nodes by distributing the responsibility of intrusion detection among different layers. We also introduce a policy based intrusion response system for hierarchical architecture.',\n",
       "  'len': 1010},\n",
       " {'abstract': 'The purpose of this paper is to investigate the generalized formulation of weighted optimal guidance laws with impact angle constraint. From the generalized formulation, we explicitly find the feasible set of weighting functions that lead to analytical forms of weighted optimal guidance laws. This result has potential significance because it can provide additional degrees of freedom in designing a guidance law that accomplishes the specified guidance objective.',\n",
       "  'len': 476},\n",
       " {'abstract': 'Synthetic aperture radar (SAR) images are often blurred by phase perturbations induced by uncompensated sensor motion and /or unknown propagation effects caused by turbulent media. To get refocused images, autofocus proves to be useful post-processing technique applied to estimate and compensate the unknown phase errors. However, a severe drawback of the conventional autofocus algorithms is that they are only capable of removing one-dimensional azimuth phase errors (APE). As the resolution becomes finer, residual range cell migration (RCM), which makes the defocus inherently two-dimensional, becomes a new challenge. In this paper, correction of APE and residual RCM are presented in the framework of polar format algorithm (PFA). First, an insight into the underlying mathematical mechanism of polar reformatting is presented. Then based on this new formulation, the effect of polar reformatting on the uncompensated APE and residual RCM is investigated in detail. By using the derived analytical relationship between APE and residual RCM, an efficient two-dimensional (2-D) autofocus method is proposed. Experimental results indicate the effectiveness of the proposed method.',\n",
       "  'len': 1195},\n",
       " {'abstract': 'This paper concerns with the sensor management problem in collocated Multiple-Input Multiple-Output (MIMO) radars. After deriving the Cramer-Rao Lower Bound (CRLB) as a performance measure, the antenna allocation problem is formulated as a standard Semi-definite Programming (SDP) for the single-target case. In addition, for multiple unresolved target scenarios, a sampling-based algorithm is proposed to deal with the non-convexity of the cost function. Simulations confirm the superiority of the localization results under the optimal structure.',\n",
       "  'len': 559},\n",
       " {'abstract': 'This paper presents a novel approach for the preliminary design of Low-Thrust, many-revolution transfers. The main feature of the novel approach is a considerable reduction in the control parameters and a consequent gain in computational speed. Each spiral is built by using a predefined pattern for thrust direction and switching structure. The pattern is then optimised to minimise propellant consumption and transfer time. The variation of the orbital elements due to the thrust is computed analytically from a first-order solution of the perturbed Keplerian motion. The proposed approach allows for a realistic estimation of {\\\\Delta}V and time of flight required to transfer a spacecraft between two arbitrary orbits. Eccentricity and plane changes are both accounted for. The novel approach is applied here to the design of missions for the removal of space debris by means of an Ion Beam Shepherd Spacecraft. In particular, two slightly different variants of the proposed low-thrust control model are used for the different phases of the mission. Thanks to their low computational cost they can be included in a multiobjective optimisation problem in which the sequence and timing of the removal of five pieces of debris are optimised to minimise propellant consumption and mission duration.',\n",
       "  'len': 1308},\n",
       " {'abstract': 'In this paper, we propose an approach for assigning an interest level to the goals of a planetary rover. Assigning an interest level to goals, allows the rover autonomously to transform and reallocate the goals. The interest level is defined by data-fusing payload and navigation information. The fusion yields an \"interest map\", that quantifies the level of interest of each area around the rover. In this way the planner can choose the most interesting scientific objectives to be analyzed, with limited human intervention, and reallocates its goals autonomously. The Dezert-Smarandache Theory of Plausible and Paradoxical Reasoning was used for information fusion: this theory allows dealing with vague and conflicting data. In particular, it allows us directly to model the behavior of the scientists that have to evaluate the relevance of a particular set of goals. The paper shows an application of the proposed approach to the generation of a reliable interest map.',\n",
       "  'len': 983},\n",
       " {'abstract': 'Inertial navigation applications are usually referenced to a rotating frame. Consideration of the navigation reference frame rotation in the inertial navigation algorithm design is an important but so far less seriously treated issue, especially for ultra-high-speed flying aircraft or the future ultra-precision navigation system of several meters per hour. This paper proposes a rigorous approach to tackle the issue of navigation frame rotation in velocity/position computation by use of the newly-devised velocity/position integration formulae in the Part I companion paper. The two integration formulae set a well-founded cornerstone for the velocity/position algorithms design that makes the comprehension of the inertial navigation computation principle more accessible to practitioners, and different approximations to the integrals involved will give birth to various velocity/position update algorithms. Two-sample velocity and position algorithms are derived to exemplify the design process. In the context of level-flight airplane examples, the derived algorithm is analytically and numerically compared to the typical algorithms existing in the literature. The results throw light on the problems in existing algorithms and the potential benefits of the derived algorithm.',\n",
       "  'len': 1296},\n",
       " {'abstract': 'The in-flight alignment is a critical stage for airborne INS/GPS applications. The alignment task is usually carried out by the Kalman filtering technique that necessitates a good initial attitude to obtain satisfying performance. Due to the airborne dynamics, the in-flight alignment is much difficult than alignment on the ground. This paper proposes an optimization-based coarse alignment approach using GPS position/velocity as input, founded on the newly-derived velocity/position integration formulae. Simulation and flight test results show that, with the GPS lever arm well handled, it is potentially able to yield the initial heading up to one degree accuracy in ten seconds. It can serve as a nice coarse in-flight alignment without any prior attitude information for the subsequent fine Kalman alignment. The approach can also be applied to other applications that require aligning the INS on the run.',\n",
       "  'len': 923},\n",
       " {'abstract': 'This paper presents an algorithm for multiobjective optimization that blends together a number of heuristics. A population of agents combines heuristics that aim at exploring the search space both globally and in a neighborhood of each agent. These heuristics are complemented with a combination of a local and global archive. The novel agent- based algorithm is tested at first on a set of standard problems and then on three specific problems in space trajectory design. Its performance is compared against a number of state-of-the-art multiobjective optimisation algorithms that use the Pareto dominance as selection criterion: NSGA-II, PAES, MOPSO, MTS. The results demonstrate that the agent-based search can identify parts of the Pareto set that the other algorithms were not able to capture. Furthermore, convergence is statistically better although the variance of the results is in some cases higher.',\n",
       "  'len': 920},\n",
       " {'abstract': 'Mg-based alloys have recently attracted major interest in view of their potential applications in the aerospace, aircraft and automotive industries. Here, we show that the effects of Y and Zn atoms on their compressibility can be reliably estimated by a simple thermodynamical model deduced by means of first-principles calculations based on density functional theory that just appeared.',\n",
       "  'len': 398},\n",
       " {'abstract': 'The Stratospheric Observatory for Infrared Astronomy (SOFIA) is an airborne observatory consisting of a specially modified Boeing 747SP with a 2.7-m telescope, flying at altitudes as high as 13.7 km (45,000 ft). Designed to observe at wavelengths from 0.3 micron to 1.6 mm, SOFIA operates above 99.8 % of the water vapor that obscures much of the infrared and submillimeter. SOFIA has seven science instruments under development, including an occultation photometer, near-, mid-, and far-infrared cameras, infrared spectrometers, and heterodyne receivers. SOFIA, a joint project between NASA and the German Aerospace Center DLR, began initial science flights in 2010 December, and has conducted 30 science flights in the subsequent year. During this early science period three instruments have flown: the mid-infrared camera FORCAST, the heterodyne spectrometer GREAT, and the occultation photometer HIPO. This article provides an overview of the observatory and its early performance.',\n",
       "  'len': 996},\n",
       " {'abstract': 'In multiple time-scale (singularly perturbed) dynamical systems, canards are counterintuitive solutions that evolve along both attracting and repelling invariant manifolds. In two dimensions, canards result in periodic oscillations whose amplitude and period grow in a highly nonlinear way: they are slowly varying with respect to a control parameter, except for an exponentially small range of values where they grow extremely rapidly. This sudden growth, called a canard explosion, has been encountered in many applications ranging from chemistry to neuronal dynamics, aerospace engineering and ecology. Canards were initially studied using nonstandard analysis, and later the same results were proved by standard techniques such as matched asymptotics, invariant manifold theory and parameter blow-up. More recently, canard-like behaviour has been linked to surfaces of discontinuity in piecewise-smooth dynamical systems. This paper provides a new perspective on the canard phenomenon by showing that the nonstandard analysis of canard explosions can be recast into the framework of piecewise-smooth dynamical systems. An exponential coordinate scaling is applied to a singularly perturbed system of ordinary differential equations. The scaling acts as a lens that resolves dynamics across all time-scales. The changes of local curvature that are responsible for canard explosions are then analysed. Regions where different time-scales dominate are separated by hypersurfaces, and these are pinched together to obtain a piecewise-smooth system, in which curvature changes manifest as discontinuity-induced bifurcations. The method is used to classify canards in arbitrary dimensions, and to derive the parameter values over which canards form either small cycles (canards without head) or large cycles (canards with head).',\n",
       "  'len': 1837},\n",
       " {'abstract': 'Recent developments in random finite sets (RFSs) have yielded a variety of tracking methods that avoid data association. This paper derives a form of the full Bayes RFS filter and observes that data association is implicitly present, in a data structure similar to MHT. Subsequently, algorithms are obtained by approximating the distribution of associations. Two algorithms result: one nearly identical to JIPDA, and another related to the MeMBer filter. Both improve performance in challenging environments.',\n",
       "  'len': 519},\n",
       " {'abstract': 'This paper presents the development of a real time tracking algorithm that runs on a 1.2 GHz PC/104 computer on-board a small UAV. The algorithm uses zero mean normalized cross correlation to detect and locate an object in the image. A kalman filter is used to make the tracking algorithm computationally efficient. Object position in an image frame is predicted using the motion model and a search window, centered at the predicted position is generated. Object position is updated with the measurement from object detection. The detected position is sent to the motion controller to move the gimbal so that the object stays at the center of the image frame. Detection and tracking is autonomously carried out on the payload computer and the system is able to work in two different methods. The first method starts detecting and tracking using a stored image patch. The second method allows the operator on the ground to select the interest object for the UAV to track. The system is capable of re-detecting an object, in the event of tracking failure. Performance of the tracking system was verified both in the lab and on the field by mounting the payload on a vehicle and simulating a flight. Tests show that the system can detect and track a diverse set of objects in real time. Flight testing of the system will be conducted at the next available opportunity.',\n",
       "  'len': 1376},\n",
       " {'abstract': 'Fibre optic sensors (FOS) are an established technique for environmental and deformation monitoring in several areas like civil engineering, aerospace, and energy. Their immunity to electromagnetic and magnetic fields and nuclear environments, its small size, multiplexing capability and the possibility to be embedded make them an attractive technology for the structural and environmental monitoring of collider particle physics experiments. Between all the possible Fibre Optic sensors FBGs (Fiber Bragg Grating) seems to be the best solution for HEP applications. The first step was to characterize FBG sensors for it use in High Energy Physics environment. During last two years we have checked the resistance of the Fibre Bragg Grating sensors to radiation. Two irradiation campaigns with protons have been done at CNA (Centro Nacional de Aceleradores). In the near future these sensors are being planned to be used in detectors (the closest one Belle II.). Several work on integration issues in Belle II PXD-SVD, and checking for environmental and deformation monitoring in the detectors inner part has been done.',\n",
       "  'len': 1131},\n",
       " {'abstract': 'LISA (Laser Interferometer Space Antenna) is a joint mission of ESA and NASA which aims to be the first space-borne gravita- tional wave observatory. Due to the high complexity and technological challenges that LISA will face, ESA decided to launch a technological demonstrator, LISA Pathfinder. The payload of LISA Pathfinder is the so-called LISA Technology Package, and will be the highest sensitivity geodesic explorer flown to date. The LISA Technology Package is designed to measure relative accelerations between two test masses in nominal free fall (geodesic motion). The magnetic, thermal and radiation disturbances affecting the payload are monitored and dealt by the diagnostics subsystem. The diagnostics subsystem consists of several modules, and one of these is the magnetic diagnostics unit. Its main function is the assessment of differential acceleration noise between test masses due to the magnetic effects. To do so, it has to determine the magnetic characteristics of the test masses, namely their magnetic remanences and susceptibilities. In this paper we show how this can be achieved to the desired accuracy.',\n",
       "  'len': 1143},\n",
       " {'abstract': 'The six axis robots are widely used in automotive industry for their good repeatability (as defined in the ISO92983) (painting, welding, mastic deposition, handling etc.). In the aerospace industry, robot starts to be used for complex applications such as drilling, riveting, fiber placement, NDT, etc. Given the positioning performance of serial robots, precision applications require usually external measurement device with complexes calibration procedure in order to reach the precision needed. New applications in the machining field of composite material (aerospace, naval, or wind turbine for example) intend to use off line programming of serial robot without the use of calibration or external measurement device. For those applications, the position, orientation and path trajectory precision of the tool center point of the robot are needed to generate the machining operation. This article presents the different conditions that currently limit the development of robots in robotic machining applications. We analyze the dynamical behavior of a robot KUKA KR240-2 (located at the University of Bordeaux 1) equipped with a HSM Spindle (42000 rpm, 18kW). This analysis is done in three stages. The first step is determining the self-excited frequencies of the robot structure for three different configurations of work. The second phase aims to analyze the dynamical vibration of the structure as the spindle is activated without cutting. The third stage consists of vibration analysis during a milling operation.',\n",
       "  'len': 1534},\n",
       " {'abstract': 'Alignment of the strapdown inertial navigation system (INS) has strong nonlinearity, even worse when maneuvers, e.g., tumbling techniques, are employed to improve the alignment. There is no general rule to attack the observability of a nonlinear system, so most previous works addressed the observability of the corresponding linearized system by implicitly assuming that the original nonlinear system and the linearized one have identical observability characteristics. Strapdown INS alignment is a nonlinear system that has its own characteristics. Using the inherent properties of strapdown INS, e.g., the attitude evolution on the SO(3) manifold, we start from the basic definition and develop a global and constructive approach to investigate the observability of strapdown INS static and tumbling alignment, highlighting the effects of the attitude maneuver on observability. We prove that strapdown INS alignment, considering the unknown constant sensor biases, will be completely observable if the strapdown INS is rotated successively about two different axes and will be nearly observable for finite known unobservable states (no more than two) if it is rotated about a single axis. Observability from a global perspective provides us with insights into and a clearer picture of the problem, shedding light on previous theoretical results on strapdown INS alignment that were not comprehensive or consistent.. The reporting of inconsistencies calls for a review of all linearization-based observability studies in the vast literature. Extensive simulations with constructed ideal observers and an extended Kalman filter are carried out, and the numerical results accord with the analysis. The conclusions can also assist in designing the optimal tumbling strategy and the appropriate state observer in practice to maximize the alignment performance.',\n",
       "  'len': 1870},\n",
       " {'abstract': 'Elliptic partial differential equations arise in many fields of science and engineering such as steady state distribution of heat, fluid dynamics, structural/mechanical engineering, aerospace engineering and seismology etc. In three dimensions it is well known that the solutions of elliptic boundary value problems have singular behavior near the corners and edges of the domain. The singularities which arise are known as vertex, edge, and vertex-edge singularities. We propose a nonconforming h-p spectral element method to solve three dimensional elliptic boundary value problems on non-smooth domains to exponential accuracy. To overcome the singularities which arise in the neighbourhoods of the vertices, vertex-edges and edges we use local systems of coordinates. These local coordinates are modified versions of spherical and cylindrical coordinate systems in their respective neighbourhoods. Away from these neighbourhoods standard Cartesian coordinates are used. In each of these neighbourhoods we use a geometrical mesh which becomes finer near the corners and edges. We then derive differentiability estimates in these new set of variables and a stability estimate on which our method is based for a non-conforming h-p spectral element method. The Sobolev spaces in vertex-edge and edge neighbourhoods are anisotropic and become singular at the corners and edges. The method is essentially a least-squares collocation} method and a solution can be obtained using Preconditioned Conjugate Gradient Method (PCGM). To solve the minimization problem we need to solve the normal equations for the least-squares problem. The residuals in the normal equations can be obtained without computing and storing mass and stiffness matrices. Computational results for a number of model problems confirm the theoretical estimates obtained for the error and computational complexity.',\n",
       "  'len': 1891},\n",
       " {'abstract': 'A one-step, a two-step, an abridged, a skeletal and four detailed kinetic schemes of hydrogen oxidation have been tested. A new skeletal kinetic scheme of hydrogen oxidation has been developed. The CFD calculations were carried out using ANSYS CFX software. Ignition delay times and speeds of flames were derived from the computational results. The computational data obtained using ANSYS CFX and CHEMKIN, and experimental data were compared. The precision, reliability, and range of validity of the kinetic schemes in CFD simulations were estimated. The impact of kinetic scheme on the results of computations was discussed. The relationship between grid spacing, timestep, accuracy, and computational cost were analyzed.',\n",
       "  'len': 733},\n",
       " {'abstract': 'Monte Carlo (MC) techniques are often used to estimate integrals of a multivariate function using randomly generated samples of the function. In light of the increasing interest in uncertainty quantification and robust design applications in aerospace engineering, the calculation of expected values of such functions (e.g. performance measures) becomes important. However, MC techniques often suffer from high variance and slow convergence as the number of samples increases. In this paper we present Stacked Monte Carlo (StackMC), a new method for post-processing an existing set of MC samples to improve the associated integral estimate. StackMC is based on the supervised learning techniques of fitting functions and cross validation. It should reduce the variance of any type of Monte Carlo integral estimate (simple sampling, importance sampling, quasi-Monte Carlo, MCMC, etc.) without adding bias. We report on an extensive set of experiments confirming that the StackMC estimate of an integral is more accurate than both the associated unprocessed Monte Carlo estimate and an estimate based on a functional fit to the MC samples. These experiments run over a wide variety of integration spaces, numbers of sample points, dimensions, and fitting functions. In particular, we apply StackMC in estimating the expected value of the fuel burn metric of future commercial aircraft and in estimating sonic boom loudness measures. We compare the efficiency of StackMC with that of more standard methods and show that for negligible additional computational cost significant increases in accuracy are gained.',\n",
       "  'len': 1618},\n",
       " {'abstract': 'A predictive Bayesian model selection approach is presented to discriminate coupled models used to predict an unobserved quantity of interest (QoI). The need for accurate predictions arises in a variety of critical applications such as climate, aerospace and defense. A model problem is introduced to study the prediction yielded by the coupling of two physics/sub-components. For each single physics domain, a set of model classes and a set of sensor observations are available. A goal-oriented algorithm using a predictive approach to Bayesian model selection is then used to select the combination of single physics models that best predict the QoI. It is shown that the best coupled model for prediction is the one that provides the most robust predictive distribution for the QoI.',\n",
       "  'len': 796},\n",
       " {'abstract': 'A structurally stable crystalline carbon allotrope is predicted by means of the first-principles calculations. This allotrope can be derived by substituting each atom in diamond with a carbon tetrahedron, and possesses the same space group Fd^1 3m as diamond, which is thus coined as T- carbon. The calculations on geometrical, vibrational and electronic properties reveal that T-carbon, with a considerable structural stability and a much lower density 1.50 g/cm3, is a semiconductor with a direct band gap about 3.0 eV, and has a Vickers hardness 61.1 GPa lower than diamond but comparable with cubic boron nitride. Such a form of carbon, once obtained, would have wide applications in photocatalysis, adsoption, hydrogen storage and aerospace materials.',\n",
       "  'len': 767},\n",
       " {'abstract': 'This paper derives a general expression for the Cramér-Rao bound (CRB) of wireless localization algorithms using range measurements subject to bias corruption. Specifically, the a priori knowledge about which range measurements are biased, and the probability density functions (PDF) of the biases are assumed to be available. For each range measurement, the error due to estimating the time-of-arrival of the detected signal is modeled as a Gaussian distributed random variable with zero mean and known variance. In general, the derived CRB expression can be evaluated numerically. An approximate CRB expression is also derived when the bias PDF is very informative. Using these CRB expressions, we study the impact of the bias distribution on the mean square error (MSE) bound corresponding to the CRB. The analysis is corroborated by numerical experiments.',\n",
       "  'len': 870},\n",
       " {'abstract': 'In 2006 Tajmar et al. reported on the measurements of extreme gravitomagnetic fields from small Nb rings at cryogenic temperatures that are about 18 orders of magnitude larger than gravitomagnetic fields obtained from GR (general relativity). Cifuolini in 2004 and the NASA-Stanford Gravity Probe-B experiment in 2007 confirmed the Lense-Thirring effect as predicted by GR (gravitomagnetic fields generated by a rotating massive body, i.e. Earth) within some 10%. In 2007 gravitomagnetic fields generated by a rotating cryogenic lead disk were measured by Graham et al. Though these measurements were not conclusive (the accuracy of the laser gyrometer was not sufficient to produce a standard deviation small enough) their experiment seems to have seen the same phenomenon reported earlier by Tajmar et al., termed parity violation. This means that gravitomagnetic fields produced by the cryogenic rotating ring or disk vary substantially and change sign for clockwise and counter-clockwise directions of rotation. The experimental situation therefore occurs to be contradictory. On the one hand GR has been confirmed while at the same time, there seems to be experimental evidence for the existence of extreme gravitomagnetic fields that cannot be generated by the movement of large masses. If these experiments can be confirmed, they give a clear indication for the existence of additional gravitational fields of non-Newtonian nature. As was shown by the GP-B experiment, measuring gravitomagnetic fields from GR poses extreme difficulties. Therefore a novel physical mechanism should exist for the generation of gravity-like fields, which might also provide the key to gravitational engineering similar to electromagnetic technology.',\n",
       "  'len': 1749},\n",
       " {'abstract': 'In this paper, a continuous finite-time-convergent differentiator is presented based on a strong Lyapunov function. The continuous differentiator can reduce chattering phenomenon sufficiently than normal sliding mode differentiator, and the outputs of signal tracking and derivative estimation are all smooth. Frequency analysis is applied to compare the continuous differentiator with sliding mode differentiator. The beauties of the continuous finite-time-convergent differentiator include its simplicity, restraining noises sufficiently, and avoiding the chattering phenomenon.',\n",
       "  'len': 591},\n",
       " {'abstract': 'Development of robust dynamical systems and networks such as autonomous aircraft systems capable of accomplishing complex missions faces challenges due to the dynamically evolving uncertainties coming from model uncertainties, necessity to operate in a hostile cluttered urban environment, and the distributed and dynamic nature of the communication and computation resources. Model-based robust design is difficult because of the complexity of the hybrid dynamic models including continuous vehicle dynamics, the discrete models of computations and communications, and the size of the problem. We will overview recent advances in methodology and tools to model, analyze, and design robust autonomous aerospace systems operating in uncertain environment, with stress on efficient uncertainty quantification and robust design using the case studies of the mission including model-based target tracking and search, and trajectory planning in uncertain urban environment. To show that the methodology is generally applicable to uncertain dynamical systems, we will also show examples of application of the new methods to efficient uncertainty quantification of energy usage in buildings, and stability assessment of interconnected power networks.',\n",
       "  'len': 1254},\n",
       " {'abstract': 'The Saratoga transfer protocol was developed by Surrey Satellite Technology Ltd (SSTL) for its Disaster Monitoring Constellation (DMC) satellites. In over seven years of operation, Saratoga has provided efficient delivery of remote-sensing Earth observation imagery, across private wireless links, from these seven low-orbit satellites to ground stations, using the Internet Protocol (IP). Saratoga is designed to cope with high bandwidth-delay products, constrained acknowledgement channels, and high loss while streaming or delivering extremely large files. An implementation of this protocol has now been developed at the Australian Commonwealth Scientific and Industrial Research Organisation (CSIRO) for wider use and testing. This is intended to prototype delivery of data across dedicated astronomy radio telescope networks on the ground, where networked sensors in Very Long Baseline Interferometer (VLBI) instruments generate large amounts of data for processing and can send that data across private IP- and Ethernet-based links at very high rates. We describe this new Saratoga implementation, its features and focus on high throughput and link utilization, and lessons learned in developing this protocol for sensor-network applications.',\n",
       "  'len': 1260},\n",
       " {'abstract': 'Polyimides, due to their superior mechanical behavior at high temperatures, are used in a variety of applications that include aerospace, automobile and electronic packaging industries, as matrices for composites, as adhesives etc. In this paper, we extend our previous model in [S. Karra, K. R. Rajagopal, Modeling the non-linear viscoelastic response of high temperature polyimides, Mechanics of Materials, In press, doi:10.1016/j.mechmat.2010.09.006], to include oxidative degradation of these high temperature polyimides. Appropriate forms for the Helmholtz potential and the rate of dissipation are chosen to describe the degradation. The results for a specific boundary value problem, using our model compares well with the experimental creep data for PMR-15 resin that is aged in air.',\n",
       "  'len': 802},\n",
       " {'abstract': 'By coupling controllable quantum systems into larger structures we introduce the concept of a quantum metamaterial. Conventional meta-materials represent one of the most important frontiers in optical design, with applications in diverse fields ranging from medicine to aerospace. Up until now however, metamaterials have themselves been classical structures and interact only with the classical properties of light. Here we describe a class of dynamic metamaterials, based on the quantum properties of coupled atom-cavity arrays, which are intrinsically lossless, reconfigurable, and operate fundamentally at the quantum level. We show how this new class of metamaterial could be used to create a reconfigurable quantum superlens possessing a negative index gradient for single photon imaging. With the inherent features of quantum superposition and entanglement of metamaterial properties, this new class of dynamic quantum metamaterial, opens a new vista for quantum science and technology.',\n",
       "  'len': 1004},\n",
       " {'abstract': 'On computers, discrete problems are solved instead of continuous ones. One must be sure that the solutions of the former problems, obtained in real time (i.e., when the stepsize h is not infinitesimal) are good approximations of the solutions of the latter ones. However, since the discrete world is much richer than the continuous one (the latter being a limit case of the former), the classical definitions and techniques, devised to analyze the behaviors of continuous problems, are often insufficient to handle the discrete case, and new specific tools are needed. Often, the insistence in following a path already traced in the continuous setting, has caused waste of time and efforts, whereas new specific tools have solved the problems both more easily and elegantly. In this paper we survey three of the main difficulties encountered in the numerical solutions of ODEs, along with the novel solutions proposed.',\n",
       "  'len': 929},\n",
       " {'abstract': 'Space-time adaptive processing (STAP) is an effective tool for detecting a moving target in spaceborne or airborne radar systems. Statistical-based STAP methods generally need sufficient statistically independent and identically distributed (IID) training data to estimate the clutter characteristics. However, most actual clutter scenarios appear only locally stationary and lack sufficient IID training data. In this paper, by exploiting the intrinsic sparsity of the clutter distribution in the angle-Doppler domain, a new STAP algorithm called SR-STAP is proposed, which uses the technique of sparse recovery to estimate the clutter space-time spectrum. Joint sparse recovery with several training samples is also used to improve the estimation performance. Finally, an effective clutter covariance matrix (CCM) estimate and the corresponding STAP filter are designed based on the estimated clutter spectrum. Both the Mountaintop data and simulated experiments have illustrated the fast convergence rate of this approach. Moreover, SR-STAP is less dependent on prior knowledge, so it is more robust to the mismatch in the prior knowledge than knowledge-based STAP methods. Due to these advantages, SR-STAP has great potential for application in actual clutter scenarios.',\n",
       "  'len': 1285},\n",
       " {'abstract': 'The recent Cosmic Microwave Background (CMB) experiments have shown that the average density of the universe is close to the critical one and the universe is asymptotically flat (Euclidean). Taking into account that the universe remains flat and the total density of the universe $\\\\Omega_{0}$ is conserved equal to a unit during the cosmological expansion, the Schwarzschild radius of the observable universe has been determined equal to the Hubble distance $R_{s}=2GM/c^{2}=R\\\\sim c/H$, where M is the mass of the observable universe, R is the Hubble distance and H is the Hubble constant. Besides, it has been shown that the speed of the light c appears the parabolic velocity for the observable universe $c=\\\\sqrt{2GM/R}=v_{p}$ and the recessional velocity $v_{r}=Hr$ of an arbitrary galaxy at a distance r > 100 Mps from the observer, is equal to the parabolic velocity for the sphere, having radius r and a centre, coinciding with the observer. The requirement for conservation of $\\\\Omega_{0}=1$ during the expansion enables to derive the Hoyle-Carvalho formula for the mass of the observable universe $M=c^{3}/(2GH)$ by a new approach. Key words: flat universe; critical density of the universe; Schwarzschild radius; mass of the universe; parabolic velocity',\n",
       "  'len': 1273},\n",
       " {'abstract': 'In probability density function (PDF) methods a transport equation is solved numerically to compute the time and space dependent probability distribution of several flow variables in a turbulent flow. The joint PDF of the velocity components contains information on all one-point one-time statistics of the turbulent velocity field, including the mean, the Reynolds stresses and higher-order statistics. We developed a series of numerical algorithms to model the joint PDF of turbulent velocity, frequency and scalar compositions for high-Reynolds-number incompressible flows in complex geometries using unstructured grids. Advection, viscous diffusion and chemical reaction appear in closed form in the PDF formulation, thus require no closure hypotheses. The generalized Langevin model (GLM) is combined with an elliptic relaxation technique to represent the non-local effect of walls on the pressure redistribution and anisotropic dissipation of turbulent kinetic energy. The governing system of equations is solved fully in the Lagrangian framework employing a large number of particles representing a finite sample of all fluid particles. Eulerian statistics are extracted at gridpoints of the unstructured mesh. Compared to other particle-in-cell approaches for the PDF equations, this methodology is non-hybrid, thus the computed fields remain fully consistent without requiring any specific treatment. Two testcases demonstrate the applicability of the algorithm: a fully developed turbulent channel flow and the classical cavity flow both with scalars released from concentrated sources.',\n",
       "  'len': 1607},\n",
       " {'abstract': 'This paper aims to implement the six channel redundancy to achieve fault tolerance in testing of satellites with acoustic spectrum. We mainly focus here on achieving fault tolerance. An immediate application is the microphone data acquisition and to do analysis at the Acoustic Test Facility (ATF) centre, National Aerospace Laboratories. It has an 1100 cubic meter reverberation chamber in which a maximum sound pressure level of 157 dB is generated. The six channel Redundancy software with fault tolerant operation is devised and developed. The data are applied to program written in C language. The program is run using the Code Composer Studio by accepting the inputs. This is tested with the TMS 320C 6727 DSP, Pro Audio Development Kit (PADK).',\n",
       "  'len': 761},\n",
       " {'abstract': 'We examine several conducting spheres moving through a magnetic field gradient. An analytical approximation is derived and an experiment is conducted to verify the analytical solution. The experiment is simulated as well to produce a numerical result. Both the low and high magnetic Reynolds number regimes are studied. Deformation of the sphere is noted in the high Reynolds number case. It is suggested that this deformation effect could be useful for designing or enhancing present protection systems against space debris.',\n",
       "  'len': 536},\n",
       " {'abstract': 'The situational analysis lies in the basis of space and ground-based experiment planning. It is connected with the use of complex computation models of environment and with verification of the restricting conditions, due to the character of the conducted experiments and the solved scientific tasks. The present work proposes a formulation of the situational analysis on the basis of the finite abstract automata theory. On this basis, optimization of the situational analysis is suggested by formal schemes for adaptation to the conditions of the model environment. The efficiency enhancement is illustrated by results from the application of the proposed real-time optimization for photometric system control.',\n",
       "  'len': 722},\n",
       " {'abstract': 'A data and computation center for helioseismology has been set up at the Max Planck Institute for Solar System Research in Germany to prepare for the SDO mission. Here we present the system infrastructure and the scientific aims of this project, which is funded through grants from the German Aerospace Center and the European Research Council.',\n",
       "  'len': 355},\n",
       " {'abstract': 'This paper examines the problem of introducing advanced forms of fault-tolerance via reconfiguration into safety-critical avionic systems. This is required to enable increased availability after fault occurrence in distributed integrated avionic systems(compared to static federated systems). The approach taken is to identify a migration path from current architectures to those that incorporate re-configuration to a lesser or greater degree. Other challenges identified include change of the development process; incremental and flexible timing and safety analyses; configurable kernels applicable for safety-critical systems.',\n",
       "  'len': 640},\n",
       " {'abstract': 'This paper gives an overview of radio interfaces devoted for high data rate Wireless Sensor Networks. Four aerospace applications of WSN are presented to underline the importance of achieving high data rate. Then, two modulation schemes by which High Data Rate can be achieved are compared : Multi carrier approaches, represented by the popular Orthogonal Frequency Division Multiplexing (OFDM) and Single carrier methods, represented by Single Carrier Frequency division Equalization and its application for multiple access Single Carrier Frequency division multiple Access (SC-FDMA). SC-FDMA, with a very low Peak Average Power Ratio (PAPR), is as strong alternative to the OFDM scheme for highly power constraint application. The Chosen radio interface will be, finally, tested by a model based design approach based on Simulink and FPGA realization. SC-FDMA, with a very low Peak Average Power Ratio (PAPR), is as strong alternative to the OFDM scheme for highly power constraint application. The Chosen radio interface will be, finally, tested by a model based design approach based on Simulink and FPGA realization.',\n",
       "  'len': 1132},\n",
       " {'abstract': 'The main workshop objective was to promote a holistic view and interdisciplinary methods for design, verification and co-ordination of aerospace systems, by combining formal methods with techniques from control engineering and artificial intelligence. The very demanding safety, robustness and performance requirements of these systems require unprecedented integration of heterogeneous techniques and models. The aim of FMA was to bring together active researchers from all the above areas to discuss and present their work.',\n",
       "  'len': 536},\n",
       " {'abstract': 'Two formal stochastic models are said to be bisimilar if their solutions as a stochastic process are probabilistically equivalent. Bisimilarity between two stochastic model formalisms means that the strengths of one stochastic model formalism can be used by the other stochastic model formalism. The aim of this paper is to explain bisimilarity relations between stochastic hybrid automata, stochastic differential equations on hybrid space and stochastic hybrid Petri nets. These bisimilarity relations make it possible to combine the formal verification power of automata with the analysis power of stochastic differential equations and the compositional specification power of Petri nets. The relations and their combined strengths are illustrated for an air traffic example.',\n",
       "  'len': 789},\n",
       " {'abstract': 'This article presents a complete scheme for the development of Critical Embedded Systems with Multiple Real-Time Constraints. The system is programmed with a language that extends the synchronous approach with high-level real-time primitives. It enables to assemble in a modular and hierarchical manner several locally mono-periodic synchronous systems into a globally multi-periodic synchronous system. It also allows to specify flow latency constraints. A program is translated into a set of real-time tasks. The generated code (\\\\C\\\\ code) can be executed on a simple real-time platform with a dynamic-priority scheduler (EDF). The compilation process (each algorithm of the process, not the compiler itself) is formally proved correct, meaning that the generated code respects the real-time semantics of the original program (respect of periods, deadlines, release dates and precedences) as well as its functional semantics (respect of variable consumption).',\n",
       "  'len': 971},\n",
       " {'abstract': 'The validation of requirements is a fundamental step in the development process of safety-critical systems. In safety critical applications such as aerospace, avionics and railways, the use of formal methods is of paramount importance both for requirements and for design validation. Nevertheless, while for the verification of the design, many formal techniques have been conceived and applied, the research on formal methods for requirements validation is not yet mature. The main obstacles are that, on the one hand, the correctness of requirements is not formally defined; on the other hand that the formalization and the validation of the requirements usually demands a strong involvement of domain experts. We report on a methodology and a series of techniques that we developed for the formalization and validation of high-level requirements for safety-critical applications. The main ingredients are a very expressive formal language and automatic satisfiability procedures. The language combines first-order, temporal, and hybrid logic. The satisfiability procedures are based on model checking and satisfiability modulo theory. We applied this technology within an industrial project to the validation of railways requirements.',\n",
       "  'len': 1248},\n",
       " {'abstract': 'The property that every control system should posses is stability, which translates into safety in real-life applications. A central tool in systems theory for synthesizing control laws that achieve stability are control Lyapunov functions (CLFs). Classically, a CLF enforces that the resulting closed-loop state trajectory is contained within a cone with a fixed, predefined shape, and which is centered at and converges to a desired converging point. However, such a requirement often proves to be overconservative, which is why most of the real-time controllers do not have a stability guarantee. Recently, a novel idea that improves the design of CLFs in terms of flexibility was proposed. The focus of this new approach is on the design of optimization problems that allow certain parameters that define a cone associated with a standard CLF to be decision variables. In this way non-monotonicity of the CLF is explicitly linked with a decision variable that can be optimized on-line. Conservativeness is significantly reduced compared to classical CLFs, which makes \\\\emph{flexible CLFs} more suitable for stabilization of constrained discrete-time nonlinear systems and real-time control. The purpose of this overview is to highlight the potential of flexible CLFs for real-time control of fast mechatronic systems, with sampling periods below one millisecond, which are widely employed in aerospace and automotive applications.',\n",
       "  'len': 1445},\n",
       " {'abstract': \"Current approaches to the engineering of space software such as satellite control systems are based around the development of feedback controllers using packages such as MatLab's Simulink toolbox. These provide powerful tools for engineering real time systems that adapt to changes in the environment but are limited when the controller itself needs to be adapted. We are investigating ways in which ideas from temporal logics and agent programming can be integrated with the use of such control systems to provide a more powerful layer of autonomous decision making. This paper will discuss our initial approaches to the engineering of such systems.\",\n",
       "  'len': 661},\n",
       " {'abstract': 'The SPaCIFY project, which aims at bringing advances in MDE to the satellite flight software industry, advocates a top-down approach built on a domain-specific modeling language named Synoptic. In line with previous approaches to real-time modeling such as Statecharts and Simulink, Synoptic features hierarchical decomposition of application and control modules in synchronous block diagrams and state machines. Its semantics is described in the polychronous model of computation, which is that of the synchronous language Signal.',\n",
       "  'len': 542},\n",
       " {'abstract': \"NASA's new age of space exploration augurs great promise for deep space exploration missions whereby spacecraft should be independent, autonomous, and smart. Nowadays NASA increasingly relies on the concepts of autonomic computing, exploiting these to increase the survivability of remote missions, particularly when human tending is not feasible. Autonomic computing has been recognized as a promising approach to the development of self-managing spacecraft systems that employ onboard intelligence and rely less on control links. The Autonomic System Specification Language (ASSL) is a framework for formally specifying and generating autonomic systems. As part of long-term research targeted at the development of models for space exploration missions that rely on principles of autonomic computing, we have employed ASSL to develop formal models and generate functional prototypes for NASA missions. This helps to validate features and perform experiments through simulation. Here, we discuss our work on developing such missions with ASSL.\",\n",
       "  'len': 1055},\n",
       " {'abstract': \"Different possible sources are discussed for enhancement of the calculation time when solving ordinary differential equations systems to forecast space objects' motion. This paper presents an approach for building an integrator of ordinary differential equations systems for simultaneous solving of motion equations of multiple objects. A parallelization of calculation on the base of threads is offered. A method for synchronization is presented. The technological advance and the invasion of multi-core processors make actual the examined approach for developing an integrator of ordinary differential equations systems.\",\n",
       "  'len': 633},\n",
       " {'abstract': \"An approach is treated for numerical integration of ordinary differential equations systems of the first order with choice of a computation scheme, ensuring the required local precision. The treatment is made on the basis of schemes of Runge-Kutta-Fehlberg type. Criteria are proposed as well as a method for the realization of the choice of an 'optimum' scheme. The effectiveness of the presented approach to problems in the field of satellite dynamics is illustrated by results from a numerical experiment. These results refer to a case when a satisfactory global stability of the solution for all treated cases is available. The effectiveness has been evaluated as good, especially when solving multi-variable problems in the sphere of simulation modelling.\",\n",
       "  'len': 771},\n",
       " {'abstract': \"An analytical method is proposed in this work for verification whether an artificial earth satellite during its orbital motion passes over a region of the earth surface. The method is based on undisturbed Keppler's approximation of the orbit and approximation of the region by a circular segment S. In order to define the situational condition, a conic surface is used with apex in the earth centre, cutting out the circular segment. The tangents of the conical surface with Keppler's plane determine the time intervals in which the satellite trace on the earth surface occurs inside the segment S. The transformation of these tangents in the plane of Keppler's orbit and the determination of their crossing points with Keppler's ellipse lies in the basis of the examined method.\",\n",
       "  'len': 790},\n",
       " {'abstract': 'When engineering complex and distributed software and hardware systems (increasingly used in many sectors, such as manufacturing, aerospace, transportation, communication, energy, and health-care), quality has become a big issue, since failures can have economics consequences and can also endanger human life. Model-based specifications of a component-based system permit to explicitly model the structure and behaviour of components and their integration. In particular Software Architectures (SA) has been advocated as an effective means to produce quality systems. In this chapter by combining different technologies and tools for analysis and development, we propose an architecture-centric model-driven approach to validate required properties and to generate the system code. Functional requirements are elicited and used for identifying expected properties the architecture shall express. The architectural compliance to the properties is formally demonstrated, and the produced architectural model is used to automatically generate the Java code. Suitable transformations assure that the code is conforming to both structural and behavioural SA constraints. This chapter describes the process and discusses how some existing tools and languages can be exploited to support the approach.',\n",
       "  'len': 1306},\n",
       " {'abstract': \"Context. Near-Earth asteroid-comet transition object 107P/ (4015) Wilson-Harrington is a possible target of the joint European Space Agency (ESA) and Japanese Aerospace Exploration Agency (JAXA) Marco Polo sample return mission. Physical studies of this object are relevant to this mission, and also to understanding its asteroidal or cometary nature. Aims. Our aim is to obtain significant new constraints on the surface thermal properties of this object. Methods. We present mid-infrared photometry in two filters (16 and 22 microns) obtained with NASA's Spitzer Space Telescope on February 12, 2007, and results from the application of the Near Earth Asteroid Thermal Model (NEATM).We obtained high S/N in two mid-IR bands allowing accurate measurements of its thermal emission. Results. We obtain a well constrained beaming parameter (eta = 1.39 +/- 0.26) and obtain a diameter and geometric albedo of D = 3.46 +/- 0.32 km, and pV = 0.059 +/- 0.011. We also obtain similar results when we apply this best-fitting thermal model to single-band mid-IR photometry reported by Campins et al. (1995), Kraemer et al. (2005) and Reach et al. (2007). Conclusions. The albedo of 4015 Wilson-Harrington is low, consistent with those of comet nuclei and primitive C-, P-, D-type asteorids. We establish a rough lower limit for the thermal inertia of W-H of 60 Jm^-2s^(-0.5)K^-1 when it is at r=1AU, which is slightly over the limit of 30 Jm^-2s^(-0.5)K-1 derived by Groussin et al. (2009) for the thermal inertia of the nucleus of comet 22P/Kopff.\",\n",
       "  'len': 1550},\n",
       " {'abstract': 'In aerospace and defense, training is being carried out on the web by viewing PowerPoint presentations, manuals and videos that are limited in their ability to convey information to the technician. Interactive training in the form of 3D is a more cost effective approach compared to creation of physical simulations and mockups. This paper demonstrates how training using interactive 3D simulations in elearning achieves a reduction in the time spent in training and improves the efficiency of a trainee performing the installation or removal.',\n",
       "  'len': 554},\n",
       " {'abstract': 'The research project has been made for mathematical modeling of aerospace system Space-to-Surface for avoid intercepting process by flight objects Surface-to-Air. The research has been completed and created mathematical models which used for research and statistical analysis. In mathematical modeling has been including a few models: Model of atmosphere, Model of speed of sound, Model of flight head in space, Model of flight in atmosphere, Models of navigation and guidance, Model and statistical analysis of approximation of aerodynamic characteristics. Modeling has been created for a Space-to-Surface system defined for an optimal trajectory in terminal phase. The modeling includes models for simulation atmosphere, aerodynamic flight and navigation by an infrared system. The modeling simulation includes statistical analysis of the modeling results.',\n",
       "  'len': 869},\n",
       " {'abstract': 'The erosion of lunar soil by rocket exhaust plumes is investigated experimentally. This has identified the diffusion-driven flow in the bulk of the sand as an important but previously unrecognized mechanism for erosion dynamics. It has also shown that slow regime cratering is governed by the recirculation of sand in the widening geometry of the crater. Scaling relationships and erosion mechanisms have been characterized in detail for the slow regime. The diffusion-driven flow occurs in both slow and fast regime cratering. Because diffusion-driven flow had been omitted from the lunar erosion theory and from the pressure cratering theory of the Apollo and Viking era, those theories cannot be entirely correct.',\n",
       "  'len': 727},\n",
       " {'abstract': \"The paper provides a review of A.M. Mathai's applications of the theory of special functions, particularly generalized hypergeometric functions, to problems in stellar physics and formation of structure in the Universe and to questions related to reaction, diffusion, and reaction-diffusion models. The essay also highlights Mathai's recent work on entropic, distributional, and differential pathways to basic concepts in statistical mechanics, making use of his earlier research results in information and statistical distribution theory. The results presented in the essay cover a period of time in Mathai's research from 1982 to 2008 and are all related to the thematic area of the gravitationally stabilized solar fusion reactor and fractional reaction-diffusion, taking into account concepts of non-extensive statistical mechanics. The time period referred to above coincides also with Mathai's exceptional contributions to the establishment and operation of the Centre for Mathematical Sciences, India, as well as the holding of the United Nations (UN)/European Space Agency (ESA)/National Aeronautics and Space Administration (NASA) of the United States/ Japanese Aerospace Exploration Agency (JAXA) Workshops on basic space science and the International Heliophysical Year 2007, around the world. Professor Mathai's contributions to the latter, since 1991, are a testimony for his social conscience applied to international scientific activity.\",\n",
       "  'len': 1463},\n",
       " {'abstract': \"The global one-dimensional quantum gravity is the model of quantum gravity which arises from the global one-dimensionality conjecture within quantum general relativity, first considered by the author in 2010 and then in 2012. In this model the global dimension is a determinant of a metric of three-dimensional space embedded into an enveloping Lorentizan four-dimensional spacetime. In 2012, it has already been presented by the author that this model can be extended to any Lorentzian D + 1-dimensional spacetime, where D is a dimension of space, and resulting in the global one-dimensional model of a higher dimensional quantum gravity. The purely quantum-mechanical part of this model is a minimal effective model within the quantum geometrodynamics, introduced by J.A. Wheeler and B.S. DeWitt in the 1960s, but the effective potential is manifestly different from the one considered by Wheeler & DeWitt. Moreover, in our model the wave functionals solving the quantum gravity are one-variable smooth functions and, therefore, the troublesome mathematical technique of the Feynman functional integration present in the Hawking formulation of quantum gravity, is absent is this model, what makes it a mathematically consistent theory of quantum gravitation. In this paper, we discuss in some detail a certain part of the global one-dimensional model already proposed in 2010, and then developed in 2012. The generalized functional expansion of the effective potential and the residual approximation, which describe the embedded spaces which are maximally symmetric three-dimensional Einstein's manifolds, whose lead to the Newton-Coulomb type potential in the quantum gravity model, are considered. Furthermore, scenarios related to few selected specific forms of the effective potential are suggested as physically interesting and discussed.\",\n",
       "  'len': 1856},\n",
       " {'abstract': 'Based on the experimental data and estimations of the charged leptons and quarks masses, a close power law with exponent 3/4 has been found, connecting charged leptons masses and up quarks masses. A similar mass relation has been suggested for the masses of neutral leptons and down quarks. The latter mass relation and the results of the solar and atmospheric neutrino experiments have been used for prediction of neutrino masses. The obtained masses of electron neutrino, muon neutrino and tau neutrino are 0.0003 eV, 0.003 eV and 0.04 eV, respectively. These values are compatible with the recent experimental data and support the normal hierarchy of neutrino masses.',\n",
       "  'len': 681},\n",
       " {'abstract': \"In response to the interest to re-use Palapa B2R satellite nearing its End of Life (EOL) time, an idea to incline the satellite orbit in order to cover a new region has emerged in the recent years. As a prolate dual-spin vehicle, Palapa B2R has to be stabilized against its internal energy dissipation effect. This work is focused on analyzing the dynamics of the reusable satellite in its inclined orbit. The study discusses in particular the stability of the prolate dual-spin satellite under the effect of perturbed field of gravitation due to the inclination of its elliptical orbit. Palapa B2R physical data was substituted into the dual-spin's equation of motion. The coefficient of zonal harmonics J2 was induced into the gravity-gradient moment term that affects the satellite attitude. The satellite's motion and attitude were then simulated in the perturbed gravitational field by J2, with the variation of orbit's eccentricity and inclination. The analysis of the satellite dynamics and its stability was conducted for designing a control system for the vehicle in its new inclined orbit.\",\n",
       "  'len': 1110},\n",
       " {'abstract': 'In general, most of communication satellites were designed to be operated in geostationary orbit. And many of them were designed in prolate dual-spin configuration. As a prolate dual-spin vehicle, they have to be stabilized against their internal energy dissipation effect. Several countries that located in southern hemisphere, has shown interest to use communication satellite. Because of those countries southern latitude, an idea emerged to incline the communication satellite (due to its prolate dualspin configuration) in elliptical orbit. This work is focused on designing Attitude Stability System for prolate dual-spin satellite in the effect of perturbed field of gravity due to the inclination of its elliptical orbit. DANDE (De-spin Active Nutation Damping Electronics) provides primary stabilization method for the satellite in its orbit. Classical Control Approach is used for the iteration of DANDE parameters. The control performance is evaluated based on time response analysis.',\n",
       "  'len': 1006},\n",
       " {'abstract': 'The author offers a new kind of thermonuclear reflect reactor. The remarkable feature of this new reactor is a three net AB reflector, which confines the high temperature plasma. The plasma loses part of its energy when it contacts with the net but this loss can be compensated by an additional permanent plasma heating. When the plasma is rarefied (has a small density), the heat flow to the AB reflector is not large and the temperature in the triple reflector net is lower than 2000 - 3000 K. This offered AB-reactor has significantly less power then the currently contemplated power reactors with magnetic or inertial confinement (hundreds-thousands of kW, not millions of kW). But it is enough for many vehicles and ships and particularly valuable for tunnelers, subs and space apparatus, where air to burn chemical fuel is at a premium or simply not available. The author has made a number of innovations in this reactor, researched its theory, developed methods of computation, made a sample computation of typical project. The main point of preference for the offered reactor is its likely cheapness as a power source. Key words: Micro-thermonuclear reactor, Multi-reflex AB-thermonuclear reactor, Self-magnetic AB-thermonuclear reactor, aerospace thermonuclear engine.',\n",
       "  'len': 1288},\n",
       " {'abstract': 'This paper presents an experimental study of the fretting crack nucleation threshold, expressed in terms of loading conditions, with a cylinder/plane contact. The studied material is a damage tolerant aluminium alloy widely used in the aerospace application. Since in industrial problems, the surface quality is often variable, the impact of a unidirectional roughness is investigated via varying the roughness of the counter body in the fretting experiments. As expected, experimental results show a large effect of the contact roughness on the crack nucleation conditions. Rationalisation of the crack nucleation boundary independently of the studied roughnesses was successfully obtained by introducing the concept of effective contact area. This does show that the fretting crack nucleation of the studied material can be efficiently described by the local effective loadings inside the contact. Analytical prediction of the crack nucleation is presented with the Smith-Watson-Topper (SWT) parameter and size effect is also studied and discussed.',\n",
       "  'len': 1061},\n",
       " {'abstract': \"Time-averaged series of granulation images are analysed using COLIBRI, a purpose-adapted version of a code originally developed to detect straight or curvilinear features in aerospace images. The algorithm of image processing utilises a nonparametric statistical criterion that identifies a straight-line segment as a linear feature (lineament) if the photospheric brightness at a certain distance from this line is on both sides stochastically lower or higher than at the line itself. Curvilinear features can be detected as chains of lineaments, using a criterion modified in some way. Once the input parameters used by the algorithm are properly adjusted, the algorithm highlights ``ridges'' and ``trenches'' in the relief of the brightness field, drawing white and dark lanes. The most remarkable property of the trenching patterns is a nearly-universally-present parallelism of ridges and trenches. Since the material upflows are brighter than the downflows, the alternating parallel light and dark lanes should reflect the presence of roll convection in the subphotospheric layers. If the numerous images processed by us are representative, the patterns revealed suggest a widespread occurrence of roll convection in the outer solar convection zone. In particular, the roll systems could form the fine structure of larger-scale, supergranular and/or mesogranular convection flows. Granules appear to be overheated blobs of material that could develop in convection rolls due to some instabilities of roll motion.\",\n",
       "  'len': 1529},\n",
       " {'abstract': 'The focus in this paper is an analysis of existing state of the arts directed toward the development of the next generation of vibration damping systems. The research work concentrates on an investigation related to nanoparticles/fibres/tubes-reinforced materials and coatings dynamic characterization and modeling of the fundamental phenomena that control relationships between structure and damping/mechanical properties of the materials. We simulated composite materials using finite element and mesh free methods, using a hollow shell representation of the individual nanotube/fiber. Results of the research work will provide a platform for the development of nanoparticle-reinforced damping materials that are light-weight, vibration and shock resistant. The outcome of the research work is expected to have wide-ranging technical benefits with direct relevance to industry in areas of transportation (aerospace, automotive, rail), electronics and civil infrastructure development.',\n",
       "  'len': 997},\n",
       " {'abstract': 'There are two main methods of nulcear fusion: inertial confinement fusion (ICF) and magnetic confinement fusion (MCF). Existing thermonuclear reactors are very complex, expensive, large, and heavy. They cannot achieve the Lawson creterion. The author offers an innovation. ICF has on the inside surface of the shell-shaped combustion chamber a covering of small Prism Reflectors (PR) and plasma reflector. These prism reflectors have a noteworthy advantage, in comparison with conventional mirror and especially with conventional shell: they multi-reflect the heat and laser radiation exactly back into collision with the fuel target capsule (pellet). The plasma reflector reflects the Bremsstrahlung radiation. The offered innovation decreases radiation losses, creates significant radiation pressure and increases the reaction time. The Lawson criterion increases by hundreds of times. The size, cost, and weight of a typical installation will decrease by tens of times. The author is researching the efficiency of these innovations. Keywords: Thermonuclear reactor, Multi-reflex AB-thermonuclear reactor, aerospace thermonuclear engine. This work is presented as paper AIAA-2006-7225 to Space-2006 Conference, 19-21 September, 2006, San Jose, CA, USA.',\n",
       "  'len': 1265},\n",
       " {'abstract': 'Howard Brenner has recently proposed modifications to the Navier-Stokes equations that relate to a diffusion of fluid volume that would be significant for flows with high density gradients. In a previous paper (Greenshields & Reese, 2007), we found these modifications gave good predictions of the viscous structure of shock waves in argon in the range Mach 1.0-12.0 (while conventional Navier-Stokes equations are known to fail above about Mach 2). However, some areas of concern with this model were a somewhat arbitrary choice of modelling coefficient, and potentially unphysical and unstable solutions. In this paper, we therefore present slightly different modifications to include molecule mass diffusion fully in the Navier-Stokes equations. These modifications are shown to be stable and produce physical solutions to the shock problem of a quality broadly similar to those from the family of extended hydrodynamic models that includes the Burnett equations. The modifications primarily add a diffusion term to the mass conservation equation, so are at least as simple to solve as the Navier-Stokes equations; there are none of the numerical implementation problems of conventional extended hydrodynamics models, particularly in respect of boundary conditions. We recommend further investigation and testing on a number of different benchmark non-equilibrium flow cases.',\n",
       "  'len': 1389},\n",
       " {'abstract': 'The author offers several innovations that he first suggested publicly early in 1983 for the AB multi-reflex engine, space propulsion, getting energy from plasma, etc. (see: A. Bolonkin, Non-Rocket Space Launch and Flight, Elsevier, London, 2006, Chapters 12, 3A). It is the micro-thermonuclear AB-Reactors. That is new micro-thermonuclear reactor with very small fuel pellet that uses plasma confinement generated by multi-reflection of laser beam or its own magnetic field. The Lawson criterion increases by hundreds of times. The author also suggests a new method of heating the power-making fuel pellet by outer electric current as well as new direct method of transformation of ion kinetic energy into harvestable electricity. These offered innovations dramatically decrease the size, weight and cost of thermonuclear reactor, installation, propulsion system and electric generator. Non-industrial countries can produce these researches and constructions. Currently, the author is researching the efficiency of these innovations for two types of the micro-thermonuclear reactors: multi-reflection reactor (ICF) and self-magnetic reactor (MCF).',\n",
       "  'len': 1159},\n",
       " {'abstract': 'Current research suggests the use of a liner quadratic performance index for optimal control of regulators in various applications. Some examples include correcting the trajectory of rocket and air vehicles, vibration suppression of flexible structures, and airplane stability. In all these cases, the focus is in suppressing/decreasing system deviations rapidly. However, if one compares the Linear Quadratic Regulator (LQR) solution with optimal solutions (minimum time), it is seen that the LQR solution is less than optimal in some cases indeed (3-6) times that obtained using a minimum time solution. Moreover, the LQR solution is sometimes unacceptable in practice due to the fact that values of control extend beyond admissible limits and thus the designer must choose coefficients in the linear quadratic form, which are unknown. The authors suggest methods which allow finding a quasi-optimal LQR solution with bounded control which is closed to the minimum time solution. They also remand the process of the minimum time decision. Keywords: Optimal regulator, minimum time controller, Linear Quadratic Regulator (LQR). -- This paper is declared a work of the U.S. Government and not subject to copyright protection in the USA. The manuscript is accepted as paper AIAA-2003-6638 by 2nd AIAA Unmanned Unlimited Systems, Technologies, and Operations-Aerospace, Land, and See Conference and Workshop - Exhibit, San Diego, California, USA, 15-18 Sep. 2003.',\n",
       "  'len': 1472},\n",
       " {'abstract': 'An observational study was conducted on a professional designer working on a design project in aerospace industry. The protocol data were analyzed in order to gain insight into the actions the designer used for the development of a solution to the corresponding problem. Different processes are described: from the \"simple\" evocation of a solution existing in memory, to the elaboration of a \"new\" solution out of mnesic entities without any clear link to the current problem. Control is addressed in so far as it concerns the priority among the different types of development processes: the progression from evocation of a \"standard\" solution to elaboration of a \"new\" solution is supposed to correspond to the resulting order, that is, the one in which the designer\\'s activity proceeds. Short discussions of * the double status of \"problem\" and \"solution,\" * the problem/solution knowledge units in memory and their access, and * the different abstraction levels on which problem and solution representations are developed, are illustrated by the results.',\n",
       "  'len': 1068},\n",
       " {'abstract': 'The new proposed \"energy gradient theory,\" which physically explains the phenomena of flow instability and turbulent transition in shear flows and has been shown to be valid for parallel flows, is extended to curved flows in this study. Then, three important theorems for fluid dynamics are deduced. These theorems are (1) Potential flow (inviscid and irrotational) is stable. (2) Inviscid rotational (nonzero vorticity) flow is unstable. (3) Velocity profile with an inflectional point is unstable when there is no work input or output to the system, for both inviscid and viscous flows. These theorems are, for the first time, deduced, and are of great significance for the understanding of generation of turbulence and the explanation of complex flows. From these results, it is concluded that the classical Rayleigh theorem (1880) on inflectional velocity instability of inviscid flows is incorrect which has last for more than a century. It is demonstrated that existence of inflection point on velocity profile is a sufficient condition, but not a necessary condition for flow instability, for both inviscid and viscous flows. The paradox of dual role of viscosity has been resolved: viscosity has only stable role. Why the paradox appeared is due to that the Rayleigh Theorem of inviscid flows by linear analysis is incorrect.',\n",
       "  'len': 1344},\n",
       " {'abstract': 'More than 150 years after their invention by Hamilton, quaternions are now widely used in the aerospace and computer animation industries to track the paths of moving objects undergoing three-axis rotations. It is shown here that they provide a natural way of selecting an appropriate ortho-normal frame -- designated the quaternion-frame -- for a particle in a Lagrangian flow, and of obtaining the equations for its dynamics. How these ideas can be applied to the three-dimensional Euler fluid equations is then considered. This work has a bearing on the issue of whether the Euler equations develop a singularity in a finite time. Some of the literature on this topic is reviewed, which includes both the Beale-Kato-Majda theorem and associated work on the direction of vorticity by both Constantin, Fefferman & Majda and Deng, Hou and Yu. It is then shown how the quaternion formulation provides a further direction of vorticity result using the Hessian of the pressure.',\n",
       "  'len': 985},\n",
       " {'abstract': \"Two series of solar-granulation images -- the La Palma series of 5 June 1993 and the SOHO MDI series of 17--18 January 1997 -- are analysed both qualitatively and quantitatively. New evidence is presented for the existence of long-lived, quasi-regular structures (first reported by Getling and Brandt (2002)), which no longer appear unusual in images averaged over 1--2-h time intervals. Such structures appear as families of light and dark concentric rings or families of light and dark parallel strips (``ridges'' and ``trenches'' in the brightness distributions). In some cases, rings are combined with radial ``spokes'' and can thus form ``web'' patterns. The characteristic width of a ridge or trench is somewhat larger than the typical size of granules. Running-average movies constructed from the series of images are used to seek such structures. An algorithm is developed to obtain, for automatically selected centres, the radial distributions of the azimuthally averaged intensity, which highlight the concentric-ring patterns. We also present a time-averaged granulation image processed with a software package intended for the detection of geological structures in aerospace images. A technique of running-average-based correlations between the brightness variations at various points of the granular field is developed and indications are found for a dynamical link between the emergence and sinking of hot and cool parcels of the solar plasma. In particular, such a correlation analysis confirms our suggestion that granules -- overheated blobs -- may repeatedly emerge on the solar surface. Based on our study, the critical remarks by Rast (2002) on the original paper by Getling and Brandt (2002) can be dismissed.\",\n",
       "  'len': 1741},\n",
       " {'abstract': 'We present the first Spitzer Infrared Spectrograph (IRS; The IRS was a collaborative venture between Cornell University and Ball Aerospace Corporation funded by NASA through the Jet Propulsion Laboratory and the Ames Research Center.) observations of the disks around classical T Tauri stars: spectra in the 5.2-30 micron range of six stars. The spectra are dominated by emission features from amorphous silicate dust, and a continuous component from 5 to 8 microns that in most cases comprises an excess above the photosphere throughout our spectral range. There is considerable variation in the silicate feature/continuum ratio, which implies variations of inclination, disk flaring, and stellar mass accretion rate. In most of our stars, structure in the silicate feature suggests the presence of a crystalline component. In one, CoKu Tau/4, no excess above the photosphere appears at wavelengths shortward of the silicate features, similar to 10 Myr old TW Hya, Hen 3-600, and HR 4796A. This indicates the optically thick inner disk is largely absent. The silicate emission features with peaks at 9.7 and 18 microns indicate small dust grains are present. The extremely low 10-20 micron color temperature of the dust excess, 135 K, indicates these grains are located more than 10 AU from the star. These features are suggestive of gravitational influence by planets or close stellar companions and grain growth in the region within 10 AU of the star, somewhat surprising for a star this young (1 Myr).',\n",
       "  'len': 1516},\n",
       " {'abstract': 'The 8-14 micron emission spectra of 12 T Tauri stars in the Taurus/Auriga dark clouds and in the TW Hydrae association obtained with the Infrared Spectrograph (IRS; The IRS is a collaborative venture between Cornell University and Ball Aerospace Corporation funded by NASA through the Jet Propulsion Laboratory and the Ames Research Center.) on board Spitzer are analyzed. Assuming the 10 micron features originate from silicate grains in the optically thin surface layers of T Tauri disks, the 8-14 micron dust emissivity for each object is derived from its Spitzer spectrum. The emissivities are fit with the opacities of laboratory analogs of cosmic dust. The fits include small nonspherical grains of amorphous silicates (pyroxene and olivine), crystalline silicates (forsterite and pyroxene), and quartz, together with large fluffy amorphous silicate grains. A wide range in the fraction of crystalline silicate grains as well as large silicate grains among these stars are found. The dust in the transitional-disk objects CoKu Tau/4, GM Aur, and DM Tau has the simplest form of silicates, with almost no hint of crystalline components and modest amounts of large grains. This indicates that the dust grains in these objects have been modified little from their origin in the interstellar medium. Other stars show various amounts of crystalline silicates, similar to the wide dispersion of the degree of crystallinity reported for Herbig Ae/Be stars of mass <2.5 solar masses. Late spectral type, low-mass stars can have significant fractions of crystalline silicate grains. Higher quartz mass fractions often accompany low amorphous olivine-to-amorphous pyroxene ratios. It is also found that lower contrast of the 10 micron feature accompanies greater crystallinity.',\n",
       "  'len': 1784},\n",
       " {'abstract': \"The Laser Astrometric Test of Relativity (LATOR) is a Michelson-Morley-type experiment designed to test the Einstein's general theory of relativity in the most intense gravitational environment available in the solar system -- the close proximity to the Sun. By using independent time-series of highly accurate measurements of the Shapiro time-delay (laser ranging accurate to 1 cm) and interferometric astrometry (accurate to 0.1 picoradian), LATOR will measure gravitational deflection of light by the solar gravity with accuracy of 1 part in a billion, a factor ~30,000 better than currently available. LATOR will perform series of highly-accurate tests of gravitation and cosmology in its search for cosmological remnants of scalar field in the solar system. We present science, technology and mission design for the LATOR mission.\",\n",
       "  'len': 846},\n",
       " {'abstract': 'We advocate in this paper the use of grid-based infrastructures that are designed for seamless approaches to the numerical expert users, i.e., the multiphysics applications designers. It relies on sophisticated computing environments based on computing grids, connecting heterogeneous computing resources: mainframes, PC-clusters and workstations running multiphysics codes and utility software, e.g., visualization tools. The approach is based on concepts defined by the HEAVEN* consortium. HEAVEN is a European scientific consortium including industrial partners from the aerospace, telecommunication and software industries, as well as academic research institutes. Currently, the HEAVEN consortium works on a project that aims to create advanced services platforms. It is intended to enable \"virtual private grids\" supporting various environments for users manipulating a suitable high-level interface. This will become the basis for future generalized services allowing the integration of various services without the need to deploy specific grid infrastructures.',\n",
       "  'len': 1079},\n",
       " {'abstract': 'We present the formulation for finding the distribution of eigenstrains, i.e. the sources of residual stress, from a set of measurements of residual elastic strain (e.g. by diffraction), or residual stress, or stress redistribution, or distortion. The variational formulation employed seeks to achieve the best agreement between the model prediction and some measured parameters in the sense of a minimum of a functional given by a sum over the entire set of measurements. The advantage of this approach lies in its flexibility: different sets of measurements and information about different components of the stress-strain state can be incorporated. We demonstrate the power of the technique by analysing experimental data for welds in thin sheet of a nickel superalloy aerospace material. Very good agreement can be achieved between the prediction and the measurement results without the necessity of using iterative solution. In practice complete characterisation of residual stress states is often very difficult, due to limitations of facility access, measurement time or specimen dimensions. Implications of the new technique for experimental analysis are all the more significant, since it allows the reconstruction of the entire stress state from incomplete sets of data.',\n",
       "  'len': 1290},\n",
       " {'abstract': 'The ratio between the proton and electron masses is shown to be close to the ratio between the strong and electromagnetic interaction coupling constants at Extremely Low Energy (ELE). Based on the experimental data, this relation has been extended for the weak and gravitational interactions, too. Thus, a mass relation has been found, according to which the rest mass of the Lightest Free Massive Stable Particle (LFMSP), acted upon by a particular interaction, is proportional to the coupling constant of the respective interaction at ELE. On the basis of this mass relation, the electron neutrino and graviton masses have been approximately estimated to 2.1x10^(-4) eV/c^2 and 2.3x10^(-34) eV/c^2, respectively. The last value is of the order of the magnitude of hbar*H/c^2, where H is the Hubble constant and hbar is the reduced Planck constant. It is worth noting that this value has been obtained by fundamental constants only, without consideration of any cosmological models.',\n",
       "  'len': 994},\n",
       " {'abstract': 'In this study, we analyze the aerospace stocks prices in order to characterize the sector behavior. The data analyzed cover the period from January 1987 to April 1999. We present a new index for the aerospace sector and we investigate the statistical characteristics of this index. Our results show that this index is well described by Tsallis distribution. We explore this result and modify the standard Value-at-Risk (VaR), financial risk assessment methodology in order to reflect an asset which obeys Tsallis non-extensive statistics.',\n",
       "  'len': 549},\n",
       " {'abstract': 'The complex spatiotemporal patterns of atmospheric flows resulting from the cooperative existence of fluctuations ranging in size from millimeters to thousands of kilometers are found to exhibit long-range spatial and temporal correlations manifested as the selfsimilar fractal geometry to the global cloud cover pattern and the inverse power law form for the atmospheric eddy energy spectrum. Such long-range spatial and temporal correlations are ubiquitous to extended natural dynamical systems and is a signature of the strange attractor design characterizing deterministic chaos or self-organized criticality. The unified network of global atmospheric circulations is analogous to the neural networks of the human brain.',\n",
       "  'len': 735},\n",
       " {'abstract': \"The Fizeau Interferometer Testbed (FIT) is a collaborative effort between NASA's Goddard Space Flight Center, the Naval Research Laboratory, Sigma Space Corporation, and the University of Maryland. The testbed will be used to explore the principles of and the requirements for the full, as well as the pathfinder, Stellar Imager mission concept. It has a long term goal of demonstrating closed-loop control of a sparse array of numerous articulated mirrors to keep optical beams in phase and optimize interferometric synthesis imaging. In this paper we present the optical and data acquisition system design of the testbed, and discuss the wavefront sensing and control algorithms to be used. Currently we have completed the initial design and hardware procurement for the FIT. The assembly and testing of the Testbed will be underway at Goddard's Instrument Development Lab in the coming months.\",\n",
       "  'len': 907},\n",
       " {'abstract': 'The Calibration enhancement effort for the Space Telescope Imaging Spectrograph (STIS) aims to improve data calibration via the application of physical modelling techniques. We describe here a model of the Charge Transfer process during read-out of modern Charge Coupled Devices, and its application to data from STIS. The model draws upon previous investigations of this process and, in particular, the trapping and emission model developed by Robert Philbrick of Ball Aerospace. Early comparison to calibration data is encouraging. Essentially, a physical description of the STIS CCD combined with the physics of known defects in the silicon lattice expected to arise in a hostile radiation environment, is enough to yield results which approximately match real data. Uncertainties remain, however, in the details of the model and the physical description of STIS.',\n",
       "  'len': 877},\n",
       " {'abstract': 'In this paper we outline a software development process for safety-critical systems that aims at combining some of the specific strengths of model-based development with those of programming language based development using safety-critical subsets of Ada. Model-based software development and model-based test case generation techniques are combined with code generation techniques and tools providing a transition from model to code both for a system itself and for its test cases. This allows developers to combine domain-oriented, model-based techniques with source code based validation techniques, as required for conformity with standards for the development of safety-critical software, such as the avionics standard RTCA/DO-178B. We introduce the AutoFocus and Validator modeling and validation toolset and sketch its usage for modeling, test case generation, and code generation in a combined approach, which is further illustrated by a simplified leading edge aerospace model with built-in fault tolerance.',\n",
       "  'len': 1027},\n",
       " {'abstract': \"We are developing a Wide-Field Imaging Interferometry Testbed (WIIT) in support of design studies for NASA's future space interferometry missions, in particular the SPIRIT and SPECS far-infrared/submillimeter interferometers. WIIT operates at optical wavelengths and uses Michelson beam combination to achieve both wide-field imaging and high-resolution spectroscopy. It will be used chiefly to test the feasibility of using a large-format detector array at the image plane of the sky to obtain wide-field interferometry images through mosaicing techniques. In this setup each detector pixel records interferograms corresponding to averaging a particular pointing range on the sky as the optical path length is scanned and as the baseline separation and orientation is varied. The final image is constructed through spatial and spectral Fourier transforms of the recorded interferograms for each pixel, followed by a mosaic/joint-deconvolution procedure of all the pixels. In this manner the image within the pointing range of each detector pixel is further resolved to an angular resolution corresponding to the maximum baseline separation for fringe measurements. We present the motivation for building the testbed, show the optical, mechanical, control, and data system design, and describe the image processing requirements and algorithms. WIIT is presently under construction at NASA's Goddard Space Flight Center.\",\n",
       "  'len': 1430},\n",
       " {'abstract': \"Roberts' model of lunar soil erosion beneath a landing rocket has been updated in several ways to predict the effects of future lunar landings. The model predicts, among other things, the number of divots that would result on surrounding hardware due to the impact of high velocity particulates, the amount and depth of surface material removed, the volume of ejected soil, its velocity, and the distance the particles travel on the Moon. The results are compared against measured results from the Apollo program and predictions are made for mitigating the spray around a future lunar outpost.\",\n",
       "  'len': 604},\n",
       " {'abstract': 'Manufacturing, automotive, and aerospace environments use embedded systems for control and automation and need to fulfill strict real-time guarantees. To facilitate more efficient business processes and remote control, such devices are being connected to IP networks. Due to the difficulty in predicting network packets and the interrelated workloads of interrupt handlers and drivers, devices controlling time critical processes stand under the risk of missing process deadlines when under high network loads. Additionally, devices at the edge of large networks and the internet are subject to a high risk of load spikes and network packet overloads. In this paper, we investigate strategies to detect network packet overloads in real-time and present four approaches to adaptively mitigate local deadline misses. In addition to two strategies mitigating network bursts with and without hysteresis, we present and discuss two novel mitigation algorithms, called Budget and Queue Mitigation. In an experimental evaluation, all algorithms showed mitigating effects, with the Queue Mitigation strategy enabling most packet processing while preventing lateness of critical tasks.',\n",
       "  'len': 1187},\n",
       " {'abstract': 'Powder bed fusion additive manufacturing (PBFAM) of metals has the potential to enable new paradigms of product design, manufacturing and supply chains while accelerating the realization of new technologies in the medical, aerospace, and other industries. Currently, wider adoption of PBFAM is held back by difficulty in part qualification, high production costs and low production rates, as extensive process tuning, post-processing, and inspection are required before a final part can be produced and deployed. Physics-based modeling and predictive simulation of PBFAM offers the potential to advance fundamental understanding of physical mechanisms that initiate process instabilities and cause defects. In turn, these insights can help link process and feedstock parameters with resulting part and material properties, thereby predicting optimal processing conditions and inspiring the development of improved processing hardware, strategies and materials. This work presents recent developments of our research team in the modeling of metal PBFAM processes spanning length scales, namely mesoscale powder modeling, mesoscale melt pool modeling, macroscale thermo-solid-mechanical modeling and microstructure modeling. Ongoing work in experimental validation of these models is also summarized. In conclusion, we discuss the interplay of these individual submodels within an integrated overall modeling approach, along with future research directions.',\n",
       "  'len': 1466},\n",
       " {'abstract': 'Science-based simulation tools such as Finite Element (FE) models are routinely used in scientific and engineering applications. While their success is strongly dependent on our understanding of underlying governing physical laws, they suffer inherent limitations including trade-off between fidelity/accuracy and speed. The recent rise of Machine Learning (ML) proposes a theory-agnostic paradigm. In complex multi-physics problems, however, creating large enough datasets for successful training of ML models has proven to be challenging. One promising strategy to bridge the divide between these approaches and take advantage of their respective strengths is Theory-Guided Machine Learning (TGML) which aims to integrate physical laws into ML algorithms. In this paper, three case studies on thermal management during processing of advanced composites are presented and studied using FE, ML and TGML. A structured approach to incrementally adding increasingly complex physics to training of TGML model is presented. The benefits of TGML over ML models are seen in more accurate predictions, particularly outside the training region, and ability to train with small datasets. One benefit of TGML over FE is significant speed improvement to potentially develop real-time feedback systems. A recent successful implementation of a TGML model to assess producibility of aerospace composite parts is presented.',\n",
       "  'len': 1418},\n",
       " {'abstract': 'The potential of digital twin technology is immense, specifically in the infrastructure, aerospace, and automotive sector. However, practical implementation of this technology is not at an expected speed, specifically because of lack of application-specific details. In this paper, we propose a novel digital twin framework for stochastic nonlinear multi-degree of freedom (MDOF) dynamical systems. The approach proposed in this paper strategically decouples the problem into two time-scales -- (a) a fast time-scale governing the system dynamics and (b) a slow time-scale governing the degradation in the system. The proposed digital twin has four components - (a) a physics-based nominal model (low-fidelity), (b) a Bayesian filtering algorithm a (c) a supervised machine learning algorithm and (d) a high-fidelity model for predicting future responses. The physics-based nominal model combined with Bayesian filtering is used combined parameter state estimation and the supervised machine learning algorithm is used for learning the temporal evolution of the parameters. While the proposed framework can be used with any choice of Bayesian filtering and machine learning algorithm, we propose to use unscented Kalman filter and Gaussian process. Performance of the proposed approach is illustrated using two examples. Results obtained indicate the applicability and excellent performance of the proposed digital twin framework.',\n",
       "  'len': 1441},\n",
       " {'abstract': 'Linear impulsively controlled systems are suitable to describe a venue of real-life problems, going from disease treatment to aerospace guidance. The main characteristic of such systems is that they remain uncontrolled for certain periods of time. As a consequence, punctual equilibria characterizations outside the origin are no longer useful, and the whole concept of equilibrium and its natural extension, the controlled invariant sets, needs to be redefined. Also, an exact characterization of the admissible states, i.e., states such that their uncontrolled evolution between impulse times remain within a predefined set, is required. An approach to such tasks -- based on the Markov-Lukasz theorem -- is presented, providing a tractable and non-conservative characterization, emerging from polynomial positivity that has application to systems with rational eigenvalues. This is in turn the basis for obtaining a tractable approximation to the maximal admissible invariant sets. In this work, it is also demonstrated that, in order for the problem to have a solution, an invariant set (and moreover, an equilibrium set) must be contained within the target zone. To assess the proposal, the so-obtained impulsive invariant set is explicitly used in the formulation of a set-based model predictive controller, with application to zone tracking. In this context, specific MPC theory needs to be considered, as the target is not necessarily stable in the sense of Lyapunov. A zone MPC formulation is proposed, which is able to i) track an invariant set such that the uncontrolled propagation fulfills the zone constraint at all times and ii) converge asymptotically to the set of periodic orbits completely contained within the target zone.',\n",
       "  'len': 1753},\n",
       " {'abstract': \"While innovations in scientific instrumentation have pushed the boundaries of Mars rover mission capabilities, the increase in data complexity has pressured Mars Science Laboratory (MSL) and future Mars rover operations staff to quickly analyze complex data sets to meet progressively shorter tactical and strategic planning timelines. MSLWEB is an internal data tracking tool used by operations staff to perform first pass analysis on MSL image sequences, a series of products taken by the Mast camera, Mastcam. Mastcam's multiband multispectral image sequences require more complex analysis compared to standard 3-band RGB images. Typically, these are analyzed using traditional methods to identify unique features within the sequence. Given the short time frame of tactical planning in which downlinked images might need to be analyzed (within 5-10 hours before the next uplink), there exists a need to triage analysis time to focus on the most important sequences and parts of a sequence. We address this need by creating products for MSLWEB that use novelty detection to help operations staff identify unusual data that might be diagnostic of new or atypical compositions or mineralogies detected within an imaging scene. This was achieved in two ways: 1) by creating products for each sequence to identify novel regions in the image, and 2) by assigning multispectral sequences a sortable novelty score. These new products provide colorized heat maps of inferred novelty that operations staff can use to rapidly review downlinked data and focus their efforts on analyzing potentially new kinds of diagnostic multispectral signatures. This approach has the potential to guide scientists to new discoveries by quickly drawing their attention to often subtle variations not detectable with simple color composites.\",\n",
       "  'len': 1828},\n",
       " {'abstract': 'Model predictive control (MPC) is an optimal control strategy where control input calculation is based on minimizing the predicted tracking error over a finite horizon that moves with time. This strategy has an advantage over conventional state feedback and output feedback controllers because it predicts the response of the system, rather than simply reacting to it. Therefore, MPC can offer improved performance in the presence of input and output constraints. Many implementations of MPC on aerospace vehicles appear in literature [1]. Some of these include spacecraft and satellite attitude control [2-4], spacecraft rendezvous and docking [5], helicopters [6], and atmospheric re-entry [7, 8].',\n",
       "  'len': 710},\n",
       " {'abstract': \"Ultrasonic motors (USMs) are commonly used in aerospace, robotics, and medical devices, where fast and precise motion is needed. Remarkably, sliding mode controller (SMC) is an effective controller to achieve precision motion control of the USMs. To improve the tracking accuracy and lower the chattering in the SMC, the fractional-order calculus is introduced in the design of an adaptive SMC in this paper, namely, adaptive fractional-order SMC (AFOSMC), in which the bound of the uncertainty existing in the USMs is estimated by a designed adaptive law. Additionally, a short memory principle is employed to overcome the difficulty of implementing the fractional-order calculus on a practical system in real-time. Here, the short memory principle may increase the tracking errors because some information is lost during its operation. Thus, a compensator according to the framework of Bellman's optimal control theory is proposed so that the residual errors caused by the short memory principle can be attenuated. Lastly, experiments on a USM are conducted, which comparative results verify the performance of the designed controller.\",\n",
       "  'len': 1148},\n",
       " {'abstract': 'Reinforcement learning algorithms in multi-agent systems deliver highly resilient and adaptable solutions for common problems in telecommunications,aerospace, and industrial robotics. However, achieving an optimal global goal remains a persistent obstacle for collaborative multi-agent systems, where learning affects the behaviour of more than one agent. A number of nonlinear function approximation methods have been proposed for solving the Bellman equation, which describe a recursive format of an optimal policy. However, how to leverage the value distribution based on reinforcement learning, and how to improve the efficiency and efficacy of such systems remain a challenge. In this work, we developed a reward-reinforced generative adversarial network to represent the distribution of the value function, replacing the approximation of Bellman updates. We demonstrated our method is resilient and outperforms other conventional reinforcement learning methods. This method is also applied to a practical case study: maximising the number of user connections to autonomous airborne base stations in a mobile communication network. Our method maximises the data likelihood using a cost function under which agents have optimal learned behaviours. This reward-reinforced generative adversarial network can be used as ageneric framework for multi-agent learning at the system level',\n",
       "  'len': 1395},\n",
       " {'abstract': \"Artificial intelligence (AI) and Machine Learning (ML) are becoming pervasive in today's applications, such as autonomous vehicles, healthcare, aerospace, cybersecurity, and many critical applications. Ensuring the reliability and robustness of the underlying AI/ML hardware becomes our paramount importance. In this paper, we explore and evaluate the reliability of different AI/ML hardware. The first section outlines the reliability issues in a commercial systolic array-based ML accelerator in the presence of faults engendering from device-level non-idealities in the DRAM. Next, we quantified the impact of circuit-level faults in the MSB and LSB logic cones of the Multiply and Accumulate (MAC) block of the AI accelerator on the AI/ML accuracy. Finally, we present two key reliability issues -- circuit aging and endurance in emerging neuromorphic hardware platforms and present our system-level approach to mitigate them.\",\n",
       "  'len': 941},\n",
       " {'abstract': 'A potential Mars Sample Return (MSR) architecture is being jointly studied by NASA and ESA. As currently envisioned, the MSR campaign consists of a series of 3 missions: sample cache, fetch and return to Earth. In this paper, we focus on the fetch part of the MSR, and more specifically the problem of autonomously detecting and localizing sample tubes deposited on the Martian surface. Towards this end, we study two machine-vision based approaches: First, a geometry-driven approach based on template matching that uses hard-coded filters and a 3D shape model of the tube; and second, a data-driven approach based on convolutional neural networks (CNNs) and learned features. Furthermore, we present a large benchmark dataset of sample-tube images, collected in representative outdoor environments and annotated with ground truth segmentation masks and locations. The dataset was acquired systematically across different terrain, illumination conditions and dust-coverage; and benchmarking was performed to study the feasibility of each approach, their relative strengths and weaknesses, and robustness in the presence of adverse environmental conditions.',\n",
       "  'len': 1168},\n",
       " {'abstract': \"The impact of radiation dramatically increases at high altitudes in the Earth's atmosphere and in space. Therefore, monitoring and access to radiation environment measurements are critical for estimating the radiation exposure risks of aircraft and spacecraft crews and the impact of space weather disturbances on electronics. Addressing these needs requires convenient access to multi-source radiation environment data and enhancement of visualization and search capabilities. The Radiation Data Portal represents an interactive web-based application for search and visualization of in-flight radiation measurements. The Portal enhances the exploration capabilities of various properties of the radiation environment and provides measurements of dose rates along with information on space weather-related conditions. The Radiation Data Portal back-end is a MySQL relational database that contains the radiation measurements obtained from the Automated Radiation Measurements for Aerospace Safety (ARMAS) device and the soft X-ray and proton flux measurements from the Geostationary Operational Environmental Satellite (GOES). The implemented Application Programming Interface (API) and Python routines allow a user to retrieve the database records without interaction with the web interface. As a use case of the Radiation Data Portal, we examine ARMAS measurements during an enhancement of the Solar Proton (SP) fluxes, known as Solar Proton Events (SPEs), and compare them to measurements during SP-quiet periods.\",\n",
       "  'len': 1527},\n",
       " {'abstract': \"Additive manufacturing (AM) is growing as fast as anyone can imagine, and it is now a multi-billion-dollar industry. AM becomes popular in a variety of sectors, such as automotive, aerospace, biomedical, and pharmaceutical, for producing parts/ components/ subsystems. However, current AM technologies can face vast risks of security issues and privacy loss. For the security of AM process, many researchers are working on the defense mechanism to countermeasure such security concerns and finding efficient ways to eliminate those risks. Researchers have also been conducting experiments to establish a secure framework for the user's privacy and security components. This survey consists of four sections. In the first section, we will explore the relevant limitations of additive manufacturing in terms of printing capability, security, and possible solutions. The second section will present different kinds of attacks on AM and their effects. The next part will analyze and discuss the mechanisms and frameworks for access control and authentication for AM devices. The final section examines the security issues in various industrial sectors and provides the observations on the security of the additive manufacturing process.\",\n",
       "  'len': 1243},\n",
       " {'abstract': \"A sequential decision process in which an adaptive radar system repeatedly interacts with a finite-state target channel is studied. The radar is capable of passively sensing the spectrum at regular intervals, which provides side information for the waveform selection process. The radar transmitter uses the sequence of spectrum observations as well as feedback from a collocated receiver to select waveforms which accurately estimate target parameters. It is shown that the waveform selection problem can be effectively addressed using a linear contextual bandit formulation in a manner that is both computationally feasible and sample efficient. Stochastic and adversarial linear contextual bandit models are introduced, allowing the radar to achieve effective performance in broad classes of physical environments. Simulations in a radar-communication coexistence scenario, as well as in an adversarial radar-jammer scenario, demonstrate that the proposed formulation provides a substantial improvement in target detection performance when Thompson Sampling and EXP3 algorithms are used to drive the waveform selection process. Further, it is shown that the harmful impacts of pulse-agile behavior on coherently processed radar data can be mitigated by adopting a time-varying constraint on the radar's waveform catalog.\",\n",
       "  'len': 1334},\n",
       " {'abstract': 'Automatic draping of carbon-fiber prepreg plies for the aerospace industry is a promising technique for lowering the manufacturing costs and to this end, a thorough in-process quality control is crucial. In this paper, out-of-plane defects in the layup are investigated. After draping, air pockets are occasionally encountered. The question is, if such apparent defects can be mitigated sufficiently during vacuum debulking. The 3D topology is measured by means of a structured-light 3D scanner and air pockets are segmented. An approximate mass-spring ply model is used to study the behavior of the air pockets during application of vacuum pressure. The model is computationally fast and will indicate online whether the air pocket will be removed or manual intervention is required. Upon comparing the model predictions with experimental data, it is shown that the system is capable of correctly predicting 13 out of 14 air pockets in a test layup.',\n",
       "  'len': 961},\n",
       " {'abstract': 'The quest for reductions in fuel consumption and CO2 emissions in transport has been a powerful driving force for scientific research into methods that might underpin drag-reducing technologies for a variety of vehicular transport on roads, by rail, in the air, and on or in the water. In civil aviation, skin-friction drag accounts for around 50% of the total drag in cruise conditions, thus being a preferential target for research. With laminar conditions excluded, skin friction is intimately linked to the turbulence physics in the fluid layer closest to the skin. Thus, research into drag reduction has focused on methods to depress the turbulence activity near the surface. The most effective method of doing so is to subject the drag-producing flow in the near-wall layer to an unsteady and/or spatially varying cross-flow component, either by the action of transverse wall oscillations, by embedding rotating discs into the surface or by plasma-producing electrodes that accelerate the near-wall fluid in the transverse direction. In ideal conditions, drag-reduction margins of order of 50% can be achieved. The present article provides a review of research into the response of turbulent near-wall layers to the imposition of unsteady and wavy transverse motion, encompassing experiments, simulation, analysis and modelling. It covers issues such as the drag-reduction margin in a variety of actuation scenarios, the underlying physical phenomena that contribute to the interpretation of the origin of the drag reduction, the dependence of the drag reduction on the Reynolds number, passive control methods that are inspired by active control, and a forward look towards possible future research and practical realizations. The authors hope that this review, by far the most extensive of its kind for this subject, will be judged as a useful foundation for future research targeting friction-drag reduction',\n",
       "  'len': 1927},\n",
       " {'abstract': 'Hyperbolic metamaterials (HMMs) are highly anisotropic optical materials that behave as metals or as dielectrics depending on the direction of propagation of light. They are becoming essential for a plethora of applications, ranging from aerospace to automotive, from wireless to medical and IoT. These applications often work in harsh environments or may sustain remarkable external stresses. This calls for materials that show enhanced optical properties as well as tailorable mechanical properties. Depending on their specific use, both hard and ultrasoft materials could be required, although the combination with optical hyperbolic response is rarely addressed. Here, we demonstrate the possibility to combine optical hyperbolicity and tunable mechanical properties in the same (meta)material, focusing on the case of extreme mechanical hardness. Using high-throughput calculations from first principles and effective medium theory, we explored a large class of layered materials with hyperbolic optical activity in the near-IR and visible range, and we identified a reduced number of ultrasoft and hard HMMs among more than 1800 combinations of transition metal rocksalt crystals. Once validated by the experiments, this new class of metamaterials may foster previously unexplored optical/mechanical applications.',\n",
       "  'len': 1330},\n",
       " {'abstract': 'Materials with periodic architectures exhibit many beneficial characteristics such as high specific stiffness thanks to the material placement along the stress paths and the nano-scale strength amplification achieved through the use of hierarchical architectures. Recently, the porosity of architectured materials was leveraged to increase the efficiency of compact heat exchangers, and their internal aerodynamics was studied. However, their performance on external aerodynamics applications is generally assumed to be detrimental. Here, we demonstrate that exposing 3D lattice material to the external flow reduced the drag of a circular cylinder when placed at carefully selected angular locations. We tested two configurations with the lattice material installed at the windward and leeward regions. On the one hand, the windward configuration showed a strong Re dependency, with a drag reduction of up to 45% at Re=11E4. On the other hand, the lattice material in the leeward region reduced the drag by 25% with weak Re dependency. Alterations of the lattice material topology had a noticeable effect on the drag reduction in both cases. Adding aerodynamic features to the already proven beneficial structural properties of 3D lattice materials might aid in the development of low-powered automotive, naval, and aerospace vehicles.',\n",
       "  'len': 1347},\n",
       " {'abstract': 'The last years have witnessed an enormous interest in the use of artificial intelligence methods, especially machine learning algorithms. This also has a major impact on aerospace engineering in general, and the design and operation of liquid rocket engines in particular, and research in this area is growing rapidly. The paper describes current machine learning applications at the DLR Institute of Space Propulsion. Not only applications in the field of modeling are presented, but also convincing results that prove the capabilities of machine learning methods for control and condition monitoring are described in detail. Furthermore, the advantages and disadvantages of the presented methods as well as current and future research directions are discussed.',\n",
       "  'len': 773},\n",
       " {'abstract': 'Large Eddy Simulation (LES) with dynamic Smagorinsky model has been applied to numerically investigate the complicated flow structures that evolve in the near wake of a cylindrical after body aligned with a uniform Mach 2.46 flow. Mean flow field properties obtained from numerical simulations, such as axial velocity, pressure on base surface, have been compared with the experimental measurements as well as with other published results. It has been found that standard k-epsilon model fails to predict the flow properties in the recirculation region where better agreement has been observed between the data obtained from LES and experimental measurements. Flow Statistics like turbulent kinetic energy and primary Reynolds stress have also been calculated and compared with the results obtained from experiments in order to quantitatively assess the ability of LES technique to predict the turbulence properties of flow field in the highly compressible shear layer region. The data obtained from LES has been further analyzed to understand the evolution of coherent structures in the flow field. Proper Orthogonal Decomposition (POD) of the data obtained from central plane in the wake region has been performed in order to reveal the most energetic structures present in the flow field.',\n",
       "  'len': 1302},\n",
       " {'abstract': 'In this study, we perform a two-dimensional axisymmetric simulation to assess the flow characteristics and understand the film cooling process in a dual bell nozzle. The secondary stream with low temperature is injected at three different axial locations on the nozzle wall, and the simulations are carried out to emphasize the impact of injection location (secondary flow) on film cooling of the dual bell nozzle. The cooling effect is demonstrated through the temperature and pressure distributions on the nozzle wall or, in-turn, the separation point movement. Downstream of the injection point, the Mach number and temperature profiles document the mixing of the main flow and secondary flow. The inflection region is observed to be the most promising location for the injection of the secondary flow. We have further investigated the effect of Mach number of the secondary stream. The current study demonstrates that one can control the separation point in a dual bell nozzle with the help of secondary injection (Mach number) so that an optimum amount of thrust can be achieved.',\n",
       "  'len': 1095},\n",
       " {'abstract': 'Large Eddy Simulation (LES) with dynamic sub-grid scale eddy viscosity model has been applied to numerically investigate the evolution of complicated flow structures in supersonic base flow with mass bleed. Mean flow properties obtained from numerical simulations, such as axial velocity, pressure on the base surface, have been compared with the experimental measurements to show that LES is able to predict the mean flow properties with acceptable accuracy. The data obtained from LES has been further analyzed to understand the evolution of coherent structures in the flow field. Periodical shedding of vortical structures from the outer shear layer has been observed and it has also been found that this vortex shedding is associated with the flapping of the outer shear layer. The frequency of flapping of the outer shear layer has been found out and the phase-averaged streamlines have been analyzed to further study the evolution of vortical structures associated with this flapping. The phase-averaged streamline plots clearly elucidate the evolution of vortical structures along the outer shear layer. Further, the study of these structures is investigated by performing Proper Orthogonal Decomposition (POD) analysis of the data, obtained along the central plane in the wake region. The POD results also seem to agree well with the observations made in the phase averaged streamline plots, as the concentrated energy and enstropy are observed in the outer shear layer with fewer POD modes.',\n",
       "  'len': 1510},\n",
       " {'abstract': 'A novel approach to reduced-order modeling of high-dimensional time varying systems is proposed. It leverages the formalism of the Dynamic Mode Decomposition technique together with the concept of balanced realization. It is assumed that the only information available on the system comes from input, state, and output trajectories generated by numerical simulations or recorded and estimated during experiments, thus the approach is fully data-driven. The goal is to obtain an input-output low dimensional linear model which approximates the system across its operating range. Since the dynamics of aeroservoelastic systems markedly changes in operation (e.g. due to change in flight speed or altitude), time-varying features are retained in the constructed models. This is achieved by generating a Linear Parameter-Varying representation made of a collection of state-consistent linear time-invariant reduced-order models. The algorithm formulation hinges on the idea of replacing the orthogonal projection onto the Proper Orthogonal Decomposition modes, used in Dynamic Mode Decomposition-based approaches, with a balancing oblique projection constructed entirely from data. As a consequence, the input-output information captured in the lower-dimensional representation is increased compared to other projections onto subspaces of same or lower size. Moreover, a parameter-varying projection is possible while also achieving state-consistency. The validity of the proposed approach is demonstrated on a morphing wing for airborne wind energy applications by comparing the performance against two algorithms recently proposed in the literature. Comparisons cover both prediction accuracy and performance in model predictive control applications.',\n",
       "  'len': 1759},\n",
       " {'abstract': 'The problem of engine unstart of scramjets has not been resolved. In this paper, the mechanism of engine unstart is discussed from the point of view of shock/shock interaction and deflagration-to-detonation transition. The shock/shock interaction leads to the nonlinear, transient and discontinuous process of the supersonic combustion flow field. This process is similar to the deflagration-to-detonation transition process. If the velocity of pre-combustion shock wave is faster than the velocity in the isolator, it will propagate upstream and cause the engine unstart. The C-J detonation velocity is defined as the stable operation boundary of scramjets, which is the maximum shock wave produced by combustion theoretically. The scramjets will work stable if the velocity in the isolator is faster than the corresponding C-J detonation velocity. The combustion characteristics and propulsive performance of scramjets is theoretically analyzed by using C-J detonation theory. For high Mach number scramjets, the velocity in the isolator is much faster than the C-J detonation velocity. Therefore, extra fuel and oxygen can be injected into the combustor to increase the thrust as long as the shock wave velocity driven by the combustion products is slower than the air velocity in the isolator. The theoretical results agree well with the existing experimental results, which can be used as a baseline for the development of scramjets.',\n",
       "  'len': 1449},\n",
       " {'abstract': 'While polarisation sensing is vital in many areas of research, with applications spanning from microscopy to aerospace, traditional approaches are limited by method-related error amplification or accumulation, placing fundamental limitations on precision and accuracy in single-shot polarimetry. Here, we put forward a new measurement paradigm to circumvent this, introducing the notion of a universal full Poincaré generator to map all polarisation analyser states into a single vectorially structured light field, allowing all vector components to be analysed in a single-shot with theoretically user-defined precision. To demonstrate the advantage of our approach, we use a common GRIN optic as our mapping device and show mean errors of <1% for each vector component, enhancing the sensitivity by around three times, allowing us to sense weak polarisation aberrations not measurable by traditional single-shot techniques. Our work paves the way for next-generation polarimetry, impacting a wide variety of applications relying on weak vector measurement.',\n",
       "  'len': 1069},\n",
       " {'abstract': 'Multiple-input multiple-output (MIMO) array based millimeter-wave (MMW) imaging has a tangible prospect in applications of concealed weapons detection. A near-field imaging algorithm based on wavenumber domain processing is proposed for a cylindrical MIMO array scheme with uniformly spaced transmit and receive antennas over both the vertical and horizontal-arc directions. The spectrum aliasing associated with the proposed MIMO array is analyzed through a zero-filling discrete-time Fourier transform. The analysis shows that an undersampled array can be used in recovering the MMW image by a wavenumber domain algorithm. The requirements for the antenna inter-element spacing of the MIMO array are delineated. Numerical simulations as well as comparisons with the backprojection (BP) algorithm are provided to demonstrate the effectiveness of the proposed method.',\n",
       "  'len': 878},\n",
       " {'abstract': 'Understanding surface reactivity is crucial in many fields, going from heterogeneous catalysis to materials oxidation and corrosion. In order to decipher the surface reactions of ZrB2 exposed to the harsh environment of aerospace components, the chemical activity of both Zr- and B-surfaces is predicted and compared by using density functional theory and nudged elastic band methods. In particular the adsorption, dissociation and diffusion of O2, CO and H2O are extensively examined through the calculation of surface adsorption energies and reaction pathways. We find the dissociative adsorption of O2 dominating the reactivity of ZrB2 surfaces, while the dissociation of H2O and CO is weakly active on Zr-surfaces, and even less activated on B-terminated ones. Importantly, we discover that the reaction of O2 and H2O can trigger strong surface reconstruction at B-surfaces. Our work thus provides significant insights into the diverse adsorption and reaction mechanisms of ZrB2 surfaces.',\n",
       "  'len': 1003},\n",
       " {'abstract': 'Engineering Thermodynamics has been the core course of many science and engineering majors around the world, including energy and power, mechanical engineering, civil engineering, aerospace, cryogenic refrigeration, food engineering, chemical engineering, and environmental engineering, among which gas power cycle is one of the important contents. However, many Engineering Thermodynamics textbooks focus only on evaluating the thermal efficiency of gas power cycle, while the important concept of specific cycle work is ignored. Based on the generalized temperature-entropy diagram for the gas power cycles proposed by the authors, an ideal Otto cycle and an ideal Miller-Diesel cycle are taking as examples for the thermodynamic analyses of gas power cycles. The optimum compression ratio (or the pressure ratio) for the maximum specific cycle work or the maximum mean effective pressure is analyzed and determined. The ideal Otto and the ideal Miller-Diesel cycles, and also other gas power cycles for movable applications, are concluded that the operation under the maximum specific cycle work or the maximum mean effective pressure, instead of under the higher efficiency, is more economic and more reasonable. We concluded that the very important concept, i.e., the optimum compression (or pressure) ratio for the gas power cycles, should be emphasized in the Engineering Thermodynamics teaching process and in the latter revised or the newly edited textbooks, in order to better guide the engineering applications.',\n",
       "  'len': 1533},\n",
       " {'abstract': 'Cyber-Physical Systems (CPSs) combine software and physical components. These systems are widely applied in society within many domains, including the automotive, aerospace, railway, etc. Testing these systems is extremely challenging, therefore, it has attracted significant attention from the research community. A driving CPS testing technique in industry is simulation-based testing. However, this poses significant challenges. In this new-idea paper we present a novel approach to enhance the testing processes of CPSs. This novel approach is motivated with examples and open questions.',\n",
       "  'len': 602},\n",
       " {'abstract': 'Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality.',\n",
       "  'len': 1910},\n",
       " {'abstract': 'Context: As Industrial Cyber-Physical Systems (ICPS) become more connected and widely-distributed, often operating in safety-critical environments, we require innovative approaches to detect and diagnose the faults that occur in them. Objective: We profile fault identification and diagnosis techniques employed in the aerospace, automotive, and industrial control domains. By examining both theoretical presentations as well as case studies from production environments, we present a profile of the current approaches being employed and identify gaps. Methodology: A scoping study was used to identify and compare fault detection and diagnosis methodologies that are presented in the current literature. Results: Fault identification and analysis studies from 127 papers published from 2004 to 2019 reveal a wide diversity of promising techniques, both emerging and in-use. These range from traditional Physics-based Models to Data-Driven Artificial Intelligence (AI) and Knowledge-Based approaches. Predictive diagnostics or prognostics featured prominently across all sectors, along with discussions of techniques including Fault trees, Petri nets and Markov approaches. We also profile some of the techniques that have reached the highest Technology Readiness Levels, showing how those methods are being applied in real-world environments beyond the laboratory. Conclusions: Our results suggest that the continuing wide use of both Model-Based and Data-Driven AI techniques across all domains, especially when they are used together in hybrid configuration, reflects the complexity of the current ICPS application space. While creating sufficiently-complete models is labor intensive, Model-free AI techniques were evidenced as a viable way of addressing aspects of this challenge, demonstrating the increasing sophistication of current machine learning systems.(Abridged)',\n",
       "  'len': 1887},\n",
       " {'abstract': 'Industrial inspection automation in aerospace presents numerous challenges due to the dynamic, information-rich and regulated aspects of the domain. To diagnose the condition of an aircraft component, expert inspectors rely on a significant amount of procedural and tacit knowledge (know-how). As systems capabilities do not match high level human cognitive functions, the role of humans in future automated work systems will remain important. A Cyber-Physical-Social System (CPSS) is a suitable solution that envisions humans and agents in a joint activity to enhance cognitive/computational capabilities and produce better outcomes. This paper investigates how a work-centred approach can support and guide the engineering process of a CPSS with an industrial use case. We present a robust methodology that combines fieldwork inquiries and model-based engineering to elicit and formalize rich mental models into exploitable design patterns. Our results exhibit how inspectors process and apply knowledge to diagnose the component`s condition, how they deal with the institution`s rules and operational constraints (norms, safety policies, standard operating procedures). We suggest how these patterns can be incorporated in software modules or can conceptualize Human-Agent Teaming requirements. We argue that this framework can corroborate the right fit between a system`s technical and ecological validity (system fit with operating context) that enhances data reliability, productivity-related factors and system acceptance by end-users.',\n",
       "  'len': 1553},\n",
       " {'abstract': 'Fiber-reinforced ceramic-matrix composites are advanced materials resistant to high temperatures, with application to aerospace engineering. Their analysis depends on the detection of embedded fibers, with semi-supervised techniques usually employed to separate fibers within the fiber beds. Here we present an open computational pipeline to detect fibers in ex-situ X-ray computed tomography fiber beds. To separate the fibers in these samples, we tested four different architectures of fully convolutional neural networks. When comparing our neural network approach to a semi-supervised one, we obtained Dice and Matthews coefficients greater than $92.28 \\\\pm 9.65\\\\%$, reaching up to $98.42 \\\\pm 0.03 \\\\%$, showing that the network results are close to the human-supervised ones in these fiber beds, in some cases separating fibers that human-curated algorithms could not find. The software we generated in this project is open source, released under a permissive license, and can be freely adapted and re-used in other domains. All data and instructions on how to download and use it are also available.',\n",
       "  'len': 1114},\n",
       " {'abstract': 'This work demonstrates the application of OpenCV towards feature extraction from 2D engineering drawings. The extracted features are used in the reconstruction of 3D CAD models in SCAD format and generation of 3D point cloud data that is equivalent to LIDAR scan data. Several legacy designs in mechanical and aerospace engineering are available as engineering drawings rather than software generated CAD models. Even in civil engineering, many of the earlier plans are available as drawings rather than BIM/CAD models. The methods proposed in this work can help convert such drawings easily into 3D CAD and BIM models using camera capture or scanned drawing data. The method demonstrates the applicability for simple shapes and does not yet account for the presence of hidden lines in CAD drawings. The developed codes are made available as python Jupyter notebooks through Github.',\n",
       "  'len': 893},\n",
       " {'abstract': 'We propose a design that uses the principle of chaos for UAV secure communication. A UAV identified as an aerial base station communicates with a ground base station over an RF channel. The communication units have dynamics based on the logistic map. The map is chaotic in the appropriate parameter space. Its states are non-periodic, broadband, and noise-like in the frequency domain. They are useful for spreading information during transmission, making it difficult for an eavesdropper to recover the modulated message since state prediction is ultimately impossible. To retrieve it, we propose a variable feedback controller. It asymptotically stabilizes the error dynamics when the information source is off. During transmission, the controller synchronizes the units such that the error contains signatures of the information signal. Therefore, the information signal is retrievable by a suitable detection mechanism. Security depends on the confidentiality of the map, the variable feedback controller, including its scale factor and bounded feedback gain, and the designer`s choice of invertible function for use in the scrambling and descrambling process. Also, the method is less prone to jamming attacks and multipath effects as the broadband spectrum can be used to randomly select RF channels. It uses only a few simple algorithms, including a correlation summation and a detection mechanism. The algorithms collect subsamples of the received signal sequences and averages over each subsample length. The method requires minimal programming efforts and low hardware resource utilization. It is energy-efficient, which is a vital consideration for any UAV security model. Moreover, we realize a prototype of the communication system on field-programmable gate arrays. We presented a digital design of the secure communication system involving the transmission of bitstreams between the ABS and GBS.',\n",
       "  'len': 1921},\n",
       " {'abstract': \"Data-driven methods open up unprecedented possibilities for maritime surveillance using Automatic Identification System (AIS) data. In this work, we explore deep learning strategies using historical AIS observations to address the problem of predicting future vessel trajectories with a prediction horizon of several hours. We propose novel sequence-to-sequence vessel trajectory prediction models based on encoder-decoder recurrent neural networks (RNNs) that are trained on historical trajectory data to predict future trajectory samples given previous observations. The proposed architecture combines Long Short-Term Memory (LSTM) RNNs for sequence modeling to encode the observed data and generate future predictions with different intermediate aggregation layers to capture space-time dependencies in sequential data. Experimental results on vessel trajectories from an AIS dataset made freely available by the Danish Maritime Authority show the effectiveness of deep-learning methods for trajectory prediction based on sequence-to-sequence neural networks, which achieve better performance than baseline approaches based on linear regression or on the Multi-Layer Perceptron (MLP) architecture. The comparative evaluation of results shows: i) the superiority of attention pooling over static pooling for the specific application, and ii) the remarkable performance improvement that can be obtained with labeled trajectories, i.e., when predictions are conditioned on a low-level context representation encoded from the sequence of past observations, as well as on additional inputs (e.g., port of departure or arrival) about the vessel's high-level intention, which may be available from AIS.\",\n",
       "  'len': 1709},\n",
       " {'abstract': 'While analytical solutions of critical (phase) transitions in physical systems are abundant for simple nonlinear systems, such analysis remains intractable for real-life dynamical systems. A key example of such a physical system is thermoacoustic instability in combustion, where prediction or early detection of an onset of instability is a hard technical challenge, which needs to be addressed to build safer and more energy-efficient gas turbine engines powering aerospace and energy industries. The instabilities arising in combustion chambers of engines are mathematically too complex to model. To address this issue in a data-driven manner instead, we propose a novel deep learning architecture called 3D convolutional selective autoencoder (3D-CSAE) to detect the evolution of self-excited oscillations using spatiotemporal data, i.e., hi-speed videos taken from a swirl-stabilized combustor (laboratory surrogate of gas turbine engine combustor). 3D-CSAE consists of filters to learn, in a hierarchical fashion, the complex visual and dynamic features related to combustion instability. We train the 3D-CSAE on frames of videos obtained from a limited set of operating conditions. We select the 3D-CSAE hyper-parameters that are effective for characterizing hierarchical and multiscale instability structure evolution by utilizing the dynamic information available in the video. The proposed model clearly shows performance improvement in detecting the precursors of instability. The machine learning-driven results are verified with physics-based off-line measures. Advanced active control mechanisms can directly leverage the proposed online detection capability of 3D-CSAE to mitigate the adverse effects of combustion instabilities on the engine operating under various stringent requirements and conditions.',\n",
       "  'len': 1831},\n",
       " {'abstract': 'Obtaining the ability to make informed decisions regarding the operation and maintenance of structures, provides a major incentive for the implementation of structural health monitoring (SHM) systems. Probabilistic risk assessment (PRA) is an established methodology that allows engineers to make risk-informed decisions regarding the design and operation of safety-critical and high-value assets in industries such as nuclear and aerospace. The current paper aims to formulate a risk-based decision framework for structural health monitoring that combines elements of PRA with the existing SHM paradigm. As an apt tool for reasoning and decision-making under uncertainty, probabilistic graphical models serve as the foundation of the framework. The framework involves modelling failure modes of structures as Bayesian network representations of fault trees and then assigning costs or utilities to the failure events. The fault trees allow for information to pass from probabilistic classifiers to influence diagram representations of decision processes whilst also providing nodes within the graphical model that may be queried to obtain marginal probability distributions over local damage states within a structure. Optimal courses of action for structures are selected by determining the strategies that maximise expected utility. The risk-based framework is demonstrated on a realistic truss-like structure and supported by experimental data. Finally, a discussion of the risk-based approach is made and further challenges pertaining to decision-making processes in the context of SHM are identified.',\n",
       "  'len': 1617},\n",
       " {'abstract': 'This paper presents a framework for the design and analysis of an $\\\\mathcal{L}_1$ adaptive controller with a switching reference system. The use of a switching reference system allows the desired behavior to be scheduled across the operating envelope, which is often required in aerospace applications. The analysis uses a switched reference system that assumes perfect knowledge of uncertainties and uses a corresponding non-adaptive controller. Provided that this switched reference system is stable, it is shown that the closed-loop system with unknown parameters and disturbances and the $\\\\mathcal{L}_1$ adaptive controller can behave arbitrarily close to this reference system. Simulations of the short period dynamics of a transport class aircraft during the approach phase illustrate the theoretical results.',\n",
       "  'len': 826},\n",
       " {'abstract': 'Mixtures of fluids and granular sediments play an important role in many industrial, geotechnical, and aerospace engineering problems, from waste management and transportation (liquid--sediment mixtures) to dust kick-up below helicopter rotors (gas--sediment mixtures). These mixed flows often involve bulk motion of hundreds of billions of individual sediment particles and can contain both highly turbulent regions and static, non-flowing regions. This breadth of phenomena necessitates the use of continuum simulation methods, such as the material point method (MPM), which can accurately capture these large deformations while also tracking the Lagrangian features of the flow (e.g.\\\\ the granular surface, elastic stress, etc.). Recent works using two-phase MPM frameworks to simulate these mixtures have shown substantial promise; however, these approaches are hindered by the numerical limitations of MPM when simulating pure fluids. In addition to the well-known particle ringing instability and difficulty defining inflow/outflow boundary conditions, MPM has a tendency to accumulate quadrature errors as materials deform, increasing the rate of overall error growth as simulations progress. In this work, we present an improved, two-phase continuum simulation framework that uses the finite volume method (FVM) to solve the fluid phase equations of motion and MPM to solve the solid phase equations of motion, substantially reducing the effect of these errors and providing better accuracy and stability for long-duration simulations of these mixtures.',\n",
       "  'len': 1572},\n",
       " {'abstract': 'This paper presents a nonlinear model reduction method for systems of equations using a structured neural network. The neural network takes the form of a \"three-layer\" network with the first layer constrained to lie on the Grassmann manifold and the first activation function set to identity, while the remaining network is a standard two-layer ReLU neural network. The Grassmann layer determines the reduced basis for the input space, while the remaining layers approximate the nonlinear input-output system. The training alternates between learning the reduced basis and the nonlinear approximation, and is shown to be more effective than fixing the reduced basis and training the network only. An additional benefit of this approach is, for data that lie on low-dimensional subspaces, that the number of parameters in the network does not need to be large. We show that our method can be applied to scientific problems in the data-scarce regime, which is typically not well-suited for neural network approximations. Examples include reduced order modeling for nonlinear dynamical systems and several aerospace engineering problems.',\n",
       "  'len': 1145},\n",
       " {'abstract': 'The recent, rapid advancement in space exploration is thanks to the accelerated miniaturization of electronics components on a spacecraft that is reducing the mass, volume and cost of satellites. Yet, access to space remains a distant dream as there is growing complexity in what is required of satellites and increasing space traffic. Interplanetary exploration is even harder and has limited possibilities for low cost mission. All of these factors make even CubeSats, the entry-level standard too expensive for most and therefore a better way needs to be found. The proposed solution in this report is a low-mass, low-cost, disposable solution that exploits the latest advances in electronics and is relatively easy to integrate: FemtoSats. FemtoSats are sub-100-gram spacecraft. The FemtoSat concept is based on launching a swarm where the main tasks are divided between the members of the swarm. This means that if one fails the swarm can take its place and therefore substitute it without risking the whole mission. In this paper we explore the utility of FemtoSats to perform first exploration and mapping of a Lunar PSR. This concept was recognized as finalist for the NASA BIG Competition in 2020. This is an example of a high-risk, high-reward mission where losing one FemtoSat does not mean the mission is in danger as it happens with regular satellite missions.',\n",
       "  'len': 1384},\n",
       " {'abstract': \"In this paper, we derive a highly accurate approximation for the probability of detection (PD) of a non-coherent detector operating with Weibull fluctuation targets. To do so, we assume a pulse-to-pulse decorrelation during the coherent processing interval (CPI). Specifically, the proposed approximation is given in terms of: i) a closed-form expression derived in terms of the Fox's H-function, for which we also provide a portable and efficient MATHEMATICA routine; and ii) a fast converging series obtained through a comprehensive calculus of residues. Both solutions are fast and provide very accurate results. In particular, our series representation, besides being a more tractable solution, also exhibits impressive savings in computational load and computation time compared to previous studies. Numerical results and Monte-Carlo simulations corroborated the validity of our expressions.\",\n",
       "  'len': 907},\n",
       " {'abstract': 'Most common mechanistic models are traditionally presented in mathematical forms to explain a given physical phenomenon. Machine learning algorithms, on the other hand, provide a mechanism to map the input data to output without explicitly describing the underlying physical process that generated the data. We propose a Data-based Physics Discovery (DPD) framework for automatic discovery of governing equations from observed data. Without a prior definition of the model structure, first a free-form of the equation is discovered, and then calibrated and validated against the available data. In addition to the observed data, the DPD framework can utilize available prior physical models, and domain expert feedback. When prior models are available, the DPD framework can discover an additive or multiplicative correction term represented symbolically. The correction term can be a function of the existing input variable to the prior model, or a newly introduced variable. In case a prior model is not available, the DPD framework discovers a new data-based standalone model governing the observations. We demonstrate the performance of the proposed framework on a real-world application in the aerospace industry.',\n",
       "  'len': 1229},\n",
       " {'abstract': \"We use long-term electron and proton in-situ measurements made by the CXD particle instruments, developed by Los Alamos National Laboratory and carried on board GPS satellites, to determine total ionizing dose (TID) values and daily/yearly dose rate (DR) values in medium Earth orbits (MEOs) caused by the natural space radiation environment. Here measurement-based TID and DR values on a simplified sample geometry--a small (with a radius of 0.1 mm) Silicon detector within an Aluminum shielding sphere with a thickness of 100 mil--are compared to those calculated from empirical radiation models. Results over the solar cycle 24 show that electron TID from measurements in GPS orbit is well above the values calculated from the median/mean fluences from AE8 and AE9 models, but close to model fluences at high percentiles. Also, it is confirmed that in MEOs proton contributions to TID are minor and mainly dominated by solar energetic protons. Several factors affecting those dose calculations are discussed and evaluated. Results from this study provide us another out-of-sample test on the reliability of existing empirical space radiation models, and also help estimate the margin factors on calculated dose values in MEOs that pass through the heart of the Earth's outer radiation belt.\",\n",
       "  'len': 1304},\n",
       " {'abstract': 'The introduction of nanostructured interlayers is one of the most promising strategies for interlaminar reinforcement in structural composites. In this work, we study the failure mechanism and interlayer microstructure of aerospace-grade structural composites reinforced with thin veils of carbon nanotube produced using an industrialised spinning process. Samples of unidirectional carbon fibre/epoxy matrix composites interleaved with different composition CNT veils were prepared using hot press method and tested for interlaminar fracture toughness (IFT), measured in Mode-I (opening) and Mode-II (in-plane shear), and for interlaminar shear strength (ILSS), evaluated by the short beam shear (SBS) test. The crack propagation mode could be directly determined through fractography analysis by electron microscopy and resin/CNT spatial discrimination by Raman spectroscopy, showing a clear correlation between interlaminar reinforcement and the balance between cohesive/adhesive failure mode at the interlayer region. Composites with full resin infiltration of the CNT veils give a large increase of Mode II IFT (88%) to 1500 J/m2 and a slight enhancement of apparent interlaminar shear strength (6.5%), but a decrease of Mode I IFT (-21%). The results help establish the role of interlayer infiltration, interlaminar crossings and formation of a carbon fibre bridgings, for interlaminar reinforcement with interleaves.',\n",
       "  'len': 1434},\n",
       " {'abstract': 'The aerospace industry relies on massive collections of complex and technical documents covering system descriptions, manuals or procedures. This paper presents a question answering (QA) system that would help aircraft pilots access information in this documentation by naturally interacting with the system and asking questions in natural language. After describing each module of the dialog system, we present a multi-task based approach for the QA module which enables performance improvement on a Flight Crew Operating Manual (FCOM) dataset. A method to combine scores from the retriever and the QA modules is also presented.',\n",
       "  'len': 640},\n",
       " {'abstract': 'This paper describes the envisioned interactions between the information and communication technology and aerospace industries to serve autonomous devices for next generation aerial parcel delivery networks. The autonomous features of fleet elements of the delivery network are enabled by the increased throughput, improved coverage, and near-user computation capabilities of vertical heterogeneous networks (VHetNets). A high altitude platform station (HAPS), located around 20~km above the ground level in a quasi-stationary manner, serves as the main enabler of the vision we present. In addition to the sensing potential of the HAPS nodes, the use of communication, computing, and caching capabilities demonstrate the attainability of the ambitious goal of serving a fully autonomous aerial fleet capable of addressing instantaneous user demands and enabling supply chain management interactions with delivery services in low-latency settings.',\n",
       "  'len': 958},\n",
       " {'abstract': 'Maritime surveillance (MS) is of paramount importance for search and rescue operations, fishery monitoring, pollution control, law enforcement, migration monitoring, and national security policies. Since ground-based radars and automatic identification system (AIS) do not always provide a comprehensive and seamless coverage of the entire maritime domain, the use of space-based sensors is crucial to complement them. We reviewed space-based technologies for MS in the first part of this work, titled \"Space-based Global Maritime Surveillance. Part I: Satellite Technologies\" [1]. However, future MS systems combining multiple terrestrial and space-based sensors with additional information sources will require dedicated artificial intelligence and data fusion techniques for the processing of raw satellite images and fuse heterogeneous information. The second part of our work focuses on the most promising artificial intelligence and data fusion techniques for MS using space-based sensors.',\n",
       "  'len': 1006},\n",
       " {'abstract': 'Maritime surveillance (MS) is crucial for search and rescue operations, fishery monitoring, pollution control, law enforcement, migration monitoring, and national security policies. Since the early days of seafaring, MS has been a critical task for providing security in human coexistence. Several generations of sensors providing detailed maritime information have become available for large offshore areas in real time: maritime radar sensors in the 1950s and the automatic identification system (AIS) in the 1990s among them. However, ground-based maritime radars and AIS data do not always provide a comprehensive and seamless coverage of the entire maritime space. Therefore, the exploitation of space-based sensor technologies installed on satellites orbiting around the Earth, such as satellite AIS data, synthetic aperture radar, optical sensors, and global navigation satellite systems reflectometry, becomes crucial for MS and to complement the existing terrestrial technologies. In the first part of this work, we provide an overview of the main available space-based sensors technologies and present the advantages and limitations of each technology in the scope of MS. The second part, related to artificial intelligence, signal processing and data fusion techniques, is provided in a companion paper, titled: \"Space-based Global Maritime Surveillance. Part II: Artificial Intelligence and Data Fusion Techniques\" [1].',\n",
       "  'len': 1442},\n",
       " {'abstract': 'Prediction of stock price and stock price movement patterns has always been a critical area of research. While the well-known efficient market hypothesis rules out any possibility of accurate prediction of stock prices, there are formal propositions in the literature demonstrating accurate modeling of the predictive systems that can enable us to predict stock prices with a very high level of accuracy. In this paper, we present a suite of deep learning-based regression models that yields a very high level of accuracy in stock price prediction. To build our predictive models, we use the historical stock price data of a well-known company listed in the National Stock Exchange (NSE) of India during the period December 31, 2012 to January 9, 2015. The stock prices are recorded at five minutes intervals of time during each working day in a week. Using these extremely granular stock price data, we build four convolutional neural network (CNN) and five long- and short-term memory (LSTM)-based deep learning models for accurate forecasting of the future stock prices. We provide detailed results on the forecasting accuracies of all our proposed models based on their execution time and their root mean square error (RMSE) values.',\n",
       "  'len': 1247},\n",
       " {'abstract': 'We present an analytical solution for the inverse kinematics (IK) of a robotic arm with one prismatic joint and four revolute joints. This 5-DoF design is a result of minimizing weight while preserving functionality of the device in a wearable usage context. Generally, the IK problem for a 5-DoF robot does not guarantee solutions due to the system being over-constrained. We obtain an analytical solution by applying geometric projections and limiting the ranges of motion for each DoF. We validate this solution by reconstructing randomly sampled end-effector poses, and find position errors below 2 cm and orientation errors below 4 degrees.',\n",
       "  'len': 656},\n",
       " {'abstract': 'Advanced electrical conductors that outperform copper and aluminum can revolutionize our lives, enabling billions of dollars in energy savings and facilitating a transition to an electric mobility future. Nanocarbons (carbon nanotubes and graphene) present a unique opportunity for developing advanced conductors for electrical power, communications, electronics, and electric machines for the aerospace, marine, and automotive industries. This paper compiles the major research progress in the field of advanced nanocarbon-based and metal-nanocarbon conductors. It also elucidates the competitiveness and implications of advanced conductors with respect to conventional ones and describes their materials science, properties, and characterization. Finally, several areas of future research for advanced electrical conductors are proposed.',\n",
       "  'len': 850},\n",
       " {'abstract': 'Managing uncertainty is a fundamental and critical issue in spacecraft entry guidance. This paper presents a novel approach for uncertainty propagation during entry, descent and landing that relies on a new sum-of-squares robust verification technique. Unlike risk-based and probabilistic approaches, our technique does not rely on any probabilistic assumptions. It uses a set-based description to bound uncertainties and disturbances like vehicle and atmospheric parameters and winds. The approach leverages a recently developed sampling-based version of sum-of-squares programming to compute regions of finite time invariance, commonly referred to as \"invariant funnels\". We apply this approach to a three-degree-of-freedom entry vehicle model and test it using a Mars Science Laboratory reference trajectory. We compute tight approximations of robust invariant funnels that are guaranteed to reach a goal region with increased landing accuracy while respecting realistic thermal constraints.',\n",
       "  'len': 1005},\n",
       " {'abstract': 'The test pyramid is a conceptual model that describes how quality checks can be organized to ensure coverage of all components of a system, at all scales. Originally conceived to help aerospace engineers plan tests to determine how material changes impact system integrity, the concept was gradually introduced into software engineering. Today, the test pyramid is typically used to illustrate that the majority of tests should be performed at the lowest (unit test) level, with fewer integration tests, and even fewer acceptance tests (which are the most expensive to produce, and the slowest to execute). Although the value of acceptance tests and integration tests increasingly depends on the integrity of the underlying data, models, and pipelines, software development and data management organizations have traditionally been siloed and quality assurance practice is not as mature in data operations as it is for software. Companies that close this gap by developing cross-organizational systems will create new competitive advantage and differentiation. By taking a more holistic view of testing that crosses these boundaries, practitioners can help their organizations close the gap.',\n",
       "  'len': 1202},\n",
       " {'abstract': 'Refractory high entropy alloys (R-HEAs) are having properties and uses as high strength and high hardness materials for ambient and high temperature, aerospace and nuclear radiation tolerance applications, orthopedic applications etc. The mechanical properties like yield strength and ductility of TaNbHfZr R-HEA depend on the local nanostructure and chemical ordering. In this study we have computationally obtained various properties of the TaNbHfZr alloy like the role of configurational entropy in the thermodynamic property, rate of evolution of nanostructure morphology in thermally annealed systems, dislocation simulation based quantitative prediction of yield strength, nature of dislocation movement through short range clustering (SRC) and qualitative prediction of ductile to brittle transition behavior. The simulation starts with hybrid Monte Carlo/ Molecular Dynamics (MC/MD) based nanostructure evolution of an initial random solid solution alloy structure with BCC lattice structure created with principal axes along [1 1 1], [-1 1 0] and [-1 -1 2] directions suitable for simulation of 1/2[1 1 1] edge dislocations. Thermodynamic properties are calculated from the change in enthalpy and the configurational entropy by next-neighbor bond counting statistics. The MC/MD evolved structures mimic the annealing treatment at 1800°C and the output structures are replicated in periodic directions to make larger 384000 atom structures used for dislocation simulations. Edge dislocations were utilized to obtain and explain for the extra strengthening observed because of the formations of SRCs. Lastly the MC/MD evolved structures containing dislocations are subjected to a high shear stress beyond CRSS to investigate the stability of the dislocations and the lattice structures to explain the experimentally observed transition from ductile to brittle behavior for the TaNbHfZr R-HEA.',\n",
       "  'len': 1910},\n",
       " {'abstract': 'Metalization of carbon fiber reinforced polymers (CFRPs) composites by the surface modification method to enhance their electrical conductivity, thermal conductivity, electromagnetic shielding, erosion, and radiation protection, has a significant meaning in the aerospace field. In this study, Sn coating was successfully fabricated on the CFRP composite substrate via low-pressure cold spray under four gas temperatures (473K, 523K, 573K, and 623K). Their bonding mechanism was explored via the surface observation after peel-off adhesion strength, accompanying with surface temperature distribution investigation. The results indicates that we cannot obtain coating at 623 K, the epoxy matrix of the CFRP substrate was gradually eroded during deposition over 523 K. Meanwhile, Sn particles melt under 623 K condition. Three kinds of interfaces: Sn/epoxy, Sn/CF, and Sn/CF/epoxy are revealed as characteristics with respect to different gas temperatures to explore the bonding mechanisms.',\n",
       "  'len': 1000},\n",
       " {'abstract': 'As the space industry approaches a period of rapid change, securing both emerging and legacy satellite missions will become vital. However, space technology has been largely overlooked by the systems security community. This systematization of knowledge paper seeks to understand why this is the case and to offer a starting point for technical security researchers seeking impactful contributions beyond the Earth\\'s mesosphere. The paper begins with a cross-disciplinary synthesis of relevant threat models from a diverse array of fields, ranging from legal and policy studies to aerospace engineering. This is presented as a \"threat matrix toolbox\" which security researchers may leverage to motivate technical research into given attack vectors and defenses. We subsequently apply this model to an original chronology of more than 100 significant satellite hacking incidents spanning the previous 60 years. Together, these are used to assess the state-of-the-art in satellite security across four sub-domains: satellite radio-link security, space hardware security, ground station security, and operational/mission security. In each area, we note significant findings and unresolved questions lingering in other disciplines which the systems security community is aptly poised to tackle. By consolidating this research, we present the case that satellite systems security researchers can build on strong, but disparate, academic foundations and rise to meet an urgent need for future space missions.',\n",
       "  'len': 1513},\n",
       " {'abstract': 'Additive manufacturing or 3D-printing is used to create bespoke items in many fields, such as defence, aerospace and medicine. Despite the progress made in 3D-printed orthopaedic implants, significant challenges remain in terms of creating a material capable of osseointegration while inhibiting bacterial colonisation of the implant. Diamond is rapidly emerging as a material with an extensive range of biomedical applications, especially due to its excellent biocompatibility. However, diamond is a difficult material to fabricate, owing to its extreme level of hardness and its brittleness. New methods of fabrication including additive manufacturing, have overcome some of these challenges and given rise to an increase in the use of diamond-based implants in both soft and hard tissue applications. Therefore, due to the unique properties of diamond, it is being considered as a facilitator of bone growth and subsequent tissue integration. This review outlines the recent progress in fabricating diamond for orthopaedic application, specifically focusing on the different fabrication approaches and their applicability in vitro and in vivo. The prospects and challenges for using diamond in medical implant technologies are also discussed.',\n",
       "  'len': 1256},\n",
       " {'abstract': 'The study and benchmarking of Deep Reinforcement Learning (DRL) models has become a trend in many industries, including aerospace engineering and communications. Recent studies in these fields propose these kinds of models to address certain complex real-time decision-making problems in which classic approaches do not meet time requirements or fail to obtain optimal solutions. While the good performance of DRL models has been proved for specific use cases or scenarios, most studies do not discuss the compromises and generalizability of such models during real operations. In this paper we explore the tradeoffs of different elements of DRL models and how they might impact the final performance. To that end, we choose the Frequency Plan Design (FPD) problem in the context of multibeam satellite constellations as our use case and propose a DRL model to address it. We identify 6 different core elements that have a major effect in its performance: the policy, the policy optimizer, the state, action, and reward representations, and the training environment. We analyze different alternatives for each of these elements and characterize their effect. We also use multiple environments to account for different scenarios in which we vary the dimensionality or make the environment nonstationary. Our findings show that DRL is a potential method to address the FPD problem in real operations, especially because of its speed in decision-making. However, no single DRL model is able to outperform the rest in all scenarios, and the best approach for each of the 6 core elements depends on the features of the operation environment. While we agree on the potential of DRL to solve future complex problems in the aerospace industry, we also reflect on the importance of designing appropriate models and training procedures, understanding the applicability of such models, and reporting the main performance tradeoffs.',\n",
       "  'len': 1931},\n",
       " {'abstract': 'In this paper, we present the application of successive convexification methods to autonomous driving problems borrowed from recent aerospace literature. We formulate two optimization problems within the successive convexification framework. Using arc-length parametrization in the vehicle kinematic model, we solve the speed planning and model predictive control problems with a range of constraints and obstacle configurations. This paper is the first systematic application of successive convexification methods from the aerospace literature to the autonomous driving problems. In addition, we show a simple application of logical state-trigger constraints in a continuous formulation of the optimization by including an evasion maneuver in the simulations section. We give details of the problem formulation and implementation and present and discuss the results.',\n",
       "  'len': 878},\n",
       " {'abstract': 'The vulnerability of Global Positioning System (GPS) receivers to jammers is a major concern owing to the extremely weak received signal power of GPS. Researches have been conducted on a variety of antenna array techniques to be used as countermeasures to GPS jammers, and their antijamming performance is known to be greater than that of single antenna methods. However, the application of antenna arrays remains limited because of their size, cost, and computational complexity. This study proposes and experimentally validates a novel space-time-polarization domain adaptive processing for a single-element dual-polarized antenna (STPAPS) by focusing on the polarization diversity of a dual-polarized antenna. The mathematical models of arbitrarily polarized signals received by dual-polarized antenna are derived, and an appropriate constraint matrix for dual-polarized-antenna-based GPS antijam is suggested. To reduce the computational complexity of the constraint matrix approach, the eigenvector constraint design scheme is adopted. The performance of STPAPS is quantitively and qualitatively evaluated through experiments as follows. 1) The carrier-to-noise-density ratio (C/N0) of STPAPS under synthetic jamming is demonstrated to be higher than that of the previous minimum mean squared error (MMSE) or minimum variance distortionless response (MVDR) based dual-polarized antenna methods. 2) The strengths and weaknesses of STPAPS are qualitatively compared with those of the previous single-element dual-polarized antenna methods that are not based on the MMSE or MVDR algorithms. 3) The characteristics of STPAPS (in terms of the directions and polarizations of the GPS and jamming signals) are compared with those of the conventional two-element single-polarized antenna array method, which has the same degree of freedom as that of STPAPS.',\n",
       "  'len': 1865},\n",
       " {'abstract': 'Dielectric elastomers (DEs) that couple deformation and electrostatics have the potential for use in soft sensors and actuators with applications ranging from robotic, biomedical, energy, aerospace and automotive technologies. However, currently available DEs are limited by weak electromechanical coupling and require large electric fields for significant actuation. In this work, a statistical mechanics-based model of DE chains is applied to elucidate the role of a polymer network architecture in the performance of the bulk material. Given a polymer network composed of chains that are cross-linked, the paper examines the role of cross-link density, orientational density of chains, and other network parameters in determining the material properties of interest including elastic modulus, electrical susceptibility, and the electromechanical coupling. From this analysis, a practical strategy is presented to increase the deformation and usable work derived from (anisotropic) dielectric elastomer actuators by as much as $75-100\\\\%$.',\n",
       "  'len': 1051},\n",
       " {'abstract': 'Renewed interest in Very Low Earth Orbits (VLEO) - i.e. altitudes below 450 km - has led to an increased demand for accurate environment characterisation and aerodynamic force prediction. While the former requires knowledge of the mechanisms that drive density variations in the thermosphere, the latter also depends on the interactions between the gas-particles in the residual atmosphere and the surfaces exposed to the flow. The determination of the aerodynamic coefficients is hindered by the numerous uncertainties that characterise the physical processes occurring at the exposed surfaces. Several models have been produced over the last 60 years with the intent of combining accuracy with relatively simple implementations. In this paper the most popular models have been selected and reviewed using as discriminating factors relevance with regards to orbital aerodynamics applications and theoretical agreement with gas-beam experimental data. More sophisticated models were neglected, since their increased accuracy is generally accompanied by a substantial increase in computation times which is likely to be unsuitable for most space engineering applications. For the sake of clarity, a distinction was introduced between physical and scattering kernel theory based gas-surface interaction models. The physical model category comprises the Hard Cube model, the Soft Cube model and the Washboard model, while the scattering kernel family consists of the Maxwell model, the Nocilla-Hurlbut-Sherman model and the Cercignani-Lampis-Lord model. Limits and assets of each model have been discussed with regards to the context of this paper. Wherever possible, comments have been provided to help the reader to identify possible future challenges for gas-surface interaction science with regards to orbital aerodynamic applications.',\n",
       "  'len': 1847},\n",
       " {'abstract': 'Prognostics and Health Management (PHM) are emerging approaches to product life cycle that will maintain system safety and improve reliability, while reducing operating and maintenance costs. This is particularly relevant for aerospace systems, where high levels of integrity and high performances are required at the same time. We propose a novel strategy for the nearly real-time Fault Detection and Identification (FDI) of a dynamical assembly, and for the estimation of Remaining Useful Life (RUL) of the system. The availability of a timely estimate of the health status of the system will allow for an informed adaptive planning of maintenance and a dynamical reconfiguration of the mission profile, reducing operating costs and improving reliability. This work addresses the three phases of the prognostic flow - namely (1) signal acquisition, (2) Fault Detection and Identification, and (3) Remaining Useful Life estimation - and introduces a computationally efficient procedure suitable for real-time, on-board execution. To achieve this goal, we propose to combine information from physical models of different fidelity with machine learning techniques to obtain efficient representations (surrogate models) suitable for nearly real-time applications. Additionally, we propose an importance sampling strategy and a novel approach to model damage propagation for dynamical systems. The methodology is assessed for the FDI and RUL estimation of an aircraft electromechanical actuator (EMA) for secondary flight controls. The results show that the proposed method allows for a high precision in the evaluation of the system RUL, while outperforming common model-based techniques in terms of computational time.',\n",
       "  'len': 1728},\n",
       " {'abstract': 'Detumbling refers to the act of dampening the angular velocity of the satellite. This operation is of paramount importance since it is virtually impossible to nominally perform any other operation without some degree of attitude control. Common methods used to detumble satellites usually involve magnetic actuation, paired with different types of sensors which are used to provide angular velocity feedback. This paper presents the adverse effects of time-discretization on the stability of two detumbling algorithms. An extensive literature review revealed that both algorithms achieve absolute stability for systems involving continuous feedback and output. However, the physical components involved impose limitations on the maximum frequency of the algorithm, thereby making any such system inconceivable. This asserts the need to perform a discrete-time stability analysis, as it is better suited to reflect on the actual implementation and dynamics of these algorithms. The paper starts with the current theory and views on the stability of these algorithms. The next sections describe the continuous and discrete-time stability analysis performed by the team and the conclusions derived from it. Theoretical investigation led to the discovery of multiple conditions on angular velocity and operating frequencies of the hardware, for which the algorithms were unstable. These results were then verified through various simulations on MATLAB and Python3.6.7. The paper concludes with a discussion on the various instabilities posed by time-discretization and the conditions under which the detumbling algorithm would be infeasible.',\n",
       "  'len': 1648},\n",
       " {'abstract': 'Tipping in multistable systems occurs usually by varying the input slightly, resulting in the output switching to an often unsatisfactory state. This phenomenon is manifested in thermoacoustic systems. This thermoacoustic instability may lead to the disintegration of rocket engines, gas turbines and aeroengines, so it is necessary to design control measures for its suppression. It was speculated that such unwanted instability states may be dodged by changing quickly enough the bifurcation parameters. Thus, in this work, based on a fundamental mathematical model of thermoacoustic systems driven by colored noise, the corresponding Fokker-Planck-Kolmogorov equation of the amplitude is derived by using a stochastic averaging method. A transient dynamical behavior is identified through a probability density analysis. We find that the rate of change of parameters and the correlation time of the noise are helpful to dodge thermoacoustic instability, while a relatively large noise intensity is a disadvantageous factor. In addition, power-law relationships between the maximum amplitude and the noise parameters are explored, and the probability of successfully dodging a thermoacoustic instability is calculated. These results serve as a guide to the design of engines and to propose an effective control strategy, which is of great significance to aerospace-related fields.',\n",
       "  'len': 1393},\n",
       " {'abstract': 'Traditional Global Navigation Satellite System (GNSS) immunity to interference is limited by signal power and receiver antenna directivity, and may be approaching a practical performance ceiling. Greater gains are possible outside traditional GNSS orbits and spectrum. GNSS from low Earth orbit (LEO) has long been viewed as promising but expensive, requiring large constellations for rapid navigation solutions. The recent proliferation of commercial broadband constellations offers the possibility of extracting a LEO positioning, navigation, and timing (PNT) service from existing on-orbit hardware. These constellations operate at shorter wavelengths than traditional GNSS, permitting highly directive, relatively compact receiver antennas. Dedicated on-orbit PNT resources are not required: the transmitters, antennas, clocks, and spectrum of the hosting broadband network suffice for PNT. Non-cooperative use of LEO signals for PNT is an option, but cooperation from the constellation operator (\"fusion\" with its communications mission) eases the burden of tracking a dense, low-altitude constellation from the ground, and enables a receiver to produce single-epoch stand-alone PNT solutions. This paper proposes and analyzes the fused LEO GNSS concept. Viability hinges on opportunity cost, or the burden a secondary PNT mission imposes on the communications constellation operator. An in-depth quantitative opportunity cost assessment is provided in terms of time-space-bandwidth product and energy budget. It is shown that an assured near-instantaneous-fix PNT service over $\\\\pm60$° latitude (covering 99.8% of the world\\'s population) with positioning performance superior to traditional GNSS pseudoranging would cost less than 0.8% of downlink capacity for the largest of the new constellations, SpaceX\\'s Starlink.',\n",
       "  'len': 1835},\n",
       " {'abstract': \"We consider the problem of localizing a manned, semi-autonomous, or autonomous vehicle in the environment using information coming from the vehicle's sensors, a problem known as navigation or simultaneous localization and mapping (SLAM) depending on the context. To infer knowledge from sensors' measurements, while drawing on a priori knowledge about the vehicle's dynamics, modern approaches solve an optimization problem to compute the most likely trajectory given all past observations, an approach known as smoothing. Improving smoothing solvers is an active field of research in the SLAM community. Most work is focused on reducing computation load by inverting the involved linear system while preserving its sparsity. The present paper raises an issue which, to the knowledge of the authors, has not been addressed yet: standard smoothing solvers require explicitly using the inverse of sensor noise covariance matrices. This means the parameters that reflect the noise magnitude must be sufficiently large for the smoother to properly function. When matrices are close to singular, which is the case when using high precision modern inertial measurement units (IMU), numerical issues necessarily arise, especially with 32-bits implementation demanded by most industrial aerospace applications. We discuss these issues and propose a solution that builds upon the Kalman filter to improve smoothing algorithms. We then leverage the results to devise a localization algorithm based on fusion of IMU and vision sensors. Successful real experiments using an actual car equipped with a tactical grade high performance IMU and a LiDAR illustrate the relevance of the approach to the field of autonomous vehicles.\",\n",
       "  'len': 1725},\n",
       " {'abstract': \"This paper presents a system-level engineering approach for the preliminary coverage performance analysis and the design of a generic Global Navigation Satellite System (GNSS) constellation. This analysis accounts for both the coverage requirements and the robustness to transient or catastrophic failures of the constellation. The European GNSS, Galileo, is used as reference case to prove the effectiveness of the proposed tool. This software suite, named GNSS Coverage Analysis Tool (G-CAT), requires as input the state vector of each satellite of the constellation and provides the performance of the GNSS constellation in terms of coverage. The tool offers an orbit propagator, an attitude propagator, an algorithm to identify the visibility region on the Earth's surface from each satellite, and a counter function to compute how many satellites are in view from given locations on the Earth's surface. Thanks to its low computational burden, the tool can be adopted to compute the optimal number of satellites per each orbital plane by verifying if the coverage and accuracy requirements are fulfilled under the assumption of uniform in-plane angular spacing between coplanar satellites.\",\n",
       "  'len': 1205},\n",
       " {'abstract': 'The design of navigation observers able to simultaneously estimate the position, linear velocity and orientation of a vehicle in a three-dimensional space is crucial in many robotics and aerospace applications. This problem was mainly dealt with using the extended Kalman filter and its variants which proved to be instrumental in many practical applications. Although practically efficient, the lack of strong stability guarantees of these algorithms motivated the emergence of a new class of geometric navigation observers relying on Riemannian geometry tools, leading to provable strong stability properties. The objective of this brief tutorial is to provide an overview of the existing estimation schemes, as well as some recently developed geometric nonlinear observers, for autonomous navigation systems relying on inertial measurement unit (IMU) and landmark measurements.',\n",
       "  'len': 891},\n",
       " {'abstract': \"The secure command and control (C&C) of mobile agents arises in various settings including unmanned aerial vehicles, single pilot operations in commercial settings, and mobile robots to name a few. As more and more of these applications get integrated into aerospace and defense use cases, the security of the communication channel between the ground station and the mobile agent is of increasing importance. The development of quantum computing devices poses a unique threat to secure communications due to the vulnerability of asymmetric ciphers to Shor's algorithm. Given the active development of new quantum resistant encryption techniques, we report the first integration of post-quantum secure encryption schemes with the robot operating system (ROS) and C&C of mobile agents, in general. We integrate these schemes in the application and network layers, and study the performance of these methods by comparing them to present day security schemes such as the widely used RSA algorithm.\",\n",
       "  'len': 1004},\n",
       " {'abstract': \"Time series anomalies can offer information relevant to critical situations facing various fields, from finance and aerospace to the IT, security, and medical domains. However, detecting anomalies in time series data is particularly challenging due to the vague definition of anomalies and said data's frequent lack of labels and highly complex temporal correlations. Current state-of-the-art unsupervised machine learning methods for anomaly detection suffer from scalability and portability issues, and may have high false positive rates. In this paper, we propose TadGAN, an unsupervised anomaly detection approach built on Generative Adversarial Networks (GANs). To capture the temporal correlations of time series distributions, we use LSTM Recurrent Neural Networks as base models for Generators and Critics. TadGAN is trained with cycle consistency loss to allow for effective time-series data reconstruction. We further propose several novel methods to compute reconstruction errors, as well as different approaches to combine reconstruction errors and Critic outputs to compute anomaly scores. To demonstrate the performance and generalizability of our approach, we test several anomaly scoring techniques and report the best-suited one. We compare our approach to 8 baseline anomaly detection methods on 11 datasets from multiple reputable sources such as NASA, Yahoo, Numenta, Amazon, and Twitter. The results show that our approach can effectively detect anomalies and outperform baseline methods in most cases (6 out of 11). Notably, our method has the highest averaged F1 score across all the datasets. Our code is open source and is available as a benchmarking tool.\",\n",
       "  'len': 1692},\n",
       " {'abstract': 'Recent successes combine reinforcement learning algorithms and deep neural networks, despite reinforcement learning not being widely applied to robotics and real world scenarios. This can be attributed to the fact that current state-of-the-art, end-to-end reinforcement learning approaches still require thousands or millions of data samples to converge to a satisfactory policy and are subject to catastrophic failures during training. Conversely, in real world scenarios and after just a few data samples, humans are able to either provide demonstrations of the task, intervene to prevent catastrophic actions, or simply evaluate if the policy is performing correctly. This research investigates how to integrate these human interaction modalities to the reinforcement learning loop, increasing sample efficiency and enabling real-time reinforcement learning in robotics and real world scenarios. This novel theoretical foundation is called Cycle-of-Learning, a reference to how different human interaction modalities, namely, task demonstration, intervention, and evaluation, are cycled and combined to reinforcement learning algorithms. Results presented in this work show that the reward signal that is learned based upon human interaction accelerates the rate of learning of reinforcement learning algorithms and that learning from a combination of human demonstrations and interventions is faster and more sample efficient when compared to traditional supervised learning algorithms. Finally, Cycle-of-Learning develops an effective transition between policies learned using human demonstrations and interventions to reinforcement learning. The theoretical foundation developed by this research opens new research paths to human-agent teaming scenarios where autonomous agents are able to learn from human teammates and adapt to mission performance metrics in real-time and in real world scenarios.',\n",
       "  'len': 1916},\n",
       " {'abstract': 'Unmanned aircraft systems (UAS), or unmanned aerial vehicles (UAVs), often referred to as drones, have been experiencing healthy growth in the United States and around the world. The positive uses of UAS have the potential to save lives, increase safety and efficiency, and enable more effective science and engineering research. However, UAS are subject to threats stemming from increasing reliance on computer and communication technologies, which place public safety, national security, and individual privacy at risk. To promote safe, secure and privacy-respecting UAS operations, there is an urgent need for innovative technologies for detecting, tracking, identifying and mitigating UAS. A Counter-UAS (C-UAS) system is defined as a system or device capable of lawfully and safely disabling, disrupting, or seizing control of an unmanned aircraft or unmanned aircraft system. Over the past 5 years, significant research efforts have been made to detect, and mitigate UAS: detection technologies are based on acoustic, vision, passive radio frequency, radar, and data fusion; and mitigation technologies include physical capture or jamming. In this paper, we provide a comprehensive survey of existing literature in the area of C-UAS, identify the challenges in countering unauthorized or unsafe UAS, and evaluate the trends of detection and mitigation for protecting against UAS-based threats. The objective of this survey paper is to present a systematic introduction of C-UAS technologies, thus fostering a research community committed to the safe integration of UAS into the airspace system.',\n",
       "  'len': 1611},\n",
       " {'abstract': 'Brackets are an essential component in aircraft manufacture and design, joining parts together, supporting weight, holding wires, and strengthening joints. Hundreds or thousands of unique brackets are used in every aircraft, but manufacturing a large number of distinct brackets is inefficient and expensive. Fortunately, many so-called \"different\" brackets are in fact very similar or even identical to each other. In this manuscript, we present a data-driven framework for constructing a comparatively small group of representative brackets from a large catalog of current brackets, based on hierarchical clustering of bracket data. We find that for a modern commercial aircraft, the full set of brackets can be reduced by 30\\\\% while still describing half of the test set sufficiently accurately. This approach is based on designing an inner product that quantifies a multi-objective similarity between two brackets, which are the \"bra\" and the \"ket\" of the inner product. Although we demonstrate this algorithm to reduce the number of brackets in aerospace manufacturing, it may be generally applied to any large-scale component standardization effort.',\n",
       "  'len': 1166},\n",
       " {'abstract': 'The capital market plays a vital role in marketing operations for aerospace industry. However, due to the uncertainty and complexity of the stock market and many cyclical factors, the stock prices of listed aerospace companies fluctuate significantly. This makes the share price prediction challengeable. To improve the prediction of share price for aerospace industry sector and well understand the impact of various indicators on stock prices, we provided a hybrid prediction model by the combination of Principal Component Analysis (PCA) and Recurrent Neural Networks. We investigated two types of aerospace industries (manufacturer and operator). The experimental results show that PCA could improve both accuracy and efficiency of prediction. Various factors could influence the performance of prediction models, such as finance data, extracted features, optimisation algorithms, and parameters of the prediction model. The selection of features may depend on the stability of historical data: technical features could be the first option when the share price is stable, whereas fundamental features could be better when the share price has high fluctuation. The delays of RNN also depend on the stability of historical data for different types of companies. It would be more accurate through using short-term historical data for aerospace manufacturers, whereas using long-term historical data for aerospace operating airlines. The developed model could be an intelligent agent in an automatic stock prediction system, with which, the financial industry could make a prompt decision for their economic strategies and business activities in terms of predicted future share price, thus improving the return on investment. Currently, COVID-19 severely influences aerospace industries. The developed approach can be used to predict the share price of aerospace industries at post COVID-19 time.',\n",
       "  'len': 1907},\n",
       " {'abstract': 'Data science, and machine learning in particular, is rapidly transforming the scientific and industrial landscapes. The aerospace industry is poised to capitalize on big data and machine learning, which excels at solving the types of multi-objective, constrained optimization problems that arise in aircraft design and manufacturing. Indeed, emerging methods in machine learning may be thought of as data-driven optimization techniques that are ideal for high-dimensional, non-convex, and constrained, multi-objective optimization problems, and that improve with increasing volumes of data. In this review, we will explore the opportunities and challenges of integrating data-driven science and engineering into the aerospace industry. Importantly, we will focus on the critical need for interpretable, generalizeable, explainable, and certifiable machine learning techniques for safety-critical applications. This review will include a retrospective, an assessment of the current state-of-the-art, and a roadmap looking forward. Recent algorithmic and technological trends will be explored in the context of critical challenges in aerospace design, manufacturing, verification, validation, and services. In addition, we will explore this landscape through several case studies in the aerospace industry. This document is the result of close collaboration between UW and Boeing to summarize past efforts and outline future opportunities.',\n",
       "  'len': 1448},\n",
       " {'abstract': 'In aerospace engineering and boat building, fluid-structure interaction models are considered to investigate prototypes before they are physically assembled. How a material interacts with different fluids at different Reynold numbers has to be studied before it is passed over to the manufacturing process. In addition, examining the same model not only for different fluids but also for different solids allows to optimize the choice of materials for construction even better. A possible answer on this demand is parameter-dependent discretization. Furthermore, low-rank techniques can reduce the complexity needed to compute approximations to parameter-dependent fluid-structure interaction discretizations. Low-rank methods have been applied to parameter-dependent linear fluid-structure interaction discretizations. The linearity of the operators involved allows to translate the resulting equations to a single matrix equation. The solution is approximated by a low-rank method. In this paper, we propose a new method that extends this framework to nonlinear parameter-dependent fluid-structure interaction problems by means of the Newton iteration. The parameter set is split into disjoint subsets. On each subset, the Newton approximation of the problem related to the upper median parameter is computed and serves as initial guess for one Newton step on the whole subset. This Newton step yields a matrix equation whose solution can be approximated by a low-rank method. The resulting method requires a smaller number of Newton steps if compared with a direct approach that applies the Newton iteration to the separate problems consecutively. In the experiments considered, the proposed method allowed to compute a low-rank approximation within a twentieth of the time used by the direct approach.',\n",
       "  'len': 1816},\n",
       " {'abstract': 'Constrained attitude maneuvers have numerous applications in robotics and aerospace. In our previous work, a general framework to this problem was proposed with resolution completeness guarantee. However, a smooth reference trajectory and a low-level safety-critical controller were lacking. In this work, we propose a novel construction of a $C^2$ continuous reference trajectory based on Bézier curves on $ SO(3) $ that evolves within predetermined cells and eliminates previous stop-and-go behavior. Moreover, we propose a novel zeroing control barrier function on $ SO(3) $ that provides a safety certificate over a set of overlapping cells on $ SO(3) $ while avoiding nonsmooth analysis. The safety certificate is given as a linear constraint on the control input and implemented in real-time. A remedy is proposed to handle the states where the coefficient of the control input in the linear constraint vanishes. Numerical simulations are given to verify the advantages of the proposed method.',\n",
       "  'len': 1010},\n",
       " {'abstract': \"Life events can dramatically affect our psychological state and work performance. Stress, for example, has been linked to professional dissatisfaction, increased anxiety, and workplace burnout. We explore the impact of positive and negative life events on a number of psychological constructs through a multi-month longitudinal study of hospital and aerospace workers. Through causal inference, we demonstrate that positive life events increase positive affect, while negative events increase stress, anxiety and negative affect. While most events have a transient effect on psychological states, major negative events, like illness or attending a funeral, can reduce positive affect for multiple days. Next, we assess whether these events can be detected through wearable sensors, which can cheaply and unobtrusively monitor health-related factors. We show that these sensors paired with embedding-based learning models can be used ``in the wild'' to capture atypical life events in hundreds of workers across both datasets. Overall our results suggest that automated interventions based on physiological sensing may be feasible to help workers regulate the negative effects of life events.\",\n",
       "  'len': 1202},\n",
       " {'abstract': 'In the field of computer vision, unsupervised learning for 2D object generation has advanced rapidly in the past few years. However, 3D object generation has not garnered the same attention or success as its predecessor. To facilitate novel progress at the intersection of computer vision and materials science, we propose a 3DMaterialGAN network that is capable of recognizing and synthesizing individual grains whose morphology conforms to a given 3D polycrystalline material microstructure. This Generative Adversarial Network (GAN) architecture yields complex 3D objects from probabilistic latent space vectors with no additional information from 2D rendered images. We show that this method performs comparably or better than state-of-the-art on benchmark annotated 3D datasets, while also being able to distinguish and generate objects that are not easily annotated, such as grain morphologies. The value of our algorithm is demonstrated with analysis on experimental real-world data, namely generating 3D grain structures found in a commercially relevant wrought titanium alloy, which were validated through statistical shape comparison. This framework lays the foundation for the recognition and synthesis of polycrystalline material microstructures, which are used in additive manufacturing, aerospace, and structural design applications.',\n",
       "  'len': 1358},\n",
       " {'abstract': 'The introduction of graphene-related materials (GRMs) in carbon fibre-reinforced polymers (CFRP) has been proved to enhance their mechanical and electrical properties. However, methodologies to produce the 3-phase materials (multiscale composites) at an industrial scale and in an efficient manner are still lacking. In this paper, multiscale CFRP composites containing different GRMs have been manufactured following standard procedures currently used in the aerospace industry with the aim to evaluate its potential application. Graphite nanoplateletelets (GNPs), in situ exfoliated graphene oxide (GO) and reduced graphene oxide (rGO) have been dispersed into an epoxy resin to subsequently impregnate aeronautical grade carbon fibre tape. The resulting prepregs have been used for manufacturing laminates by hand lay-up and autoclave curing at 180 °C. Abroad characterization campaign has been carried out to understand the behaviour of the different multiscale laminates manufactured. The degree of cure, glass transition temperature and degradation temperature have been evaluated by thermal evolution techniques. Similarly, their mechanical properties (tensile, flexural, in-plane shear, interlaminar shear and mode I interlaminar fracture toughness) have been analysed together with their electrical conductivity. The manufacturing process resulted appropriated for producing three-phase laminates and their quality was as good as in conventional CFRPs. The addition ofGOand rGO resulted in an enhancement of the in-plane shear properties and delamination resistance while the addition ofGNPimproved the electrical conductivity.',\n",
       "  'len': 1647},\n",
       " {'abstract': 'This article is dedicated to the Earth Remote Sensing (ERS), which the authors believe is a great way to teach geography and allows forming an idea of the actual geographic features and phenomena. One of the major problems that now constrains the active introduction of remote sensing data in the educational process is the low availability of training aerospace pictures, which meet didactic requirements. The article analyzes the main sources of ERS as a basis for educational resources formation with aerospace images: paper, various individual sources (personal stations receiving satellite information, drones, balloons, kites and balls) and Internet sources (mainstream sites, sites of scientific-technical organizations and distributors, interactive Internet geoservices, cloud platforms of geospatial analysis). The authors point out that their geospatial analysis platforms (Google Earth Engine, Land Viewer, EOS Platform, etc.), due to their unique features, are the basis for the creation of information thematic databases of ERS. The article presents an example of such a database, covering more than 800 aerospace images and dynamic models, which are combined according to such didactic principles as high information load and clarity.',\n",
       "  'len': 1259},\n",
       " {'abstract': 'Land vehicle navigation based on inertial navigation system (INS) and odometers is a classical autonomous navigation application and has been extensively studied over the past several decades. In this work, we seriously analyze the error characteristics of the odometer (OD) pulses and investigate three types of odometer measurement models in the INS/OD integrated system. Specifically, in the pulse velocity model, a preliminary Kalman filter is designed to obtain accurate vehicle velocity from the accumulated pulses; the pulse increment model is accordingly obtained by integrating the pulse velocity; a new pulse accumulation model is proposed by augmenting the travelled distance into the system state. The three types of measurements, along with the nonhonolomic constraint (NHC), are implemented in the standard extended Kalman filter. In view of the motion-related pulse error characteristics, the multiple model adaptive estimation (MMAE) approach is exploited to further enhance the performance. Simulations and long-distance experiments are conducted to verify the feasibility and effectiveness of the proposed methods. It is shown that the standard pulse velocity measurement achieves the superior performance, whereas the accumulated pulse measurement is most favorable with the MMAE enhancement.',\n",
       "  'len': 1322},\n",
       " {'abstract': 'Very low Earth orbits (VLEO), typically classified as orbits below approximately 450 km in altitude, have the potential to provide significant benefits to spacecraft over those that operate in higher altitude orbits. This paper provides a comprehensive review and analysis of these benefits to spacecraft operations in VLEO, with parametric investigation of those which apply specifically to Earth observation missions. The most significant benefit for optical imaging systems is that a reduction in orbital altitude improves spatial resolution for a similar payload specification. Alternatively mass and volume savings can be made whilst maintaining a given performance. Similarly, for radar and lidar systems, the signal-to-noise ratio can be improved. Additional benefits include improved geospatial position accuracy, improvements in communications link-budgets, and greater launch vehicle insertion capability. The collision risk with orbital debris and radiation environment can be shown to be improved in lower altitude orbits, whilst compliance with IADC guidelines for spacecraft post-mission lifetime and deorbit is also assisted. Finally, VLEO offers opportunities to exploit novel atmosphere-breathing electric propulsion systems and aerodynamic attitude and orbit control methods. However, key challenges associated with our understanding of the lower thermosphere, aerodynamic drag, the requirement to provide a meaningful orbital lifetime whilst minimising spacecraft mass and complexity, and atomic oxygen erosion still require further research. Given the scope for significant commercial, societal, and environmental impact which can be realised with higher performing Earth observation platforms, renewed research efforts to address the challenges associated with VLEO operations are required.',\n",
       "  'len': 1822},\n",
       " {'abstract': 'Previous efforts to directly write conductive metals have been narrowly focused on nanoparticle ink suspensions that require aggressive sintering (>200 °C) and result in low-density, small-grained agglomerates with electrical conductivities <25% of bulk metal. Here, we demonstrate aerosol jet printing of a reactive ink solution and characterize high-density (93%) printed silver traces having near-bulk conductivity and grain sizes greater than the electron mean free path, while only requiring a low-temperature (80 °C) treatment. We have developed a predictive electronic transport model which correlates the microstructure to the measured conductivity and identifies a strategy to approach the practical conductivity limit for printed metals. Our analysis of how grain boundaries and tortuosity contribute to electrical resistivity provides insight into the basic materials science that governs how an ink formulator or process developer might approach improving the conductivity. Transmission line measurements validate that electrical properties are preserved up to 20 GHz, which demonstrates the utility of this technique for printed RF components. This work reveals a new method of producing robust printed electronics that retain the advantages of rapid prototyping and three-dimensional fabrication while achieving the performance necessary for success within the aerospace and communications industries.',\n",
       "  'len': 1426},\n",
       " {'abstract': 'Global Navigation Satellite Systems (GNSS) provide ubiquitous, continuous and reliable positioning, navigation and timing information around the world. However, GNSS space segment design decisions have been based on the precursor Global Positioning System (GPS), which was designed in the 70s. This paper revisits those early design decisions in view of major technological advancements and new GNSS environmental threats. The rich tradespace between User Navigation Error (UNE) performance and constellation deployment costs is explored and conclusions are complemented by sensitivity analysis and association rule mining. This study finds that constellations at an orbit altitude of ~2 Earth radii can outperform existing GNSS in terms of cost, robustness and UNE. These insights should be taken into account when designing future generations of GNSS.',\n",
       "  'len': 864},\n",
       " {'abstract': 'Several formulations are describing the pattern of species-area relationship, log-log linear, semi-log linear, among others. These patterns mainly explain the species-area relationship for large areas, and for the small area, they provide significant differences from real data. We consider the geometric position of individuals of species, and base on that, we find the probability of observing at least one individual of the species. We apply a translation of the well-studied problem of mixed salt-water in a tank to describe the formula of SAR. For a rectangular sample area the species-area relationship follows the pattern, with some simplification, $S=c|A^{\\\\beta}+a|^z$, where $S$ is the number of species in the area of size $A$ and $a,c,z$, and $\\\\beta$ are constants with $z<1$ and $\\\\beta\\\\leq1$. We also show how the constant $z$ relates to some macroecological patterns, namely spatial aggregation, percentage of area coverage, and the core-satellite model. We exemplify our method using data on tropical tree species from a 50ha plot in Barro Colorado Island (BCI), Panama, using all individuals.',\n",
       "  'len': 1118},\n",
       " {'abstract': 'Raw moments are used as a way to estimate species abundance distribution. The almost linear pattern of the log transformation of raw moments across scales allow us to extrapolate species abundance distribution for larger areas. However, results may produce errors. Some of these errors are due to computational complexity, fittings of patterns, binning methods, and so on. We provide some methods to reduce some of the errors. The main result is introducing new techniques for evaluating a more accurate species abundance distributions across scales through moments across scales.',\n",
       "  'len': 591},\n",
       " {'abstract': \"Supervised imitation learning, also known as behavioral cloning, suffers from distribution drift leading to failures during policy execution. One approach to mitigate this issue is to allow an expert to correct the agent's actions during task execution, based on the expert's determination that the agent has reached a `point of no return.' The agent's policy is then retrained using this new corrective data. This approach alone can enable high-performance agents to be learned, but at a substantial cost: the expert must vigilantly observe execution until the policy reaches a specified level of success, and even at that point, there is no guarantee that the policy will always succeed. To address these limitations, we present FIRE (Failure Identification to Reduce Expert Burden in intervention-based learning), a system that can predict when a running policy will fail, halt its execution, and request a correction from the expert. Unlike existing approaches that learn only from expert data, our approach learns from both expert and non-expert data, akin to adversarial learning. We demonstrate experimentally for a series of challenging manipulation tasks that our method is able to recognize state-action pairs that lead to failures. This permits seamless integration into an intervention-based learning system, where we show an order-of-magnitude gain in sample efficiency compared with a state-of-the-art inverse reinforcement learning method and dramatically improved performance over an equivalent amount of data learned with behavioral cloning.\",\n",
       "  'len': 1569},\n",
       " {'abstract': 'The design process of complex systems such as new configurations of aircraft or launch vehicles is usually decomposed in different phases which are characterized for instance by the depth of the analyses in terms of number of design variables and fidelity of the physical models. At each phase, the designers have to compose with accurate but computationally intensive models as well as cheap but inaccurate models. Multi-fidelity modeling is a way to merge different fidelity models to provide engineers with accurate results with a limited computational cost. Within the context of multi-fidelity modeling, approaches relying on Gaussian Processes emerge as popular techniques to fuse information between the different fidelity models. The relationship between the fidelity models is a key aspect in multi-fidelity modeling. This paper provides an overview of Gaussian process-based multi-fidelity modeling techniques for variable relationship between the fidelity models (e.g., linearity, non-linearity, variable correlation). Each technique is described within a unified framework and the links between the different techniques are highlighted. All the approaches are numerically compared on a series of analytical test cases and four aerospace related engineering problems in order to assess their benefits and disadvantages with respect to the problem characteristics.',\n",
       "  'len': 1385},\n",
       " {'abstract': 'This paper discusses a PhD research project testing the hypothesis that using the United Nations Sustainable Development Goals(SDG) as explicit inputs to drive the Software Requirements Engineering process will result in requirements with improved sustainability benefits. The research has adopted the Design Science Research Method (DSRM) [21] to test a process named SDG Assessment for Requirements Elicitation (SDGARE). Three DSRM cycles are being used to test the hypothesis in safety-critical, highprecision, software-intensive systems in aerospace and healthcare. Initial results from the first two DSRM cycles support the hypothesis. However, these cycles are in a plan-driven (waterfall) development context and future research agenda would be a similar application in an Agile development context.',\n",
       "  'len': 817},\n",
       " {'abstract': 'Terrain assessment is a key aspect for autonomous exploration rovers, surrounding environment recognition is required for multiple purposes, such as optimal trajectory planning and autonomous target identification. In this work we present a technique to generate accurate three-dimensional semantic maps for Martian environment. The algorithm uses as input a stereo image acquired by a camera mounted on a rover. Firstly, images are labeled with DeepLabv3+, which is an encoder-decoder Convolutional Neural Networl (CNN). Then, the labels obtained by the semantic segmentation are combined to stereo depth-maps in a Voxel representation. We evaluate our approach on the ESA Katwijk Beach Planetary Rover Dataset.',\n",
       "  'len': 723},\n",
       " {'abstract': 'Current studies on Unmanned Aerial Vehicle (UAV) based cellular deployment consider UAVs as aerial base stations for air-to-ground communication. However, they analyze UAV coverage radius and altitude interplay while omitting or over-simplifying an important aspect of UAV deployment, i.e., effect of a realistic antenna pattern. This paper addresses the UAV deployment problem while using a realistic 3D directional antenna model. New trade-offs between UAV design space dimensions are revealed and analyzed in different scenarios. The sensitivity of coverage area to both antenna beamwidth and height is compared. The analysis is extended to multiple UAVs and a new packing scheme is proposed for multiple UAVs coverage that offers several advantages compared to prior approaches.',\n",
       "  'len': 793},\n",
       " {'abstract': \"Quantum illumination (QI) promises unprecedented performances in target detection but there are various problems surrounding its implementation. Where target ranging is a concern, signal and idler recombination forms a crucial barrier to the protocol's success. This could potentially be mitigated if performing a measurement on the idler mode could still yield a quantum advantage. In this paper we investigate the QI protocol for a generically correlated Gaussian source and study the phase-conjugating (PC) receiver, deriving the associated SNR in terms of the signal and idler energies, and their cross-correlations, which may be readily adapted to incorporate added noise due to Gaussian measurements. We confirm that a heterodyne measurement performed on the idler mode leads to a performance which asymptotically approaches that of a coherent state with homodyne detection. However, if the signal mode is affected by heterodyne but the idler mode is maintained clean, the performance asymptotically approaches that of the PC receiver without any added noise.\",\n",
       "  'len': 1076},\n",
       " {'abstract': 'Eigensystem Realization Algorithm (ERA) is a tool that can produce a reduced order model (ROM) from just input-output data of a given system. ERA creates the ROM while keeping the number of internal states to a minimum level. This was first implemented by Juang and Pappa (1984) to analyze the vibration of aerospace structures from impulse response. We reviewed ERA and tested it on single input single output (SISO) system as well as on multiple input single output (MISO) system. ERA prediction agreed with the actual data. Unlike other model reduction techniques (Balanced truncation, balanced proper orthogonal decomposition), ERA works just as fine without the need of the adjoint system, that makes ERA a promising, completely data-driven, thrifty model reduction method. In this work, we propose a modified Eigensystem Realization Algorithm that relies upon an optimally chosen time resolution for the output used and also checks for good performance through frequency analysis. Four examples are discussed: the first two confirm the model generating ability and the last two illustrate its capability to produce a low-dimensional model (for a large scale system) that is much more accurate than the one produced by the traditional ERA.',\n",
       "  'len': 1255},\n",
       " {'abstract': 'The main focus of the work presented in this thesis is to develop an optimal control based formation flying control strategy for high precision formation flying of small satellites that have restricted computation and storage capacity. Using the recently developed model predictive static programming (MPSP), and Generalized MPSP algorithm a suboptimal guidance logic is presented for the formation flying of small satellites. The proposed guidance scheme is valid both for high eccentricity chief satellite orbits as well as the large separation distance between the chief and deputy satellites. Comparative study with standard Linear Quadratic Regulator (LQR) solution (which serves as a guess solution for MPSP) and another nonlinear controller, Finite-time State-Dependent Ricatti Equation (SDRE) reveals that MPSP guidance achieves the objective with higher accuracy and with a lesser amount of control usage.',\n",
       "  'len': 925},\n",
       " {'abstract': 'Combinatorial optimization has found applications in numerous fields, from aerospace to transportation planning and economics. The goal is to find an optimal solution among a finite set of possibilities. The well-known challenge one faces with combinatorial optimization is the state-space explosion problem: the number of possibilities grows exponentially with the problem size, which makes solving intractable for large problems. In the last years, deep reinforcement learning (DRL) has shown its promise for designing good heuristics dedicated to solve NP-hard combinatorial optimization problems. However, current approaches have two shortcomings: (1) they mainly focus on the standard travelling salesman problem and they cannot be easily extended to other problems, and (2) they only provide an approximate solution with no systematic ways to improve it or to prove optimality. In another context, constraint programming (CP) is a generic tool to solve combinatorial optimization problems. Based on a complete search procedure, it will always find the optimal solution if we allow an execution time large enough. A critical design choice, that makes CP non-trivial to use in practice, is the branching decision, directing how the search space is explored. In this work, we propose a general and hybrid approach, based on DRL and CP, for solving combinatorial optimization problems. The core of our approach is based on a dynamic programming formulation, that acts as a bridge between both techniques. We experimentally show that our solver is efficient to solve two challenging problems: the traveling salesman problem with time windows, and the 4-moments portfolio optimization problem. Results obtained show that the framework introduced outperforms the stand-alone RL and CP solutions, while being competitive with industrial solvers.',\n",
       "  'len': 1854},\n",
       " {'abstract': 'Polymeric materials that couple deformation and electrostatics have the potential for use in soft sensors and actuators with potential applications ranging from robotic, biomedical, energy, aerospace and automotive technologies. In contrast to the mechanics of polymers that has been studied using statistical mechanics approaches for decades, the coupled response under deformation and electrical field has largely been modeled only phenomenologically at the continuum scale. In this work, we examine the physics of the coupled deformation and electrical response of an electrically-responsive polymer chain using statistical mechanics. We begin with a simple anisotropic model for the electrostatic dipole response to electric field of a single monomer, and use a separation of energy scales between the electrostatic field energy and the induced dipole field energy to reduce the nonlocal and infinite-dimensional statistical averaging to a simpler local finite-dimensional averaging. In this simplified setting, we derive the equations of the most likely monomer orientation density using the maximum term approximation, and a chain free energy is derived using this approximation. These equations are investigated numerically and the results provide insight into the physics of electro-mechanically coupled elastomer chains. Closed-form approximations are also developed in the limit of small electrical energy with respect to thermal energy; in the limit of small mechanical tension force acting on the chain; and using asymptotic matching for general chain conditions.',\n",
       "  'len': 1586},\n",
       " {'abstract': 'In this paper we present a simulation framework for the evaluation of the navigation and localization metrological performances of a robotic platform. The simulator, based on ROS (Robot Operating System) Gazebo, is targeted to a planetary-like research vehicle which allows to test various perception and navigation approaches for specific environment conditions. The possibility of simulating arbitrary sensor setups comprising cameras, LiDARs (Light Detection and Ranging) and IMUs makes Gazebo an excellent resource for rapid prototyping. In this work we evaluate a variety of open-source visual and LiDAR SLAM (Simultaneous Localization and Mapping) algorithms in a simulated Martian environment. Datasets are captured by driving the rover and recording sensors outputs as well as the ground truth for a precise performance evaluation.',\n",
       "  'len': 850},\n",
       " {'abstract': 'Eye gaze controlled interfaces allow us to directly manipulate a graphical user interface just by looking at it. This technology has great potential in military aviation, in particular, operating different displays in situations where pilots hands are occupied with flying the aircraft. This paper reports studies on analyzing accuracy of eye gaze controlled interface inside aircraft undertaking representative flying missions. We reported that pilots can undertake representative pointing and selection tasks at less than 2 secs on average. Further, we evaluated the accuracy of eye gaze tracking glass under various G-conditions and analyzed its failure modes. We observed that the accuracy of an eye tracker is less than 5 degree of visual angle up to +3G, although it is less accurate at minus 1G and plus 5G. We observed that eye tracker may fail to track under higher external illumination. We also infer that an eye tracker to be used in military aviation need to have larger vertical field of view than the present available systems. We used this analysis to develop eye gaze trackers for Multi-Functional displays and Head Mounted Display System. We obtained significant reduction in pointing and selection times using our proposed HMDS system compared to traditional TDS.',\n",
       "  'len': 1293},\n",
       " {'abstract': \"Advanced embedded algorithms are growing in complexity and they are an essential contributor to the growth of autonomy in many areas. However, the promise held by these algorithms cannot be kept without proper attention to the considerably stronger design constraints that arise when the applications of interest, such as aerospace systems, are safety-critical. Formal verification is the process of proving or disproving the ''correctness'' of an algorithm with respect to a certain mathematical description of it by means of a computer. This article discusses the formal verification of the Ellipsoid method, a convex optimization algorithm, and its code implementation as it applies to receding horizon control. Options for encoding code properties and their proofs are detailed. The applicability and limitations of those code properties and proofs are presented as well. Finally, floating-point errors are taken into account in a numerical analysis of the Ellipsoid algorithm. Modifications to the algorithm are presented which can be used to control its numerical stability.\",\n",
       "  'len': 1091},\n",
       " {'abstract': \"The problem of cooperative localization for a small group of Unmanned Aerial Vehicles (UAVs) in a GNSS denied environment is addressed in this paper. The presented approach contains two sequential steps: first, an algorithm called cooperative ranging localization, formulated as an Extended Kalman Filter (EKF), estimates each UAV's relative pose inside the group using inter-vehicle ranging measurements; second, an algorithm named cooperative magnetic localization, formulated as a particle filter, estimates each UAV's global pose through matching the group's magnetic anomaly measurements to a given magnetic anomaly map. In this study, each UAV is assumed to only perform a ranging measurement and data exchange with one other UAV at any point in time. A simulator is developed to evaluate the algorithms with magnetic anomaly maps acquired from airborne geophysical survey. The simulation results show that the average estimated position error of a group of 16 UAVs is approximately 20 meters after flying about 180 kilometers in 1 hour. Sensitivity analysis shows that the algorithms can tolerate large variations of velocity, yaw rate, and magnetic anomaly measurement noises. Additionally, the UAV group shows improved position estimation robustness with both high and low resolution maps as more UAVs are added into the group.\",\n",
       "  'len': 1347},\n",
       " {'abstract': 'Time-Sensitive Networking (TSN) is a set of amendments that extend Ethernet to support distributed safety-critical and real-time applications in the industrial automation, aerospace and automotive areas. TSN integrates multiple traffic types and supports interactions in several combinations. In this paper we consider the configuration supporting Scheduled Traffic (ST) traffic scheduled based on Gate-Control-Lists (GCLs), Audio-Video-Bridging (AVB) traffic according to IEEE 802.1BA that has bounded latencies, and Best-Effort (BE) traffic, for which no guarantees are provided. The paper extends the timing analysis method to multiple AVB classes and proofs the credit bounds for multiple classes of AVB traffic, respectively under frozen and non-frozen behaviors of credit during guard band (GB). They are prerequisites for non-overflow credits of Credit-Based Shaper (CBS) and preventing starvation of AVB traffic. Moreover, this paper proposes an improved timing analysis method reducing the pessimism for the worst-case end-to-end delays of AVB traffic by considering the limitations from the physical link rate and the output of CBS. Finally, we evaluate the improved analysis method on both synthetic and real-world test cases, showing the significant reduction of pessimism on latency bounds compared to related work, and presenting the correctness validation compared with simulation results. We also compare the AVB latency bounds in the case of frozen and non-frozen credit during GB. Additionally, we evaluate the scalability of our method with variation of the load of ST flows and of the bandwidth reservation for AVB traffic.',\n",
       "  'len': 1654},\n",
       " {'abstract': \"Low-latency telerobotics can enable more intricate surface tasks on extraterrestrial planetary bodies than has ever been attempted. For humanity to create a sustainable lunar presence, well-developed collaboration between humans and robots is necessary to perform complex tasks. This paper presents a methodology to assess the human factors, situational awareness (SA) and cognitive load (CL), associated with teleoperated assembly tasks. Currently, telerobotic assembly on an extraterrestrial body has never been attempted, and a valid methodology to assess the associated human factors has not been developed. The Telerobotics Laboratory at the University of Colorado-Boulder created the Telerobotic Simulation System (TSS) which enables remote operation of a rover and a robotic arm. The TSS was used in a laboratory experiment designed as an analog to a lunar mission. The operator's task was to assemble a radio interferometer. Each participant completed this task under two conditions, remote teleoperation (limited SA) and local operation (optimal SA). The goal of the experiment was to establish a methodology to accurately measure the operator's SA and CL while performing teleoperated assembly tasks. A successful methodology would yield results showing greater SA and lower CL while operating locally. Performance metrics showed greater SA and lower CL in the local environment, supported by a 27% increase in the mean time to completion of the assembly task when operating remotely. Subjective measurements of SA and CL did not align with the performance metrics. Results from this experiment will guide future work attempting to accurately quantify the human factors associated with telerobotic assembly. Once an accurate methodology has been developed, we will be able to measure how new variables affect an operator's SA and CL to optimize the efficiency and effectiveness of telerobotic assembly tasks.\",\n",
       "  'len': 1929},\n",
       " {'abstract': 'Here, we provide a theoretical framework revealing that a steady compression ramp flow must have the minimal dissipation of kinetic energy, and can be demonstrated using the least action principle. For a given inflow Mach number $M_{0}$ and ramp angle $\\\\alpha$, the separation angle $\\\\theta_{s}$ manifesting flow system states can be determined based on this theory. Thus, both the shapes of shock wave configurations and pressure peak $p_{peak}$ behind reattachment shock waves are predictable. These theoretical predictions agree excellently with both experimental data and numerical simulations, covering a wide range of $M_{0}$ and $\\\\alpha$. In addition, for a large separation, the theory indicates that $\\\\theta_{s}$ only depends on $M_{0}$ and $\\\\alpha$, but is independent of the Reynolds number $Re$ and wall temperature $T_{w}$. These facts suggest that the proposed theoretical framework can be applied to other flow systems dominated by shock waves, which are ubiquitous in aerospace engineering.',\n",
       "  'len': 1017},\n",
       " {'abstract': 'Nature has inspired various ground-breaking technological developments in applications ranging from robotics to aerospace engineering and the manufacturing of medical devices. However, accessing the information captured in scientific biology texts is a time-consuming and hard task that requires domain-specific knowledge. Improving access for outsiders can help interdisciplinary research like Nature Inspired Engineering. This paper describes a dataset of 1,500 manually-annotated sentences that express domain-independent relations between central concepts in a scientific biology text, such as trade-offs and correlations. The arguments of these relations can be Multi Word Expressions and have been annotated with modifying phrases to form non-projective graphs. The dataset allows for training and evaluating Relation Extraction algorithms that aim for coarse-grained typing of scientific biological documents, enabling a high-level filter for engineers.',\n",
       "  'len': 971},\n",
       " {'abstract': \"Digital twin technology has a huge potential for widespread applications in different industrial sectors such as infrastructure, aerospace, and automotive. However, practical adoptions of this technology have been slower, mainly due to a lack of application-specific details. Here we focus on a digital twin framework for linear single-degree-of-freedom structural dynamic systems evolving in two different operational time scales in addition to its intrinsic dynamic time-scale. Our approach strategically separates into two components -- (a) a physics-based nominal model for data processing and response predictions, and (b) a data-driven machine learning model for the time-evolution of the system parameters. The physics-based nominal model is system-specific and selected based on the problem under consideration. On the other hand, the data-driven machine learning model is generic. For tracking the multi-scale evolution of the system parameters, we propose to exploit a mixture of experts as the data-driven model. Within the mixture of experts model, Gaussian Process (GP) is used as the expert model. The primary idea is to let each expert track the evolution of the system parameters at a single time-scale. For learning the hyperparameters of the `mixture of experts using GP', an efficient framework the exploits expectation-maximization and sequential Monte Carlo sampler is used. Performance of the digital twin is illustrated on a multi-timescale dynamical system with stiffness and/or mass variations. The digital twin is found to be robust and yields reasonably accurate results. One exciting feature of the proposed digital twin is its capability to provide reasonable predictions at future time-steps. Aspects related to the data quality and data quantity are also investigated.\",\n",
       "  'len': 1810},\n",
       " {'abstract': \"Disturbances in space weather can negatively affect several fields, including aviation and aerospace, satellites, oil and gas industries, and electrical systems, leading to economic and commercial losses. Solar flares are the most significant events that can affect the Earth's atmosphere, thus leading researchers to drive efforts on their forecasting. The related literature is comprehensive and holds several systems proposed for flare forecasting. However, most techniques are tailor-made and designed for specific purposes, not allowing researchers to customize them in case of changes in data input or in the prediction algorithm. This paper proposes a framework to design, train, and evaluate flare prediction systems which present promising results. Our proposed framework involves model and feature selection, randomized hyper-parameters optimization, data resampling, and evaluation under operational settings. Compared to baseline predictions, our framework generated some proof-of-concept models with positive recalls between 0.70 and 0.75 for forecasting $\\\\geq M$ class flares up to 96 hours ahead while keeping the area under the ROC curve score at high levels.\",\n",
       "  'len': 1186},\n",
       " {'abstract': 'Vitrimers are a special class of polymers that undergo dynamic cross-linking under thermal stimuli. Their ability to exchange covalent bonds can be harnessed to mitigate damage in a composite or to achieve recyclable aerospace composites. This work addresses the primary challenge of modeling dynamic cross-linking reactions in vitrimers during thermomechanical loading. Dynamic bond exchange reaction probability change during heating and its effect on dilatometric and mechanical response are simulated for the first time in large scale molecular dynamics simulations. Healing of damage under thermal cycling is computed with mechanical properties predicted before and after self--healing.',\n",
       "  'len': 702},\n",
       " {'abstract': 'Motion models play a great role in visual tracking applications for predicting the possible locations of objects in the next frame. Unlike target tracking in radar or aerospace domain which considers only points, object tracking in computer vision involves sizes of objects. Constant velocity motion model is the most widely used motion model for visual tracking, however, there is no clear and understandable derivation involving sizes of objects specially for new researchers joining this research field. In this document, we derive the constant velocity motion model that incorporates sizes of objects that, we think, can help the new researchers to adapt to it very quickly.',\n",
       "  'len': 689},\n",
       " {'abstract': 'This work proposes a global navigation satellite system (GNSS) spoofing detection and classification technique for single antenna receivers. We formulate an optimization problem at the baseband correlator domain by using the Least Absolute Shrinkage and Selection Operator (LASSO). We model correlator tap outputs of the received signal to form a dictionary of triangle-shaped functions and leverage sparse signal processing to choose a decomposition of shifted matching triangles from said dictionary. The optimal solution of this minimization problem discriminates the presence of a potential spoofing attack peak by observing a decomposition of two different code-phase values (authentic and spoofed) in a sparse vector output. We use a threshold to mitigate false alarms. Furthermore, we present a variation of the minimization problem by enhancing the dictionary to a higher-resolution of shifted triangles. The proposed technique can be implemented as an advanced fine-acquisition monitoring tool to aid in the tracking loops for spoofing mitigation. In our experiments, we are able to distinguish authentic and spoofer peaks from synthetic data simulations and from a real dataset, namely, the Texas Spoofing Test Battery (TEXBAT). The proposed method achieves 0.3% detection error rate (DER) for a spoofer attack in nominal signal-to-noise ratio (SNR) conditions for an authentic-over-spoofer power of 3 dB.',\n",
       "  'len': 1426},\n",
       " {'abstract': \"In 2020, DIDO$^©$ turned 20! The software package emerged in 2001 as a basic, user-friendly MATLAB$^\\\\circledR$ teaching-tool to illustrate the various nuances of Pontryagin's Principle but quickly rose to prominence in 2007 after NASA announced it had executed a globally optimal maneuver using DIDO. Since then, the toolbox has grown in applications well beyond its aerospace roots: from solving problems in quantum control to ushering rapid, nonlinear sensitivity-analysis in designing high-performance automobiles. Most recently, it has been used to solve continuous-time traveling-salesman problems. Over the last two decades, DIDO's algorithms have evolved from their simple use of generic nonlinear programming solvers to a multifaceted engagement of fast spectral Hamiltonian programming techniques. A description of the internal enhancements to DIDO that define its mathematics and algorithms are described in this paper. A challenge example problem from robotics is included to showcase how the latest version of DIDO is capable of escaping the trappings of a ``local minimum'' that ensnare many other trajectory optimization methods.\",\n",
       "  'len': 1154},\n",
       " {'abstract': 'It is well-known that disciplines such as mechanical engineering, electrical engineering, civil engineering, aerospace engineering, chemical engineering and software engineering witnessed successful applications of reliability engineering concepts. However, the concept of reliability in its strict sense is missing in financial services. Therefore, in order to fill this gap, in a first-of-its-kind-study, we define the reliability of a bank/firm in terms of the financial ratios connoting the financial health of the bank to withstand the likelihood of insolvency or bankruptcy. For the purpose of estimating the reliability of a bank, we invoke a statistical and machine learning algorithm namely, logistic regression (LR). Once, the parameters are estimated in the 1st stage, we fix them and treat the financial ratios as decision variables. Thus, in the 1st stage, we accomplish the hitherto unknown way of estimating the reliability of a bank. Subsequently, in the 2nd stage, in order to maximize the reliability of the bank, we formulate an unconstrained optimization problem in a single-objective environment and solve it using the well-known particle swarm optimization (PSO) algorithm. Thus, in essence, these two stages correspond to predictive and prescriptive analytics respectively. The proposed 2-stage strategy of using them in tandem is beneficial to the decision-makers within a bank who can try to achieve the optimal or near-optimal values of the financial ratios in order to maximize the reliability which is tantamount to safeguarding their bank against solvency or bankruptcy.',\n",
       "  'len': 1610},\n",
       " {'abstract': \"With the advent of NASA's lunar reconnaissance orbiter (LRO), a large amount of high-resolution digital elevation maps (DEMs) have been constructed by using narrow-angle cameras (NACs) to characterize the Moon's surface. However, NAC DEMs commonly contain no-data gaps (voids), which makes the map less reliable. To resolve the issue, this paper provides a deep-learning-based framework for the probabilistic reconstruction of no-data gaps in NAC DEMs. The framework is built upon a state of the art stochastic process model, attentive neural processes (ANP), and predicts the conditional distribution of elevation on the target coordinates (latitude and longitude) conditioned on the observed elevation data in nearby regions. Furthermore, this paper proposes sparse attentive neural processes (SANPs) that not only reduces the linear computational complexity of the ANP O(N) to the constant complexity O(K) but enhance the reconstruction performance by preventing overfitting and over-smoothing problems. The proposed method is evaluated on the Apollo 17 landing site (20.0°N and 30.4°E), demonstrating that the suggested approach successfully reconstructs no-data gaps with uncertainty analysis while preserving the high resolution of original NAC DEMs.\",\n",
       "  'len': 1267},\n",
       " {'abstract': \"There is currently a drive in the aerospace domain to introduce machine to machine communication over wireless networks to improve ground processes at airports such as refuelling and air conditiong. To this end a session key has to be established between the aircraft and the respective ground unit such as a fuel truck or a pre-conditiong unit. This is to be provided by a `touch and go assistant in the aerospace domain' (TAGA), which allows an operator to pair up a ground unit and an aircraft present at a parking slot with the help of a NFC system. In this paper, we present the results of our security analysis and co-development of requirements, security concepts, and modular verification thereof. We show that by, and only by, a combination of advanced security protocols and local process measures we obtain secure and resilient designs for TAGA. In particular, the design of choice is fully resilient against long-term key compromises and parallel escalation of attacks.\",\n",
       "  'len': 992},\n",
       " {'abstract': \"Exploring and traversing extreme terrain with surface robots is difficult, but highly desirable for many applications, including exploration of planetary surfaces, search and rescue, among others. For these applications, to ensure the robot can predictably locomote, the interaction between the terrain and vehicle, terramechanics, must be incorporated into the model of the robot's locomotion. Modeling terramechanic effects is difficult and may be impossible in situations where the terrain is not known a priori. For these reasons, learning a terramechanics model online is desirable to increase the predictability of the robot's motion. A problem with previous implementations of learning algorithms is that the terramechanics model and corresponding generated control policies are not easily interpretable or extensible. If the models were of interpretable form, designers could use the learned models to inform vehicle and/or control design changes to refine the robot architecture for future applications. This paper explores a new method for learning a terramechanics model and a control policy using a model-based genetic algorithm. The proposed method yields an interpretable model, which can be analyzed using preexisting analysis methods. The paper provides simulation results that show for a practical application, the genetic algorithm performance is approximately equal to the performance of a state-of-the-art neural network approach, which does not provide an easily interpretable model.\",\n",
       "  'len': 1515},\n",
       " {'abstract': 'Occupation measures and linear matrix inequality (LMI) relax-ations (called the moment sums of squares or Lasserre hierarchy) are state-of-the-art methods for verification and validation (VV) in aerospace. In this document, we extend these results to a full F-16 closed-loop nonlinear dutch roll polynomial model complete with model reference adaptive control (MRAC). This is done through a new technique of approximating the reference trajectory by exploiting sparse ordinary differential equations (ODEs) with parsimony. The VV problem is then solved directly using moment LMI relaxations and off-the-shelf-software. The main results are then compared to their numerical counterparts obtained using traditional Monte-Carlo simulations.',\n",
       "  'len': 748},\n",
       " {'abstract': 'In the paper, we present an integrated data-driven modeling framework based on process modeling, material homogenization, mechanistic machine learning, and concurrent multiscale simulation. We are interested in the injection-molded short fiber reinforced composites, which have been identified as key material systems in automotive, aerospace, and electronics industries. The molding process induces spatially varying microstructures across various length scales, while the resulting strongly anisotropic and nonlinear material properties are still challenging to be captured by conventional modeling approaches. To prepare the linear elastic training data for our machine learning tasks, Representative Volume Elements (RVE) with different fiber orientations and volume fractions are generated through stochastic reconstruction. More importantly, we utilize the recently proposed Deep Material Network (DMN) to learn the hidden microscale morphologies from data. With essential physics embedded in its building blocks, this data-driven material model can be extrapolated to predict nonlinear material behaviors efficiently and accurately. Through the transfer learning of DMN, we create a unified process-guided material database that covers a full range of geometric descriptors for short fiber reinforced composites. Finally, this unified DMN database is implemented and coupled with macroscale finite element model to enable concurrent multiscale simulations. From our perspective, the proposed framework is also promising in many other emergent multiscale engineering systems, such as additive manufacturing and compressive molding.',\n",
       "  'len': 1648},\n",
       " {'abstract': 'This paper presents the foundation of a modular robotic system comprised of several novel modules in the shape of a tetrahedron. Four single-propeller submodules are assembled to create the Tetracopter, a tetrahedron-shaped quad-rotorcraft used as the elementary module of a modular flying system. This modular flying system is built by assembling the different elementary modules in a fractal shape. The fractal tetrahedron structure of the modular flying assembly grants the vehicle more rigidity than a conventional two-dimensional modular robotic flight system while maintaining the relative efficiency of a two-dimensional modular robotic flight system. A prototype of the Tetracopter has been modeled, fabricated, and successfully flight-tested by the Decision and Control Laboratory at the Georgia Institute of Technology. The results of this research set the foundation for the development of Tetrahedron rotorcraft that can maintain controllable flight and assemble in flight to create a Fractal Tetrahedron Assembly.',\n",
       "  'len': 1037},\n",
       " {'abstract': 'The utilization of extremely high frequency (EHF) bands can achieve very high throughput in satellite networks (SatNets). Nevertheless, the severe rain attenuation at EHF bands imposes strict limitations on the system availability. Smart gateway diversity (SGD) is considered indispensable in order to guarantee the required availability with reasonable cost. In this context, we examine a load-sharing SGD (LS-SGD) architecture, which has been recently proposed in the literature. For this diversity scheme, we define the system outage probability (SOP) using a rigorous probabilistic analysis based on the Poisson binomial distribution (PBD), and taking into consideration the traffic demand as well as the gateway (GW) capacity. Furthermore, we provide several methods for the exact and approximate calculation of SOP. As concerns the exact computation of SOP, a closed-form expression and an efficient algorithm based on a recursive formula are given, both with quadratic worst-case complexity in the number of GWs. Finally, the proposed approximation methods include well-known probability distributions (binomial, Poisson, normal) and a Chernoff bound. According to the numerical results, binomial and Poisson distributions are by far the most accurate approximation methods.',\n",
       "  'len': 1292},\n",
       " {'abstract': \"Pursuit and evasion conflicts represent challenging problems with important applications in aerospace and robotics. In pursuit-evasion problems, synthesis of intelligent actions must consider the adversary's potential strategies. Differential game theory provides an adequate framework to analyze possible outcomes of the conflict without assuming particular behaviors by the opponent. This article presents an organized introduction of pursuit-evasion differential games with an overview of recent advances in the area. First, a summary of the seminal work is outlined, highlighting important contributions. Next, more recent results are described by employing a classification based on the number of players: one-pursuer-one-evader, N-pursuers-one-evader, one-pursuer-M-evaders, and N-pursuer-M-evader games. In each scenario, a brief summary of the literature is presented. Finally, two representative pursuit-evasion differential games are studied in detail: the two-cutters and fugitive ship differential game and the active target defense differential game. These problems provide two important applications and, more importantly, they give great insight into the realization of cooperation between friendly agents in order to form a team and defeat the adversary.\",\n",
       "  'len': 1281},\n",
       " {'abstract': 'Due to major breakthroughs in software and engineering technologies, embedded systems are increasingly being utilized in areas ranging from aerospace and next-generation transportation systems, to smart grid and smart cities, to health care systems, and broadly speaking to what is known as Cyber-Physical Systems (CPS). A CPS is primarily composed of several electronic, communication and controller modules and some actuators and sensors. The mix of heterogeneous underlying smart technologies poses a number of technical challenges to the design and more severely to the verification of such complex infrastructure. In fact, a CPS shall adhere to strict safety, reliability, performance and security requirements, where one needs to capture both physical and random aspects of the various CPS modules and then analyze their interrelationship across interlinked continuous and discrete dynamics. Often times however, system bugs remain uncaught during the analysis and in turn cause unwanted scenarios that may have serious consequences in safety-critical applications. In this paper, we introduce some of the challenges surrounding the design and verification of contemporary CPS with the advent of smart technologies. In particular, we survey recent developments in the use of theorem proving, a formal method, for the modeling, analysis and verification of CPS, and overview some real world CPS case studies from the automotive, avionics and healthtech domains from system level to physical components.',\n",
       "  'len': 1518},\n",
       " {'abstract': 'Nondestructive evaluation methods play an important role in ensuring component integrity and safety in many industries. Operator fatigue can play a critical role in the reliability of such methods. This is important for inspecting high value assets or assets with a high consequence of failure, such as aerospace and nuclear components. Recent advances in convolution neural networks can support and automate these inspection efforts. This paper proposes using residual neural networks (ResNets) for real-time detection of corrosion, including iron oxide discoloration, pitting and stress corrosion cracking, in dry storage stainless steel canisters housing used nuclear fuel. The proposed approach crops nuclear canister images into smaller tiles, trains a ResNet on these tiles, and classifies images as corroded or intact using the per-image count of tiles predicted as corroded by the ResNet. The results demonstrate that such a deep learning approach allows to detect the locus of corrosion via smaller tiles, and at the same time to infer with high accuracy whether an image comes from a corroded canister. Thereby, the proposed approach holds promise to automate and speed up nuclear fuel canister inspections, to minimize inspection costs, and to partially replace human-conducted onsite inspections, thus reducing radiation doses to personnel.',\n",
       "  'len': 1363},\n",
       " {'abstract': 'Composite materials/structures are advancing in product efficiency, cost-effectiveness and the development of superior specific properties. There are increasing demands in their applications to load-carrying structures in aerospace, wind turbines, transportation, and medical equipment, etc. Thus robust and reliable non-destructive testing (NDT) of composites is essential to reduce safety concerns and maintenance costs. There have been various NDT methods built upon different principles for quality assurance during the whole lifecycle of a composite product. This paper reviews the most established NDT techniques for detection and evaluation of defects/damage evolution in composites. These include acoustic emission, ultrasonic testing, infrared thermography, terahertz testing, shearography, digital image correlation, as well as X-ray and neutron imaging. For each NDT technique, we cover a brief historical background, principles, standard practices, equipment and facilities used for composite research. We also compare and discuss their benefits and limitations, and further summarise their capabilities and applications to composite structures. Each NDT technique has its own potential and rarely achieves a full-scale diagnosis of structural integrity. Future development of NDT techniques for composites will be directed towards intelligent and automated inspection systems with high accuracy and efficient data processing capabilities.',\n",
       "  'len': 1462},\n",
       " {'abstract': 'We examined the solar gravitational lens (SGL) as the means to produce direct high-resolution, multipixel images of exoplanets. The properties of the SGL are remarkable: it offers maximum light amplification of ~1e11 and angular resolution of ~1e-10 arcsec. A probe with a 1-m telescope in the SGL focal region can image an exoplanet at 30 pc with 10-kilometer resolution on its surface, sufficient to observe seasonal changes, oceans, continents, surface topography. We reached and exceeded all objectives set for our study: We developed a new wave-optical approach to study the imaging of exoplanets while treating them as extended, resolved, faint sources at large but finite distances. We properly accounted for the solar corona brightness. We developed deconvolution algorithms and demonstrated the feasibility of high-quality image reconstruction under realistic conditions. We have proven that multipixel imaging and spectroscopy of exoplanets with the SGL are feasible. We have developed a new mission concept that delivers an array of optical telescopes to the SGL focal region relying on three innovations: i) a new way to enable direct exoplanet imaging, ii) use of smallsats solar sails fast transit through the solar system and beyond, iii) an open architecture to take advantage of swarm technology. This approach enables entirely new missions, providing a great leap in capabilities for NASA and the greater aerospace community. Our results are encouraging as they lead to a realistic design for a mission that will be able to make direct resolved images of exoplanets in our stellar neighborhood. It could allow exploration of exoplanets relying on the SGL capabilities decades, if not centuries, earlier than possible with other extant technologies. The architecture and mission concepts for a mission to the strong interference region of the SGL are promising and should be explored further.',\n",
       "  'len': 1920},\n",
       " {'abstract': 'This paper presents an adaptive waveform design method using Multi-Tone Sinusoidal Frequency Modulation (MTSFM). The MTSFM waveform\\'s modulation function is represented as a finite Fourier series expansion. The Fourier coefficients are utilized as a discrete set of design parameters that may be modified to adapt the waveform\\'s properties. The MTSFM\\'s design parameters are adjusted to shape the spectrum, Auto-Correlation Function (ACF), and Ambiguity Function (AF) shapes of the waveform. The MTSFM waveform model naturally possesses the constant envelope and spectrally compact waveforms that make it well suited for transmission on practical radar/sonar transmitters which utilize high power amplifiers. The MTSFM has an exact mathematical definition for its time-series using Generalized Bessel Functions which allow for deriving closed-form analytical expressions for its spectrum, AF, and ACF. These expressions allow for establishing well-defined optimization problems that finely tune the MTSFM\\'s properties. This adaptive waveform design model is demonstrated by optimizing MTSFM waveforms that initially possess a \"thumbtack-like\" AF shape. The resulting optimized designs possess substantially improved sidelobe levels over specified regions in the range-Doppler plane without increasing the Time-Bandwidth Product (TBP) that the initialized waveforms possessed. Simulations additionally demonstrate that the optimized thumbtack-like MTSFM waveforms are competitive with thumbtack-like phase-coded waveforms derived from design algorithms available in the published literature.',\n",
       "  'len': 1601},\n",
       " {'abstract': 'Accurate and efficient aeroelastic models are critically important for enabling the optimization and control of highly flexible aerospace structures, which are expected to become pervasive in future transportation and energy systems. Advanced materials and morphing wing technologies are resulting in next-generation aeroelastic systems that are characterized by highly-coupled and nonlinear interactions between the aerodynamic and structural dynamics. In this work, we leverage emerging data-driven modeling techniques to develop highly accurate and tractable reduced-order aeroelastic models that are valid over a wide range of operating conditions and are suitable for control. In particular, we develop two extensions to the recent dynamic mode decomposition with control (DMDc) algorithm to make it suitable for flexible aeroelastic systems: 1) we introduce a formulation to handle algebraic equations, and 2) we develop an interpolation scheme to smoothly connect several linear DMDc models developed in different operating regimes. Thus, the innovation lies in accurately modeling the nonlinearities of the coupled aerostructural dynamics over multiple operating regimes, not restricting the validity of the model to a narrow region around a linearization point. We demonstrate this approach on a high-fidelity, three-dimensional numerical model of an airborne wind energy (AWE) system, although the methods are generally applicable to any highly coupled aeroelastic system or dynamical system operating over multiple operating regimes. Our proposed modeling framework results in real-time prediction of nonlinear unsteady aeroelastic responses of flexible aerospace structures, and we demonstrate the enhanced model performance for model predictive control. Thus, the proposed architecture may help enable the widespread adoption of next-generation morphing wing technologies.',\n",
       "  'len': 1896},\n",
       " {'abstract': 'The design and experimental implementation of a waveform for high-accuracy inter-node ranging in a coherent distributed antenna array is presented. Based on a spectrally-sparse high-accuracy ranging waveform, the presented multi-frequency waveform enables high-accuracy ranging between multiple nodes in an array simultaneously, without interference. The waveform is based on a unique time-frequency duplexing approach combining a stepped-frequency waveform with different step cycles per node pair. The waveform also inherently includes beneficial disambiguation properties. The ambiguity function of the waveform is derived, and theoretical bounds on the ranging accuracy are obtained. Measurements were conducted in software-defined radio-based nodes in a three-element distributed array, demonstrating high-accuracy unambiguous ranging between two slave nodes and one master node.',\n",
       "  'len': 895},\n",
       " {'abstract': 'We present new developments of the laser-induced transient grating spectroscopy (TGS) technique that enable the measurement of large area 2D maps of thermal diffusivity and surface acoustic wave speed. Additional capabilities include targeted measurements and the ability to accommodate samples with increased surface roughness. These new capabilities are demonstrated by recording large TGS maps of deuterium implanted tungsten, linear friction welded aerospace alloys and high entropy alloys with a range of grain sizes. The results illustrate the ability to view grain microstructure in elastically anisotropic samples, and to detect anomalies in samples, for example due to irradiation and previous measurements. They also point to the possibility of using TGS to quantify grain size at the surface of polycrystalline materials.',\n",
       "  'len': 843},\n",
       " {'abstract': 'Specific strength (strength/density) is a crucial factor while designing high load bearing architecture in areas of aerospace and defence. Strength of the material can be enhanced by blending with high strength component or, by compositing with high strength fillers but both the options has limitations such as at certain load, materials fail due to poor filler and matrix interactions. Therefore, researchers are interested in enhancing strength of materials by playing with topology/geometry and therefore nature is best option to mimic for structures whereas, complexity limits nature mimicked structures. In this paper, we have explored Zeolite-inspired structures for load bearing capacity. Zeolite-inspired structure were obtained from molecular dynamics simulation and then fabricated via Fused deposition Modeling. The atomic scale complex topology from simulation is experimentally synthesized using 3D printing. Compressibility of as-fabricated structures was tested in different direction and compared with simulation results. Such complex architecture can be used for ultralight aerospace and automotive parts.',\n",
       "  'len': 1134},\n",
       " {'abstract': 'Digital twin technology has significant promise, relevance and potential of widespread applicability in various industrial sectors such as aerospace, infrastructure and automotive. However, the adoption of this technology has been slower due to the lack of clarity for specific applications. A discrete damped dynamic system is used in this paper to explore the concept of a digital twin. As digital twins are also expected to exploit data and computational methods, there is a compelling case for the use of surrogate models in this context. Motivated by this synergy, we have explored the possibility of using surrogate models within the digital twin technology. In particular, the use of Gaussian process (GP) emulator within the digital twin technology is explored. GP has the inherent capability of addressing noise and sparse data and hence, makes a compelling case to be used within the digital twin framework. Cases involving stiffness variation and mass variation are considered, individually and jointly along with different levels of noise and sparsity in data. Our numerical simulation results clearly demonstrate that surrogate models such as GP emulators have the potential to be an effective tool for the development of digital twins. Aspects related to data quality and sampling rate are analysed. Key concepts introduced in this paper are summarised and ideas for urgent future research needs are proposed.',\n",
       "  'len': 1434},\n",
       " {'abstract': \"For the thermal control of electronic components in aerospace, automotive or server systems, the heat sink is often located far from the heat sources. Therefore, heat transport systems are necessary to cool the electronic components effectively. Loop heat pipes (LHPs) are such heat transport systems, which use evaporation and condensation to reach a higher heat transfer coefficient than with sole heat conduction. The operating temperature of the LHP governs the temperature of the electronic components, but depends on the amount of dissipated heat and the temperature of the heat sink. For this reason, a control heater on the LHP provides the ability to control the operating temperature at a fixed setpoint temperature. For the model-based control design of the control heater controller, the current LHP state-space model in the literature focuses on the setpoint response without modeling the fluid's dynamics. However, the fluid's dynamics determine the disturbance behavior of the LHP. Therefore, the fluid's dynamics are incorporated into a new LHP state-space model, which is not only able to simulate the LHP behavior under disturbance changes, but is also used for the model-based design of a robust nonlinear controller, which achieves an improved control performance compared to the nonlinear controller based on the previous LHP state-space model.\",\n",
       "  'len': 1376},\n",
       " {'abstract': \"Software defined radio is a widely accepted paradigm for design of reconfigurable modems. The continuing march of Moore's law makes real-time signal processing on general purpose processors feasible for a large set of waveforms. Data rates in the low Mbps can be processed on low-power ARM processors, and much higher data rates can be supported on large x86 processors. The advantages of all-software development (vs. FPGA/DSP/GPU) are compelling: much wider pool of talent, lower development time and cost, and easier maintenance and porting. However, very high-rate systems (above 100 Mbps) are still firmly in the domain of custom and semi-custom hardware (mostly FPGAs). In this paper we describe an architecture and testbed for an SDR that can be easily scaled to support over 3 GHz of bandwidth and data rate up to 10 Gbps. The paper covers a novel technique to parallelize typically serial algorithms for phase and symbol tracking, followed by a discussion of data distribution for a massively parallel architecture. We provide a brief description of a mixed-signal front end and conclude with measurement results. To the best of the author's knowledge, the system described in this paper is an order of magnitude faster than any prior published result.\",\n",
       "  'len': 1272},\n",
       " {'abstract': 'A two-stage batch estimation algorithm for solving a class of nonlinear, static parameter estimation problems that appear in aerospace engineering applications is proposed. It is shown how these problems can be recast into a form suitable for the proposed two-stage estimation process. In the first stage, linear least squares is used to obtain a subset of the unknown parameters (set 1), while a residual sampling procedure is used for selecting initial values for the rest of the parameters (set 2). In the second stage, depending on the uniqueness of the local minimum, either only the parameters in the second set need to be re-estimated, or all the parameters will have to be re-estimated simultaneously, by a nonlinear constrained optimization. The estimates from the first stage are used as initial conditions for the second stage optimizer. It is shown that this approach alleviates the sensitivity to initial conditions and minimizes the likelihood of converging to an incorrect local minimum of the nonlinear cost function. An error bound analysis is presented to show that the first stage can be solved in such a way that the total cost function will be driven to the optimal cost, and the difference has an upper bound. Two tutorial examples are used to show how to implement this estimator and compare its performance to other similar nonlinear estimators. Finally, the estimator is used on a 5-hole Pitot tube calibration problem using flight test data collected from a small Unmanned Aerial Vehicle (UAV) which cannot be easily solved with single-stage methods.',\n",
       "  'len': 1587},\n",
       " {'abstract': 'In this brief paper, a new consensus protocol based on the sign of innovations is proposed. Based on this protocol each agent only requires single-bit of information about its relative state to its neighboring agents. This is significant in real-time applications, since it requires less computation and/or communication load on agents. Using Lyapunov stability theorem the convergence is proved for networks having a spanning tree. Further, the convergence is shown to be in finite-time, which is significant as compared to most asymptotic protocols in the literature. Time-variant network topologies are also considered in this paper, and final consensus value is derived for undirected networks. Applications of the proposed consensus protocol in (i) 2D/3D rendezvous task, (ii) distributed estimation, (iii) distributed optimization, and (iv) formation control are considered and significance of applying this protocol is discussed. Numerical simulations are provided to compare the protocol with the existing protocols in the literature.',\n",
       "  'len': 1053},\n",
       " {'abstract': 'Multifunctional lattice materials are of great interest to modern engineering including aerospace, robotics, wave control, and sensing. In this work, we proposed and developed a new class of functional lattice materials called liquid metal lattice materials, which are composed of liquid metals and elastomer coatings organized in a co-axial way. This hybrid design enables the liquid metal lattice materials with a thermal-activated shape memory effect and hence a variety of intriguing functionalities including recoverable energy absorption, tunable shape and rigidity, deployable and reconfigurable behaviors. The fabrication of liquid metal lattice materials is realised by a hybrid manufacturing approach combining 3D printing, vacuum casting, and coating. The mechanical properties and functionalities of these liquid metal lattice materials are studied via experimental testing. We expect that this new class of lattice materials will greatly expand the current realm of lattice materials and lead to novel multifunctional applications.',\n",
       "  'len': 1055},\n",
       " {'abstract': 'In this paper, we propose an approach to formulate the Weapon Target Assignment (WTA) problem with physical and seeker interference constraints which is solvable in Mixed Integer Linear Programming (MILP). To handle the interference constraint which is intractable in continuous time domain, we discretize the time window and generate predicted intercept point (PIP) set. Also, to handle interference constraints to the MILP formulation, the interference tables that explain which pairs of assignments make interference, are made prematurely before the solver executed. The simulation results whether considering interference or not give different optimal solutions with different targets show the effectiveness of our MILP formulation.',\n",
       "  'len': 747},\n",
       " {'abstract': 'The spatiotemporal information plays crucial roles in a multi-agent system (MAS). However, for a highly dynamic and dense MAS in unknown environments, estimating its spatiotemporal states is a difficult problem. In this paper, we present BLAS: a wireless broadcast relative localization and clock synchronization system to address these challenges. Our BLAS system exploits a broadcast architecture, under which a MAS is categorized into parent agents that broadcast wireless packets and child agents that are passive receivers, to reduce the number of required packets among agents for relative localization and clock synchronization. We first propose an asynchronous broadcasting and passively receiving (ABPR) protocol. The protocol schedules the broadcast of parent agents using a distributed time division multiple access (D-TDMA) scheme and delivers inter-agent information used for joint relative localization and clock synchronization. We then present distributed state estimation approaches in parent and child agents that utilize the broadcast inter-agent information for joint estimation of spatiotemporal states. The simulations and real-world experiments based on ultra-wideband (UWB) illustrate that our proposed BLAS cannot only enable accurate, high-frequency and real-time estimation of relative position and clock parameters but also support theoretically an unlimited number of agents.',\n",
       "  'len': 1415},\n",
       " {'abstract': 'The Active Monitor Box of Electrostatic Risks (AMBER) is a double-head thermal electron and ion electrostatic analyzer (energy range 0-30 keV) that was launched onboard the Jason-3 spacecraft in 2016. The next generation AMBER instrument, for which a first prototype was developed and then calibrated at the end of 2017, constitutes a significant evolution that is based on a single head to measure both species alternatively. The instrument developments focused on several new subsystems (front-end electronics, high-voltage electronics, mechanical design) that permit to reduce instrument resources down to ~ 1 kg and 1.5 W. AMBER is designed as a generic radiation monitor with a twofold purpose: (1) measure magnetospheric thermal ion and electron populations in the range 0-35 keV, with significant scientific potential (e.g., plasmasphere, ring current, plasma sheet), and (2) monitor spacecraft electrostatic charging and the plasma populations responsible for it, for electromagnetic cleanliness and operational purposes.',\n",
       "  'len': 1040},\n",
       " {'abstract': 'Fluid flow in the transonic regime finds relevance in aerospace engineering, particularly in the design of commercial air transportation vehicles. Computational fluid dynamics models of transonic flow for aerospace applications are computationally expensive to solve because of the high degrees of freedom as well as the coupled nature of the conservation laws. While these issues pose a bottleneck for the use of such models in aerospace design, computational costs can be significantly minimized by constructing special, structure-preserving surrogate models called reduced-order models. Such models are known to incur huge off-line costs, however, which can sometimes outweigh their potential benefits. Furthermore, their prediction accuracy is known to be poor under transonic flow conditions. In this work, we propose a machine learning method to construct reduced-order models via deep neural networks, and we demonstrate its ability to preserve accuracy with significantly lower offline and online costs. In addition, our machine learning methodology is physics-informed and constrained through the utilization of an interpretable encoding by way of proper orthogonal decomposition. Application to the inviscid transonic flow past the RAE2822 airfoil under varying freestream Mach numbers and angles of attack, as well as airfoil shape parameters with a deforming mesh, shows that the proposed approach adapts to high-dimensional parameter variation well. Notably, the proposed framework precludes knowledge of numerical operators utilized in the data generation phase, thereby demonstrating its potential utility in fast exploration of design space for diverse engineering applications.',\n",
       "  'len': 1705},\n",
       " {'abstract': 'Programmable Logic Controllers are an integral component for managing many different industrial processes (e.g., smart building management, power generation, water and wastewater management, and traffic control systems), and manufacturing and control industries (e.g., oil and natural gas, chemical, pharmaceutical, pulp and paper, food and beverage, automotive, and aerospace). Despite being used widely in many critical infrastructures, PLCs use protocols which make these control systems vulnerable to many common attacks, including man-in-the-middle attacks, denial of service attacks, and memory corruption attacks (e.g., array, stack, and heap overflows, integer overflows, and pointer corruption). In this paper, we propose PLC-PROV, a system for tracking the inputs and outputs of the control system to detect violations in the safety and security policies of the system. We consider a smart building as an example of a PLC-based system and show how PLC-PROV can be applied to ensure that the inputs and outputs are consistent with the intended safety and security policies.',\n",
       "  'len': 1093},\n",
       " {'abstract': 'Explicit quantification of uncertainty in engineering simulations is being increasingly used to inform robust and reliable design practices. In the aerospace industry, computationally-feasible analyses for design optimization purposes often introduce significant uncertainties due to deficiencies in the mathematical models employed. In this paper, we discuss two recent improvements in the quantification and combination of uncertainties from multiple sources that can help generate probabilistic aerodynamic databases for use in aerospace engineering problems. We first discuss the eigenspace perturbation methodology to estimate model-form uncertainties stemming from inadequacies in the turbulence models used in Reynolds-Averaged Navier-Stokes Computational Fluid Dynamics (RANS CFD) simulations. We then present a multi-fidelity Gaussian Process framework that can incorporate noisy observations to generate integrated surrogate models that provide mean as well as variance information for Quantities of Interest (QoIs). The process noise is varied spatially across the domain and across fidelity levels. Both these methodologies are demonstrated through their application to a full configuration aircraft example, the NASA Common Research Model (CRM) in transonic conditions. First, model-form uncertainties associated with RANS CFD simulations are estimated. Then, data from different sources is used to generate multi-fidelity probabilistic aerodynamic databases for the NASA CRM. We discuss the transformative effect that affordable and early treatment of uncertainties can have in traditional aerospace engineering practices. The results are presented and compared to those from a Gaussian Process regression performed on a single data source.',\n",
       "  'len': 1765},\n",
       " {'abstract': 'Computational fluid dynamics and aerodynamics, which complement more expensive empirical approaches, are critical for developing aerospace vehicles. During the past three decades, computational aerodynamics capability has improved remarkably, following advances in computer hardware and algorithm development. However, for complex applications, the demands on computational fluid dynamics continue to increase in a quest to gain a few percent improvements in accuracy. Herein, we numerically demonstrate that optimizing the metric terms which arise from smoothly mapping each cell to a reference element, lead to a solution whose accuracy is practically never worse and often noticeably better than the one obtained using the widely adopted Thomas and Lombard metric terms computation (Geometric conservation law and its application to flow computations on moving grids, AIAA Journal, 1979). Low and high-order accurate entropy stable schemes on distorted, high-order tensor product elements are used to simulate three-dimensional inviscid and viscous compressible test cases for which an analytical solution is known.',\n",
       "  'len': 1129},\n",
       " {'abstract': 'Determining thermal and physical quantities across a broad temperature domain, especially up to the ultra-high temperature region, is a formidable theoretical and experimental challenge. At the same time it is essential for understanding the performance of ultra-high temperature ceramic (UHTC) materials. Here we present the development of a machine-learning force field for ZrB2, one of the primary members of the UHTC family with a complex bonding structure. The force field exhibits chemistry accuracy for both energies and forces and can reproduce structural, elastic and phonon properties, including thermal expansion and thermal transport. A thorough comparison with available empirical potentials shows that our force field outperforms the competitors. Most importantly, its effectiveness is extended from room temperature to the ultra-high temperature region (up to ~ 2,500 K), where measurements are very difficult, costly and some time impossible. Our work demonstrates that machine-learning force fields can be used for simulations of materials in a harsh environment, where no experimental tools are available, but crucial for a number of engineering applications, such as in aerospace, aviation and nuclear.',\n",
       "  'len': 1232},\n",
       " {'abstract': 'We consider the fusion of two aerodynamic data sets originating from differing fidelity physical or computer experiments. We specifically address the fusion of: 1) noisy and in-complete fields from wind tunnel measurements and 2) deterministic but biased fields from numerical simulations. These two data sources are fused in order to estimate the \\\\emph{true} field that best matches measured quantities that serves as the ground truth. For example, two sources of pressure fields about an aircraft are fused based on measured forces and moments from a wind-tunnel experiment. A fundamental challenge in this problem is that the true field is unknown and can not be estimated with 100\\\\% certainty. We employ a Bayesian framework to infer the true fields conditioned on measured quantities of interest; essentially we perform a \\\\emph{statistical correction} to the data. The fused data may then be used to construct more accurate surrogate models suitable for early stages of aerospace design. We also introduce an extension of the Proper Orthogonal Decomposition with constraints to solve the same problem. Both methods are demonstrated on fusing the pressure distributions for flow past the RAE2822 airfoil and the Common Research Model wing at transonic conditions. Comparison of both methods reveal that the Bayesian method is more robust when data is scarce while capable of also accounting for uncertainties in the data. Furthermore, given adequate data, the POD based and Bayesian approaches lead to \\\\emph{similar} results.',\n",
       "  'len': 1540},\n",
       " {'abstract': 'Reliable pose estimation of uncooperative satellites is a key technology for enabling future on-orbit servicing and debris removal missions. The Kelvins Satellite Pose Estimation Challenge aims at evaluating and comparing monocular vision-based approaches and pushing the state-of-the-art on this problem. This work is based on the Satellite Pose Estimation Dataset, the first publicly available machine learning set of synthetic and real spacecraft imageries. The choice of dataset reflects one of the unique challenges associated with spaceborne computer vision tasks, namely the lack of spaceborne images to train and validate the developed algorithms. This work briefly reviews the basic properties and the collection process of the dataset which was made publicly available. The competition design, including the definition of performance metrics and the adopted testbed, is also discussed. The main contribution of this paper is the analysis of the submissions of the 48 competitors, which compares the performance of different approaches and uncovers what factors make the satellite pose estimation problem especially challenging.',\n",
       "  'len': 1148},\n",
       " {'abstract': 'As unmanned aerial systems (UASs) increasingly integrate into the US national airspace system, there is an increasing need to characterize how commercial and recreational UASs may encounter each other. To inform the development and evaluation of safety critical technologies, we demonstrate a methodology to analytically calculate all potential relative geometries between different UAS operations performing inspection missions. This method is based on a previously demonstrated technique that leverages open source geospatial information to generate representative unmanned aircraft trajectories. Using open source data and parallel processing techniques, we performed trillions of calculations to estimate the relative horizontal distance between geospatial points across sixteen locations.',\n",
       "  'len': 804},\n",
       " {'abstract': 'Convolutional Neural Networks (CNN) are being actively explored for safety-critical applications such as autonomous vehicles and aerospace, where it is essential to ensure the reliability of inference results in the presence of possible memory faults. Traditional methods such as error correction codes (ECC) and Triple Modular Redundancy (TMR) are CNN-oblivious and incur substantial memory overhead and energy cost. This paper introduces in-place zero-space ECC assisted with a new training scheme weight distribution-oriented training. The new method provides the first known zero space cost memory protection for CNNs without compromising the reliability offered by traditional ECC.',\n",
       "  'len': 697},\n",
       " {'abstract': 'We introduce the LARES 2 space experiment recently approved by the Italian Space Agency (ASI). The LARES 2 satellite is planned for launch in 2019 with the new VEGA C launch vehicle of ASI, ESA and ELV. The orbital analysis of LARES 2 experiment will be carried out by our international science team of experts in General Relativity, theoretical physics, space geodesy and aerospace engineering. The main objectives of the LARES 2 experiment are gravitational and fundamental physics, including accurate measurements of General Relativity, in particular a test of frame-dragging aimed at achieving an accuracy of a few parts in a thousand, i.e., aimed at improving by about an order of magnitude the present state-of-the-art and forthcoming tests of this general relativistic phenomenon. LARES 2 will also achieve determinations in space geodesy. LARES 2 is an improved version of the LAGEOS 3 experiment, proposed in 1984 to measure frame-dragging and analyzed in 1989 by a joint ASI and NASA study.',\n",
       "  'len': 1011},\n",
       " {'abstract': 'Data-flow reactive systems (DFRSs) are a class of embedded systems whose inputs and outputs are always available as signals. Input signals can be seen as data provided by sensors, whereas the output data are provided to system actuators. In previous works, verifying properties of DFRS models was accomplished in a programmatic way, with no formal guarantees, and test cases were generated by translating theses models into other notations. Here, we use Coq as a single framework to specify and verify DFRS models. Moreover, the specification of DFRSs in Coq is automatically derived from controlled natural-language requirements. Property verification is defined in both logical and functional terms. The latter allows for easier proof construction. Tests are generated with the support of the QuickChick tool. Considering examples from the literature, but also from the aerospace industry (Embraer), our testing strategy was evaluated in terms of performance and the ability to detect defects generated by mutation; within 8 seconds, we achieved an average mutation score of 75.80%.',\n",
       "  'len': 1095},\n",
       " {'abstract': 'Blockchain is a popular method to ensure security for trusted systems. The benefits include an auditable method to provide decentralized security without a trusted third party, but the drawback is the large computational resources needed to process and store the ever-expanding chain of security blocks. The promise of blockchain for edge devices (e.g., internet of things) poses a variety of challenges and strategies before adoption. In this paper, we explore blockchain methods and examples, with experimental data to determine the merits of the capabilities. As for an aerospace example, we address a notional example for Automatic dependent surveillance-broadcast (ADS-B) from Flight24 data (this https URL) to determine whether blockchain is feasible for avionics systems. The methods are incorporated into the Lightweight Internet of Things (IoT) based Smart Public Safety (LISPS) framework. By decoupling a complex system into independent sub-tasks, the LISPS system possesses high flexibility in the design process and online maintenance. The Blockchain-enabled decentralized avionics services provide a secured data sharing and access control mechanism. The experimental results demonstrate the feasibility of the approach.',\n",
       "  'len': 1244},\n",
       " {'abstract': \"One of today's most propitious immersive technologies is virtual reality (VR). This term is colloquially associated with headsets that transport users to a bespoke, built-for-purpose immersive 3D virtual environment. It has given rise to the field of immersive analytics---a new field of research that aims to use immersive technologies for enhancing and empowering data analytics. However, in developing such a new set of tools, one has to ask whether the move from standard hardware setup to a fully immersive 3D environment is justified---both in terms of efficiency and development costs. To this end, in this paper, we present the AeroVR--an immersive aerospace design environment with the objective of aiding the component aerodynamic design process by interactively visualizing performance and geometry. We decompose the design of such an environment into function structures, identify the primary and secondary tasks, present an implementation of the system, and verify the interface in terms of usability and expressiveness. We deploy AeroVR on a prototypical design study of a compressor blade for an engine.\",\n",
       "  'len': 1129},\n",
       " {'abstract': 'We study a scenario where a group of agents, each with multiple heterogeneous sensors are collecting measurements of a vehicle and the measurements are transmitted over a communication channel to a centralized node for processing. The communication channel presents an information-transfer bottleneck as the sensors collect measurements at a much higher rate than what is feasible to transmit over the communication channel. In order to minimize the estimation error at the centralized node, only a carefully selected subset of measurements should be transmitted. We propose to select measurements based on the Fisher information matrix (FIM), as \"minimizing\" the inverse of the FIM is required to achieve small estimation error. Selecting measurements based on the FIM leads to a combinatorial optimization problem. However, when the criteria used to select measurements is both monotone and submodular it allows the use of a greedy algorithm that is guaranteed to be within $1-1/e\\\\approx63\\\\%$ of the optimum and has the critical benefit of quadratic computational complexity. To illustrate this concept, we derive the FIM criterion for different sensor types to which we apply FIM-based measurement selection. The criteria considered include the time-of-arrival and Doppler shift of passively received radio transmissions as well as detected key-points in camera images.',\n",
       "  'len': 1383},\n",
       " {'abstract': 'This paper presents a semi-Markov decision process (SMDP) formulation of the satellite task scheduling problem. This formulation can consider multiple operational objectives simultaneously and plan transitions between distinct functional modes. We consider the problem of scheduling image collections, ground contacts, sun-pointed periods for battery recharging, and data recorder management for an agile, resource-constrained Earth-observing spacecraft. By considering multiple mission objectives simultaneously, the algorithm is able to find optimized task schedule that satisfies all operational constraints in a single planning step, thus reducing the operational complexity and number of steps involved in mission planning. We present two solution approaches based on forward search and Monte Carlo Tree search. We baseline against rule-based, graph search, and mixed-integer-linear programming approaches. The SMDP formulation is evaluated in both single-objective and multi-objective scenarios. The SMDP solution is found to perform comparably with the baseline methods at greatly increased speed in single-objective scenarios and greater schedule reward in multi-objective.',\n",
       "  'len': 1192},\n",
       " {'abstract': 'The behavior of a cyber-physical system (CPS) is usually defined in terms of the input and output signals processed by sensors and actuators. Requirements specifications of CPSs are typically expressed using signal-based temporal properties. Expressing such requirements is challenging, because of (1) the many features that can be used to characterize a signal behavior; (2) the broad variation in expressiveness of the specification languages (i.e., temporal logics) used for defining signal-based temporal properties. Thus, system and software engineers need effective guidance on selecting appropriate signal behavior types and an adequate specification language, based on the type of requirements they have to define. In this paper, we present a taxonomy of the various types of signal-based properties and provide, for each type, a comprehensive and detailed description as well as a formalization in a temporal logic. Furthermore, we review the expressiveness of state-of-the-art signal-based temporal logics in terms of the property types identified in the taxonomy. Moreover, we report on the application of our taxonomy to classify the requirements specifications of an industrial case study in the aerospace domain, in order to assess the feasibility of using the property types included in our taxonomy and the completeness of the latter.',\n",
       "  'len': 1361},\n",
       " {'abstract': 'Among the mitigation measures introduced to cope with the space debris issue there is the de-orbiting of decommissioned satellites. Guidelines for re-entering objects call for a ground casualty risk no higher than 0.0001. To comply with this requirement, satellites can be designed through a design-for-demise philosophy. Still, a spacecraft designed to demise has to survive the debris-populated space environment for many years. The demisability and the survivability of a satellite can both be influenced by a set of common design choices such as the material selection, the geometry definition, and the position of the components. Within this context, two models have been developed to analyse the demise and the survivability of satellites. Given the competing nature of the demisability and the survivability, a multi-objective optimisation framework was developed, with the aim to identify trade-off solutions for the preliminary design of satellites. As the problem is nonlinear and involves the combination of continuous and discrete variables, classical derivative based approaches are unsuited and a genetic algorithm was selected instead. The genetic algorithm uses the developed demisability and survivability criteria as the fitness functions of the multi-objective algorithm. The paper presents a test case, which considers the preliminary optimisation of tanks in terms of material, geometry, location, and number of tanks for a representative Earth observation mission. The configuration of the external structure of the spacecraft is fixed. Tanks were selected because they are sensitive to both design requirements: they represent critical components in the demise process and impact damage can cause the loss of the mission because of leaking and ruptures. The results present the possible trade off solutions, constituting the Pareto front obtained from the multi-objective optimisation.',\n",
       "  'len': 1919},\n",
       " {'abstract': 'In this paper, we present an adaptive algorithm to construct response surface approximations of high-fidelity models using a hierarchy of lower fidelity models. Our algorithm is based on multi-index stochastic collocation and automatically balances physical discretization error and response surface error to construct an approximation of model outputs. This surrogate can be used for uncertainty quantification (UQ) and sensitivity analysis (SA) at a fraction of the cost of a purely high-fidelity approach. We demonstrate the effectiveness of our algorithm on a canonical test problem from the UQ literature and a complex multi-physics model that simulates the performance of an integrated nozzle for an unmanned aerospace vehicle. We find that, when the input-output response is sufficiently smooth, our algorithm produces approximations that can be over two orders of magnitude more accurate than single fidelity approximations for a fixed computational budget.',\n",
       "  'len': 976},\n",
       " {'abstract': 'The hand-eye measurements have recently been proven to be very efficient for spacecraft attitude determination relative to an ellipsoidal asteroid. However, recent method does not guarantee full attitude observability for all conditions. This paper refines this problem by taking the vector observations into account so that the accuracy and robustness of the spacecraft attitude estimation can be improved. The vector observations come from many sources including visual perspective geometry, optical navigation and point clouds that frequently occur in aerospace electronic systems. Completely closed-form solutions along with their uncertainty descriptions are presented for the proposed problem. Experiments using our simulated dataset and real-world spacecraft measurements from NASA dawn spacecraft verify the effectiveness and superiority of the derived solution.',\n",
       "  'len': 881},\n",
       " {'abstract': 'Advances in the field of Machine Learning and Deep Neural Networks (DNNs) has enabled rapid development of sophisticated and autonomous systems. However, the inherent complexity to rigorously assure the safe operation of such systems hinders their real-world adoption in safety-critical domains such as aerospace and medical devices. Hence, there is a surge in interest to explore the use of advanced mathematical techniques such as formal methods to address this challenge. In fact, the initial results of such efforts are promising. Along these lines, we propose the use of quantifier elimination (QE) - a formal method technique, as a complimentary technique to the state-of-the-art static analysis and verification procedures. Using an airborne collision avoidance DNN as a case example, we illustrate the use of QE to formulate the precise range forward propagation through a network as well as analyze its robustness. We discuss the initial results of this ongoing work and explore the future possibilities of extending this approach and/or integrating it with other approaches to perform advanced safety assurance of DNNs.',\n",
       "  'len': 1140},\n",
       " {'abstract': 'This paper introduces two new algorithms to accurately estimate the process noise covariance of a discrete-time Kalman filter online for robust orbit determination in the presence of dynamics model uncertainties. Common orbit determination process noise techniques, such as state noise compensation and dynamic model compensation, require offline tuning and a priori knowledge of the dynamical environment. Alternatively, the process noise covariance can be estimated through adaptive filtering. However, many adaptive filtering techniques are not applicable to onboard orbit determination due to computational cost or the assumption of a linear time-invariant system. Furthermore, existing adaptive filtering techniques do not constrain the process noise covariance according to the underlying continuous-time dynamical model, and there has been limited work on adaptive filtering with colored process noise. To overcome these limitations, a novel approach is developed which optimally fuses state noise compensation and dynamic model compensation with covariance matching adaptive filtering. This yields two adaptive and dynamically constrained process noise covariance estimation techniques. Unlike many adaptive filtering approaches, the new techniques accurately extrapolate over measurement outages and do not rely on ad hoc methods to ensure the process noise covariance is positive semi-definite. The benefits of the proposed algorithms are demonstrated through two case studies: an illustrative linear system and the autonomous navigation of two spacecraft orbiting an asteroid.',\n",
       "  'len': 1598},\n",
       " {'abstract': 'Much digital instrumentation and control systems embedded in the critical medical healthcare equipment aerospace devices and nuclear industry have obvious consequence of different failure modes. These failures can affect the behavior of the overall safety critical digital system and its ability to deliver its dependability attributes if any defected area that could be a hardware component or software code embedded inside the digital system is not detected and repaired appropriately. The safety and reliability analysis of safety critical systems can be accomplished with Markov modeling techniques which could express the dynamic and regenerative behavior of the digital control system. Certain states in the system represent system failure while others represent fault free behavior or correct operation in the presence of faults. This paper presents the development of a safety and reliability modeling of a digital feedwater control system using Markov based chain models. All the Markov states and the transitions between these states were assumed and calculated from the control logic for the digital control system. Finally based on the simulation results of modeling the digital feedwater control system the system does meet its reliability requirement with the probability of being in fully operational states is 0.99 over a 6 months time.',\n",
       "  'len': 1363},\n",
       " {'abstract': 'Energy absorbing materials, like foams used in protective equipment, are able to undergo large deformations under low stresses, reducing the incoming stress wave below an injury or damage threshold. They are typically effective in absorbing energy through plastic deformation or fragmentation. However, existing solutions are passive, only effective against specific threats and they are usually damaged after use. Here, we overcome these limitations designing energy absorbing materials that use architected lattices filled with granular particles. We use architected lattices to take advantage of controlled bending and buckling of members to enhance energy absorption. We actively control the negative pressure level within the lattices, to tune the jamming phase transition of the granular particles, inducing controllable energy absorption and recoverable deformations. Our system shows tunable stiffness and yield strength by over an order of magnitude, and reduces the transmitted impact stress at different levels by up to 40% compared to the passive lattice. The demonstrated adaptive energy absorbing system sees wide potential applications from personal protective equipment, vehicle safety systems to aerospace structures.',\n",
       "  'len': 1245},\n",
       " {'abstract': \"The article dwells upon the Earth remote sensing data as one of the basic directions of Geo-Information Science, a unique source of information on processes and phenomena occurring in almost all spheres of the Earth geographic shell (atmosphere, hydrosphere, lithosphere, etc.). The authors argue that the use of aerospace images by means of the information and communication technologies involvement in the learning process allows not only to increase the information context value of learning, but also contributes to the formation of students' cognitive interest in such disciplines as geography, biology, history, physics, computer science, etc. It has been grounded that remote sensing data form students' spatial, temporal and qualitative concepts, sensory support for the perception, knowledge and explanation of the specifics of objects and phenomena of geographical reality, which, in its turn, provides an increase in the level of educational achievements. The techniques of aerospace images application into the modern school practice have been analyzed and illustrated in the examples: from using them as visual aids, to realization of practical and research orientation of training on the basis of remote sensing data. Particular attention is paid to the practical component of the Earth remote sensing implementation into the modern school practice with the help of information and communication technologies.\",\n",
       "  'len': 1434},\n",
       " {'abstract': 'Engineering Thermodynamics has been the core course of many science and engineering majors at home and abroad, including energy and power, mechanical engineering, civil engineering, aerospace, cryogenic refrigeration, food engineering, chemical engineering, and environmental engineering, among which gas power cycle is one of the important contents. However, many Engineering Thermodynamics textbooks at home and abroad focus only on evaluating the thermal efficiency of gas power cycle, while the important concept of specific cycle net work is ignored. Taking an ideal Otto cycle and an ideal Brayton as examples, the optimum compression ratio (or the pressure ratio) and the maximum specific cycle net work are analyzed and determined. The ideal Otto and the ideal Brayton cycles, and also other gas power cycles, are concluded that the operation under the optimum compression/pressure ratio of the engine, instead of under the higher efficiency, is more economic and more reasonable. We concluded that the two very important concepts, i.e., the maximum specific cycle net work and the optimum compression (or pressure) ratio for the gas power cycles, should be emphasized in the Engineering Thermodynamics teaching process and the latter revised or the newly edited textbooks, in order to better guide the engineering applications. In the end, general T-s diagram is proposed for the gas power cycles.',\n",
       "  'len': 1417},\n",
       " {'abstract': \"High altitude balloons (HABs) are typically tracked via GPS data sent via real-time radio-based communication systems such as the Automated Packet Reporting System (APRS). Prefabricated APRS-compatible tracker modules have made it trivial to transmit GPS coordinates and payload parameters in compliance with the requisite AX.25 protocol. However, in order to receive and track APRS signals, conventional methodologies call for the use of a Very High Frequency (VHF) receiver to demodulate signals transmitted on the 440/144 MHz APRS frequencies, along with a compatible antenna and custom methodology for visualizing the HAB's location on a map. The entire assembly is typically costly, cumbersome, and may require an internet connection in order to obtain real-time visualization of the HAB's location. This paper describes a low-cost, handheld system based on open-source software that operates independently of an internet connection. The miniaturized system is suited to tracking done either from a vehicle or on foot, and is cost-effective enough to be within the means of nearly any HAB user. The paper also discusses preliminary test results and further applications.\",\n",
       "  'len': 1186},\n",
       " {'abstract': 'The prevalence of bacteria in the atmosphere has been well established in relevant literature, suggesting that airborne bacteria can influence atmospheric characteristics including the development of clouds. Studies have also demonstrated that the atmospheric biological profile is influenced by the underlying terrestrial biomes. An understanding of the complex interplay of factors that can influence the atmospheric biological profile, not to mention developing a biological census of the atmosphere, requires a cost-effective experimental system capable of generating reproducible results with reliable data. However, as has been demonstrated by payloads launched by space agencies such as NASA and JAXA, these payloads are both complex and cost prohibitive. This paper discusses the design and implementation of a biologically oriented experimental payload for high-altitude ballooning that is within the means of most student-run experimental programs. The payload highlighted in this presentation, PHANTOM (Probe for High Altitude Numeration and Tracking of Microorganisms, which has the goal of capturing aerial microorganisms at multiple altitudes in order to characterize the biological composition of the upper atmosphere), has undergone a number of successful flight trials, and serves to highlight the feasibility and utility of interdisciplinary projects between aerospace and the biological sciences.',\n",
       "  'len': 1426},\n",
       " {'abstract': 'The requirement to generate robust robotic platforms is a critical enabling step to allow such platforms to permeate safety-critical applications (i.e., the localization of autonomous platforms in urban environments). One of the primary components of such a robotic platform is the state estimation engine, which enables the platform to reason about itself and the environment based upon sensor readings. When such sensor readings are degraded traditional state estimation approaches are known to breakdown. To overcome this issue, several robust state estimation frameworks have been proposed. One such method is the batch covariance estimation (BCE) framework. The BCE approach enables robust state estimation by iteratively updating the measurement error uncertainty model through the fitting of a Gaussian mixture model (GMM) to the measurement residuals. This paper extends upon the BCE approach by arguing that the uncertainty estimation process should be augmented to include metadata (e.g., the signal strength of the associated GNSS observation). The modification of the uncertainty estimation process to an augmented data space is significant because it increases the likelihood of a unique partitioning in the measurement residual domain and thus provides the ability to more accurately characterize the measurement uncertainty model. The proposed batch covariance estimation over an augmented data-space (BCE-AD) is experimentally validated on collected data where it is shown that a significant increase in state estimation accuracy can be granted compared to previously proposed robust estimation techniques.',\n",
       "  'len': 1633},\n",
       " {'abstract': \"We present RCE (Remote Component Environment), an open-source framework developed primarily at DLR (German Aerospace Center) that enables its users to construct and execute multidisciplinary engineering workflows comprising multiple disciplinary tools. To this end, RCE supplies users with an easy-to-use graphical interface that allows for the intuitive integration of disciplinary tools. Users can execute the individual tools on arbitrary nodes present in the network and all data accrued during the execution of the workflow are collected and stored centrally. Hence, RCE makes it easy for collaborating engineers to contribute their individual disciplinary tools to a multidisciplinary design or analysis, and simplifies the subsequent analysis of the workflow's results.\",\n",
       "  'len': 787},\n",
       " {'abstract': 'Combustion in automotive and aerospace applications employing diesel, gas turbine and liquid rocket engines is preceded by injection and mixing of fuel and oxidizer at high pressures, often exceeding mixture critical values. Experimental observations indicate that the jets injected at supercritical pressures exhibit significantly different dynamics than the jets at subcritical conditions, owing to the lack of distinct liquid and gas phases in supercritical state. As a result, the averaged flow quantities such as the potential core length, jet spatial growth rate and velocity decay profiles differ in the two conditions, resulting in different mixed-fluid distributions. In this study, turbulent jet direct numerical simulations (DNS) are performed to examine the variations in statistics between injection of Nitrogen ($\\\\mathrm{N_{2}}$) in Nitrogen ($\\\\mathrm{N_{2}}$) at subcritical (perfect-gas) and supercritical conditions. Isothermal round jets at Reynolds number ($Re_{D}$), based on jet diameter ($D$) and jet orifice velocity ($U_{0}$), of $5000$ and Mach number of $0.6$ are considered. For mixing analyses, a passive scalar transported with the flow is examined.',\n",
       "  'len': 1189},\n",
       " {'abstract': \"ESA's PROBA-V Earth observation satellite enables us to monitor our planet at a large scale, studying the interaction between vegetation and climate and provides guidance for important decisions on our common global future. However, the interval at which high resolution images are recorded spans over several days, in contrast to the availability of lower resolution images which is often daily. We collect an extensive dataset of both, high and low resolution images taken by PROBA-V instruments during monthly periods to investigate Multi Image Super-resolution, a technique to merge several low resolution images to one image of higher quality. We propose a convolutional neural network that is able to cope with changes in illumination, cloud coverage and landscape features which are challenges introduced by the fact that the different images are taken over successive satellite passages over the same region. Given a bicubic upscaling of low resolution images taken under optimal conditions, we find the Peak Signal to Noise Ratio of the reconstructed image of the network to be higher for a large majority of different scenes. This shows that applied machine learning has the potential to enhance large amounts of previously collected earth observation data during multiple satellite passes.\",\n",
       "  'len': 1311},\n",
       " {'abstract': 'Electromagnetic interference (EMI) shielding coating materials with thicknesses in the microscale are required in many sectors, including communications, medical, aerospace and electronics, to isolate the electromagnetic radiation emitted from electronic equipment. We report a spray, layer-by-layer (LbL) coating approach to fabricate micron thick, highly-ordered and electrically-conductive coatings with exceptional EMI shielding effectiveness (EMI SE >4830 dB/mm), through the alternating self-assembly of negatively-charged reduced graphene oxide (RGO) and a positively-charged polyelectrolyte (PEI). The microstructure and resulting electrical properties of the (PEI/RGO)n LbL structures are studied as function of increasing mass of graphene deposited per cycle (keeping the PEI content constant), number of deposited layers (n), flake diameter and type of RGO. A strong effect of the lateral flake dimensions on the electrical properties is observed, which also influences the EMI SE. A maximum EMI SE of 29 dB is obtained for a 6 um thick (PEI/RGO)10 coating with 19 vol.% loading of reduced electrochemically-exfoliated graphene oxide flakes with diameters ~3um. This SE performance exceeds those previously reported for thicker graphene papers and bulk graphene/polymer composite films with higher RGO or graphene nanoplatelets contents, which represents an important step towards the fabrication of thin and light-weight high-performance EMI shielding structures.',\n",
       "  'len': 1486},\n",
       " {'abstract': 'Breakup of a liquid jet in a high speed gaseous crossflow finds wide range of engineering and technological applications, especially in the combustors of the gas turbine engines in aerospace industry. In this study, we present volume-of-fluid method based direct numerical simulations of a liquid jet injected into a swirling crossflow of gas. The liquid is injected radially outwards from a central tube to a confined annular space with a swirling gas crossflow. The essential features of the jet breakup involving jet flattening, surface waves and stripping of droplets from the edges of the jet are captured in the simulations. We discuss the effect of swirl on the spray characteristics such as jet trajectory, column breakup-length, and size, shape-factor and velocity distribution of the drops. Drop size increases with swirl and penetration is slightly reduced. Moreover, the trajectory follows an angle (azimuthal) that is smaller than the geometric angle of the swirl at the inlet. Interestingly, we also observe coalescence events downstream of the jet that affect the final droplet size distribution for the geometry considered in this study.',\n",
       "  'len': 1164},\n",
       " {'abstract': 'The Directional Polarimetric Camera (DPC) is the first Chinese multi-angle polarized earth observation satellite sensor, which has been launched onboard the GaoFen-5 Satellite in Chinese High-resolution Earth Observation Program. GaoFen-5 runs in a sun-synchronous orbit with the 2-days revisiting period. DPC employed a charge coupled device detection unit, and can realize spatial resolution of 3.3 km under a swath width of 1850 km. Moreover, DPC has 3 polarized channels together with 5 non-polarized bands, and is able to obtain at least 9 viewing angles by continuously capturing series images over the same target on orbit. Based on the Directional Polarization Camera (DPC) onboard GF-5 satellite, the first global high-resolution (3.3 km) map of fine-mode aerosol optical depth (AODf) over land has been obtained together by Aerospace Information Institute, Chinese Academy of Sciences, Anhui Institute of Optics and Fine Mechanics, Chinese Academy of Sciences (the manufacturer of DPC sensor) and other institutes. This AODf remote sensing observation dataset has the highest spatial resolution in the world. It can reflect the spatial information of major air pollutants (PM2.5, etc.) and provide critical basic products for \"decryption\" of global haze distribution.',\n",
       "  'len': 1288},\n",
       " {'abstract': \"This work is concerned with the numerical simulation of elastoplastic, electromagnetic and thermal response of aerospace materials due to their interaction with a plasma arc under lightning strike conditions. Current approaches treat the interaction between these two states of matter either in a decoupled manner or through one-way coupled 'co-simulation'. In this paper a methodology for multiphysics simulations of two-way interaction between lightning and elastoplastic materials is presented, which can inherently capture the non-linear feedback between these two states of matter. This is achieved by simultaneously solving the magnetohydrodynamic and the elastoplastic systems of equations on the same computational mesh, evolving the magnetic and electric fields dynamically. The resulting model allows for the topological evolution and movement of the arc attachment point coupled to the structural response and Joule heating of the substrate. The dynamic communication between the elastoplastic material and the plasma is facilitated by means of Riemann problem-based ghost fluid methods. This two-way coupling, to the best of the authors' knowledge, has not been previously demonstrated. The proposed model is first validated against experimental laboratory studies, demonstrating that the growth of the plasma arc can be accurately reproduced, dependent on the electrical conductivity of the substrate. It is then subsequently evaluated in a setting where the dynamically-evolved properties within the substrate feed back into the plasma arc attachment. Results are presented for multi-layered substrates of different materials, and for a substrate with temperature-dependent electrical conductivity. It is demonstrated that these conditions generate distinct behaviour due to the interaction between the plasma arc and the substrate.\",\n",
       "  'len': 1857},\n",
       " {'abstract': 'Molybdenum disulfide (MoS$_2$) is one of the most broadly utilized solid lubricants with a wide range of applications, including but not limited to those in the aerospace/space industry. Here we present a focused review of solid lubrication with MoS$_2$ by highlighting its structure, synthesis, applications and the fundamental mechanisms underlying its lubricative properties, together with a discussion of their environmental and temperature dependence. An effort is made to cover the main theoretical and experimental studies that constitute milestones in our scientific understanding. The review also includes an extensive overview of the structure and tribological properties of doped MoS$_2$, followed by a discussion of potential future research directions.',\n",
       "  'len': 776},\n",
       " {'abstract': 'We present a deep recurrent neural network architecture to solve a class of stochastic optimal control problems described by fully nonlinear Hamilton Jacobi Bellmanpartial differential equations. Such PDEs arise when one considers stochastic dynamics characterized by uncertainties that are additive and control multiplicative. Stochastic models with the aforementioned characteristics have been used in computational neuroscience, biology, finance and aerospace systems and provide a more accurate representation of actuation than models with additive uncertainty. Previous literature has established the inadequacy of the linear HJB theory and instead rely on a non-linear Feynman-Kac lemma resulting in a second order forward-backward stochastic differential equations representation. However, the proposed solutions that use this representation suffer from compounding errors and computational complexity leading to lack of scalability. In this paper, we propose a deep learning based algorithm that leverages the second order Forward-Backward SDE representation and LSTM based recurrent neural networks to not only solve such Stochastic Optimal Control problems but also overcome the problems faced by previous approaches and scales well to high dimensional systems. The resulting control algorithm is tested on non-linear systems in robotics and biomechanics to demonstrate feasibility and out-performance against previous methods.',\n",
       "  'len': 1448},\n",
       " {'abstract': \"Accurate platform localization is an integral component of most robotic systems. As these robotic systems become more ubiquitous, it is necessary to develop robust state estimation algorithms that are able to withstand novel and non-cooperative environments. When dealing with novel and non-cooperative environments, little is known a priori about the measurement error uncertainty, thus, there is a requirement that the uncertainty models of the localization algorithm be adaptive. Within this paper, we propose the batch covariance estimation technique, which enables robust state estimation through the iterative adaptation of the measurement uncertainty model. The adaptation of the measurement uncertainty model is granted through non-parametric clustering of the residuals, which enables the characterization of the measurement uncertainty via a Gaussian mixture model. The provided Gaussian mixture model can be utilized within any non-linear least squares optimization algorithm by approximately characterizing each observation with the sufficient statistics of the assigned cluster (i.e., each observation's uncertainty model is updated based upon the assignment provided by the non-parametric clustering algorithm). The proposed algorithm is verified on several GNSS collected data sets, where it is shown that the proposed technique exhibits some advantages when compared to other robust estimation techniques when confronted with degraded data quality.\",\n",
       "  'len': 1475},\n",
       " {'abstract': 'This paper studies the air-to-ground (AG) ultra-wideband (UWB) propagation channel through measurements between 3.1 GHz to 4.8 GHz using unmanned-aerial-vehicles (UAVs). Different line-of-sight (LOS) and obstructed- LOS scenarios and two antenna orientations were used in the experiments. Channel statistics for different propagation scenarios were obtained, and the Saleh-Valenzuela (SV) model was found to provide a good fit for the statistical channel model. An analytical path loss model based on antenna gains in the elevation plane is provided for unobstructed UAV hovering and moving (in a circular path) propagation scenarios.',\n",
       "  'len': 645},\n",
       " {'abstract': 'Inspired by the extensive application of terahertz imaging technologies in the field of aerospace, we exploit a terahertz frequency modulated continuous wave imaging method with continuous wavelet transform algorithm to detect a multilayer heat shield made of special materials. This method uses the frequency modulation continuous wave system to catch the reflected terahertz signal and then processing the image data by the continuous wavelet transform with different basis functions. By calculating the sizes of the defects area in the final images and then comparing the results with real samples, a novel and practical high-precision terahertz imaging method are demonstrated. Our method can be an effective tool for the terahertz nondestructive testing of composites, drugs and some cultural heritages.',\n",
       "  'len': 819},\n",
       " {'abstract': 'Inertial navigation computation is to acquire the attitude, velocity and position information of a moving body by integrating inertial measurements from gyroscopes and accelerometers. Over half a century has witnessed great efforts in coping with the motion non-commutativity errors to accurately compute the navigation information as far as possible, so as not to compromise the quality measurements of inertial sensors. Highly dynamic applications and the forthcoming cold-atom precision inertial navigation systems demand for even more accurate inertial navigation computation. The paper gives birth to an inertial navigation algorithm to fulfill that demand, named the iNavFIter, which is based on a brand-new framework of functional iterative integration and Chebyshev polynomials. Remarkably, the proposed iNavFIter reduces the non-commutativity errors to almost machine precision, namely, the coning/sculling/scrolling errors that have perplexed the navigation community for long. Numerical results are provided to demonstrate its accuracy superiority over the state-of-the-art inertial navigation algorithms at affordable computation cost.',\n",
       "  'len': 1158},\n",
       " {'abstract': 'In this paper, we consider a problem of failure prediction in the context of predictive maintenance applications. We present a new approach for rare failures prediction, based on a general methodology, which takes into account peculiar properties of technical systems. We illustrate the applicability of the method on the real-world test cases from aircraft operations.',\n",
       "  'len': 380},\n",
       " {'abstract': 'In recent years, with the development of aerospace technology, we use more and more images captured by satellites to obtain information. But a large number of useless raw images, limited data storage resource and poor transmission capability on satellites hinder our use of valuable images. Therefore, it is necessary to deploy an on-orbit semantic segmentation model to filter out useless images before data transmission. In this paper, we present a detailed comparison on the recent deep learning models. Considering the computing environment of satellites, we compare methods from accuracy, parameters and resource consumption on the same public dataset. And we also analyze the relation between them. Based on experimental results, we further propose a viable on-orbit semantic segmentation strategy. It will be deployed on the TianZhi-2 satellite which supports deep learning methods and will be lunched soon.',\n",
       "  'len': 925},\n",
       " {'abstract': \"Aircraft failures alter the aircraft dynamics and cause maneuvering flight envelope to change. Such envelope variations are nonlinear and generally unpredictable by the pilot as they are governed by the aircraft's complex dynamics. Hence, in order to prevent in-flight Loss of Control it is crucial to practically predict the impaired aircraft's flight envelope variation due to any a-priori unknown failure degree. This paper investigates the predictability of the number of trim points within the maneuvering flight envelope and its centroid using both linear and nonlinear least-squares estimation methods. To do so, various polynomial models and nonlinear models based on hyperbolic tangent function are developed and compared which incorporate the influencing factors on the envelope variations as the inputs and estimate the centroid and the number of trim points of the maneuvering flight envelope at any intended failure degree. Results indicate that both the polynomial and hyperbolic tangent function-based models are capable of predicting the impaired fight envelope variation with good precision. Furthermore, it is shown that the regression equation of the best polynomial fit enables direct assessment of the impaired aircraft's flight envelope contraction and displacement sensitivity to the specific parameters characterizing aircraft failure and flight condition.\",\n",
       "  'len': 1391},\n",
       " {'abstract': 'In this paper an uncertain norm-bounded mathematical model for the UAV High Altitude Performance Demonstrator (HAPD) designed by Italian Aerospace Research Center (CIRA) is carried out. The linear state space description aims to describe the non-linear aircraft dynamic inside the operating envelope characterized by the following bounds: true air speed between 17 m/s and 23 m/s and altitude from 300 m to 700 m.',\n",
       "  'len': 424},\n",
       " {'abstract': 'Bayesian Optimization using Gaussian Processes is a popular approach to deal with the optimization of expensive black-box functions. However, because of the a priori on the stationarity of the covariance matrix of classic Gaussian Processes, this method may not be adapted for non-stationary functions involved in the optimization problem. To overcome this issue, a new Bayesian Optimization approach is proposed. It is based on Deep Gaussian Processes as surrogate models instead of classic Gaussian Processes. This modeling technique increases the power of representation to capture the non-stationarity by simply considering a functional composition of stationary Gaussian Processes, providing a multiple layer structure. This paper proposes a new algorithm for Global Optimization by coupling Deep Gaussian Processes and Bayesian Optimization. The specificities of this optimization method are discussed and highlighted with academic test cases. The performance of the proposed algorithm is assessed on analytical test cases and an aerospace design optimization problem and compared to the state-of-the-art stationary and non-stationary Bayesian Optimization approaches.',\n",
       "  'len': 1185},\n",
       " {'abstract': 'Hardware synthesis is a general term used to refer to the processes involved in automatically generating a hardware design from its specification. High-level synthesis (HLS) could be defined as the translation from a behavioral description of the intended hardware circuit into a structural description similar to the compilation of programming languages (such as C and Pascal into assembly language. The chained synthesis tasks at each level of the design process include system synthesis, register-transfer synthesis, logic synthesis, and circuit synthesis. The development of hardware solutions for complex applications is no more a complicated task with the emergence of various HLS tools. Many areas of application have benefited from the modern advances in hardware design, such as automotive and aerospace industries, computer graphics, signal and image processing, security, complex simulations like molecular modeling, and DND matching. The field of HLS is continuing its rapid growth to facilitate the creation of hardware and to blur more and more the border separating the processes of designing hardware and software.',\n",
       "  'len': 1141},\n",
       " {'abstract': \"In the context of a Brain Computer Interface platform implemented for the arm rehabilitation of mildly impaired stroke patients, two methods of EEG signals processing are compared in terms of (i) their identification performance rate and (ii) their computational complexity with the overall goal to select the most efficient and feasible real-time procedure. An effective signal processing is, indeed, one of the most critical issue for such kind of technology which aims to establish a real-time communication between the subject's brain and a machine, i.e. a computer, a robotic arm or another device, that should implement his/her intention to move in place of his/her impaired arm.\",\n",
       "  'len': 696},\n",
       " {'abstract': 'In the Solar System, extra-terrestrial organic molecules have been found on cometary primitive objects, on Titan and Enceladus icy moons and on Mars. Identification could be achieved for simple organic species by remote sensing based on spectroscopic methods. However in situ mass spectrometry is a key technology to determine the nature of more complex organic matter. A new concept of mass analyser offering ultra-high mass resolving power of more than 50,000 at m/z 56 (under high vacuum condition about 10-9 mbar) is currently being developed for space applications: the CosmOrbitrap (Briois et al., 2016), based on the OrbitrapTM technology. This work challenges the use of LAb-CosmOrbitrap, a space instrument prototype combining Laser Ablation ionisation and the CosmOrbitrap mass analyser, to identify solid organic molecules of relevance to the future space exploration. For this purpose a blind test was jointly organised by the JAXA-HRMS team (Japan Aerospace Exploration Agency-High Resolution Mass Spectrometry) and the CosmOrbitrap consortium. The JAXA team provided two organic samples, whereas the CosmOrbitrap consortium analysed them without prior information. Thanks to the high analytical performances of the prototype and our HRMS data post-processing, we successfully identified the two molecules as HOBt, hydroxybenzotriazole (C6H5N3O) and BBOT, 2,5-Bis(5-tert-butyl-benzoxazol-2-yl)thiophene (C26H26N2O2S), with a mass resolving power of, respectively, 123 540 and 69 219. The success of this blind test on complex organic molecules shows the strong potential of LAb-CosmOrbitrap for future space applications.',\n",
       "  'len': 1645},\n",
       " {'abstract': 'Predicting the outcomes of integrating Unmanned Aerial Systems (UAS) into the National Aerospace (NAS) is a complex problem which is required to be addressed by simulation studies before allowing the routine access of UAS into the NAS. This thesis focuses on providing 2D and 3D simulation frameworks using a game theoretical methodology to evaluate integration concepts in scenarios where manned and unmanned air vehicles co-exist. The fundamental gap in the literature is that the models of interaction between manned and unmanned vehicles are insufficient: a) they assume that pilot behavior is known a priori and b) they disregard decision making processes. The contribution of this work is to propose a modeling framework, in which, human pilot reactions are modeled using reinforcement learning and a game theoretical concept called level-k reasoning to fill this gap. The level-k reasoning concept is based on the assumption that humans have various levels of decision making. Reinforcement learning is a mathematical learning method that is rooted in human learning. In this work, a classical and an approximate reinforcement learning (Neural Fitted Q Iteration) methods are used to model time-extended decisions of pilots with 2D and 3D maneuvers. An analysis of UAS integration is conducted using example scenarios in the presence of manned aircraft and fully autonomous UAS equipped with sense and avoid algorithms.',\n",
       "  'len': 1437},\n",
       " {'abstract': 'Structures and/or materials with engineered functionality, capable of achieving targeted mechanical responses reacting to changes in external excitation, have various potential engineering applications, e.g. aerospace, oceanographic engineering, soft robot, and several others. Yet tunable mechanical performance is normally realized through carefully designing the architecture of structures, which is usually porous, leading to the complexity of the fabrication of the structures even using the recently emerged 3D printing technique. In this study we show that origami technique can provide an alternative solution to achieving the aim by carefully stacking the classical Miura sheets into the Miura-ori tube metamaterial and tuning the geometric parameters of the origami metamaterial. By combining numerical and experimental methods, we have demonstrated that an extremely broad range of natural frequency and dynamic response of the metamaterial can be achieved. The proposed structure can be easily fabricated from a single thin sheet made of one material and simultaneously owns better mechanical properties than the Miura sheet.',\n",
       "  'len': 1148},\n",
       " {'abstract': 'Mixed-integer model predictive control (MI-MPC) requires the solution of a mixed-integer quadratic program (MIQP) at each sampling instant under strict timing constraints, where part of the state and control variables can only assume a discrete set of values. Several applications in automotive, aerospace and hybrid systems are practical examples of how such discrete-valued variables arise. We utilize the sequential nature and the problem structure of MI-MPC in order to provide a branch-and-bound algorithm that can exploit not only the block-sparse optimal control structure of the problem but that can also be warm started by propagating information from branch-and-bound trees and solution paths at previous time steps. We illustrate the computational performance of the proposed algorithm and compare against current state-of-the-art solvers for multiple MPC case studies, based on a preliminary implementation in MATLAB and C code.',\n",
       "  'len': 951},\n",
       " {'abstract': \"Pits on the Moon and Mars are intriguing geological formations that have yet to be explored. These geological formations can provide protection from harsh diurnal temperature variations, ionizing radiation, and meteorite impacts. Some have proposed that these underground formations are well-suited as human outposts. Some theorize that the Martian pits may harbor remnants of past life. Unfortunately, these geo-logical formations have been off-limits to conventional wheeled rovers and lander systems due to their collapsed ceiling or 'skylight' entrances. In this paper, a new low-cost method to explore these pits is presented using the Spring Propelled Extreme Environment Robot (SPEER). The SPEER consists of a launch system that flings disposable spherical microbots through skylights into the pits. The microbots are low-cost and composed of aluminium Al-6061 disposable spheres with an array of adapted COTS sensors and a solid rocket motor for soft this http URL moving most control authority to the launcher, the microbots become very simple, lightweight, and low-cost. We present a preliminary design of the microbots that can be built today using commercial components for under 500 USD. The microbots have a total mass of 1 kg, with more than 750 g available for a science instrument. In this paper, we present the design, dynamics and control, and operation of these microbots. This is followed by initial feasibility studies of the SPEER system by simulating exploration of a known Lunar pit in Mare Tranquillitatis.\",\n",
       "  'len': 1543},\n",
       " {'abstract': 'Metal-based additive manufacturing (AM) represents a paradigm change in engineering and production methods across multiple industries and sectors. AM methods enable mass reduction and performance optimisation well beyond that achievable via conventional manufacturing, thereby impacting significantly on aerospace and space technologies. Technologies relying on high and ultra-high vacuum (UHV), such as x-ray photo-electron spectroscopy, photo-sensors, cameras and cryostats, could also benefit greatly from AM. Despite recent advances in AM processing of metals, additively manufactured UHV chambers have so far not been achieved. Reducing the mass of UHV equipment is particularly critical for the development of portable cold atom systems, which are expected to underpin the next generation of sensing and timekeeping technologies and to allow novel space-based sensors for fundamental research. We demonstrate here an additively manufactured UHV chamber reaching a pressure below $10^{-10}$ mbar, enabling a cloud of cold $^{85}$Rb atoms to be trapped - the starting point for many precision timekeeping and sensing devices. The chamber is manufactured from aluminium alloy AlSi10Mg by laser powder bed fusion and has a mass of less than a third of a commercially-available equivalent. Outgassing analysis based on mass spectrometry was performed and it was demonstrated that even without active pumping the system remains in the $10^{-9}$ mbar regime for up to 48 hours.',\n",
       "  'len': 1487},\n",
       " {'abstract': 'The 5th edition of the International Conference on Cloud and Robotics (ICCR 2018 - this http URL) will be held on November 12-14 2018 in Paris and Saint-Quentin, France. The conference is a co-event with GDR ALROB and the industry exposition Robonumerique (this http URL). The domain of cloud robotics aims to converge robots with computation, storage and communication resources provided by the cloud. The cloud may complement robotic resources in several ways, including crowd-sourcing knowledge databases, context information, computational offloading or data-intensive information processing for artificial intelligence. Today, the paradigms of cloud/fog/edge computing propose software architecture solutions for robots to share computations or offload them to ambiant and networked resources. Yet, combining distant computations with the real time constraints of robotics is very challenging. As the challenges in this domain are multi-disciplinary and similar in other research areas, Cloud Robotics aims at building bridges among experts from academia and industry working in different fields, such as robotics, cyber-physical systems, automotive, aerospace, machine learning, artificial intelligence, software architecture, big data analytics, Internet-of-Things, networked control and distributed cloud systems.',\n",
       "  'len': 1332},\n",
       " {'abstract': 'The instrumentation of real systems is often designed for control purposes and control inputs are designed to achieve nominal control objectives. Hence, the available measurements may not be sufficient to isolate faults with certainty and diagnoses are ambiguous. Active diagnosis formulates a planning problem to generate a sequence of actions that, applied to the system, enforce diagnosability and allow to iteratively refine ambiguous diagnoses. This paper analyses the requirements for applying active diagnosis to space systems and proposes ActHyDiag as an effective framework to solve this problem. It presents the results of applying ActHyDiag to a real space case study and of implementing the generated plans in the form of On-Board Control Procedures. The case study is a redundant Spacewire Network where up to 6 instruments, monitored and controlled by the on-board software hosted in the Satellite Management Unit, are transferring science data to a mass memory unit through Spacewire routers. Experiments have been conducted on a real physical benchmark developed by Thales Alenia Space and demonstrate the effectiveness of the plans proposed by ActHyDiag.',\n",
       "  'len': 1182},\n",
       " {'abstract': 'With an ever-widening domain of aerial robotic applications, including many mission critical tasks such as disaster response operations, search and rescue missions and infrastructure inspections taking place in GPS-denied environments, the need for reliable autonomous operation of aerial robots has become crucial. Operating in GPS-denied areas aerial robots rely on a multitude of sensors to localize and navigate. Visible spectrum cameras are the most commonly used sensors due to their low cost and weight. However, in environments that are visually-degraded such as in conditions of poor illumination, low texture, or presence of obscurants including fog, smoke and dust, the reliability of visible light cameras deteriorates significantly. Nevertheless, maintaining reliable robot navigation in such conditions is essential. In contrast to visible light cameras, thermal cameras offer visibility in the infrared spectrum and can be used in a complementary manner with visible spectrum cameras for robot localization and navigation tasks, without paying the significant weight and power penalty typically associated with carrying other sensors. Exploiting this fact, in this work we present a multi-sensor fusion algorithm for reliable odometry estimation in GPS-denied and degraded visual environments. The proposed method utilizes information from both the visible and thermal spectra for landmark selection and prioritizes feature extraction from informative image regions based on a metric over spatial entropy. Furthermore, inertial sensing cues are integrated to improve the robustness of the odometry estimation process. To verify our solution, a set of challenging experiments were conducted inside a) an obscurant filed machine shop-like industrial environment, as well as b) a dark subterranean mine in the presence of heavy airborne dust.',\n",
       "  'len': 1865},\n",
       " {'abstract': 'In this paper, considering an interference limited inband downlink cellular network, we study the effects of scheduling criteria, mobility constraints, path loss models, backhaul constraints, and 3D antenna radiation pattern on trajectory optimization problem of an unmanned aerial vehicle (UAV). In particular, we consider a UAV that is tasked to travel between two locations within a given amount of time (e.g., for delivery or surveillance purposes), and we consider that such a UAV can be used to improve cellular connectivity of mobile users by serving as a relay for the terrestrial network. As the optimization problem is hard to solve numerically, we explore the dynamic programming (DP) technique for finding the optimum UAV trajectory. We utilize capacity and coverage performance of the terrestrial network while studying all the effects of different techniques and phenomenon. Extensive simulations show that the maximum sum-rate trajectory provides the best per user capacity whereas, the optimal proportional fair (PF) rate trajectory provides higher coverage probability than the other two. Since, the generated trajectories are infeasible for the UAV to follow exactly as it can not take sharp turns due to kinematic constraints, we generate smooth trajectory using Bezier curves. Our results show that the cellular capacity using the Bezier curves is close to the capacity observed when using the optimal trajectories.',\n",
       "  'len': 1446},\n",
       " {'abstract': 'We have built and evaluated a prototype quantum radar, which we call a quantum two-mode squeezing radar (QTMS radar), in the laboratory. It operates solely at microwave frequencies; there is no downconversion from optical frequencies. Because the signal generation process relies on quantum mechanical principles, the system is considered to contain a quantum-enhanced radar transmitter. This transmitter generates a pair of entangled microwave signals and transmits one of them through free space, where the signal is measured using a simple and rudimentary receiver. At the heart of the transmitter is a device called a Josephson parametric amplifier (JPA), which generates a pair of entangled signals called two-mode squeezed vacuum (TMSV) at 6.1445 GHz and 7.5376 GHz. These are then sent through a chain of amplifiers. The 7.5376 GHz beam passes through 0.5 m of free space; the 6.1445 GHz signal is measured directly after amplification. The two measurement results are correlated in order to distinguish signal from noise. We compare our QTMS radar to a classical radar setup using conventional components, which we call a two-mode noise radar (TMN radar), and find that there is a significant gain when both systems broadcast signals at -82 dBm. This is shown via a comparison of receiver operator characteristic (ROC) curves. In particular, we find that the quantum radar requires 8 times fewer integrated samples compared to its classical counterpart to achieve the same performance.',\n",
       "  'len': 1504},\n",
       " {'abstract': 'We introduce an immersed high-order discontinuous Galerkin method for solving the compressible Navier-Stokes equations on non-boundary-fitted meshes. The flow equations are discretised with a mixed discontinuous Galerkin formulation and are advanced in time with an explicit time marching scheme. The discretisation meshes may contain simplicial (triangular or tetrahedral) elements of different sizes and need not be structured. On the discretisation mesh the fluid domain boundary is represented with an implicit signed distance function. The cut-elements partially covered by the solid domain are integrated after tessellation with the marching triangle or tetrahedra algorithms. Two alternative techniques are introduced to overcome the excessive stable time step restrictions imposed by cut-elements. In the first approach the cut-basis functions are replaced with the extrapolated basis functions from the nearest largest element. In the second approach the cut-basis functions are simply scaled proportionally to the fraction of the cut-element covered by the solid. To achieve high-order accuracy additional nodes are introduced on the element faces abutting the solid boundary. Subsequently, the faces are curved by projecting the introduced nodes to the boundary. The proposed approach is verified and validated with several two- and three-dimensional subsonic and hypersonic low Reynolds number flow applications, including the flow over a cylinder, a space capsule and an aerospace vehicle.',\n",
       "  'len': 1513},\n",
       " {'abstract': 'Owing to its wide (3.4 eV) and direct-tunable band gap, gallium nitride (GaN) is an excellent material platform for UV photodetectors. GaN is also stable in radiation-rich and high-temperature environments, which makes photodetectors fabricated using this material useful for in-situ flame detection and combustion monitoring. In this paper, we use a GaN photodetector to measure ultraviolet (UV) emissions from a hybrid rocket motor igniter plume. The normalized photocurrent-to-dark current ratio (NPDR) is a performance metric which simultaneously captures the two desired characteristics of high responsivity and low dark current. The NPDR of our device is record-high with a value of 6 x 10$^{14}$ W$^{-1}$ and the UV-to-visible rejection ratio is 4 x 10$^6$. The photodetector shows operation at high temperatures (up to 250°C), with the NPDR still remaining above 10$^9$ W$^{-1}$ and the peak wavelength shifting from 362 nm to 375 nm. The photodetector was placed at three radial distances (3\", 5.5\", and 7\") from the base of the igniter plume and the oxidizer-to-fuel ratio (O2/CH4) was varied. The data demonstrates a clear trend of increasing current (and thus intensity of plume emission) with increasing fuel concentration and decreasing separation between the photodetector and the plume. By treating the plume as a black body, and calculating a radiative configuration factor corresponding to the geometry of the plume and the detector, we calculated average plume temperatures at each of the three oxidizer-to-fuel ratios. The estimated plume temperatures were between 850 and 950 K for all three combustion conditions. The temperature is roughly invariant for a fixed fuel concentration for the three tested distances. These data demonstrate the functionality of GaN as a material platform for use in harsh environment flame monitoring.',\n",
       "  'len': 1864},\n",
       " {'abstract': 'Computer simulators are nowadays widely used to understand complex physical systems in many areas such as aerospace, renewable energy, climate modeling, and manufacturing. One fundamental issue in the study of computer simulators is known as experimental design, that is, how to select the input settings where the computer simulator is run and the corresponding response is collected. Extra care should be taken in the selection process because computer simulators can be computationally expensive to run. The selection shall acknowledge and achieve the goal of the analysis. This article focuses on the goal of producing more accurate prediction which is important for risk assessment and decision making. We propose two new methods of design approaches that sequentially select input settings to achieve this goal. The approaches make novel applications of simultaneous and sequential contour estimations. Numerical examples are employed to demonstrate the effectiveness of the proposed approaches.',\n",
       "  'len': 1012},\n",
       " {'abstract': \"Collision detection algorithms are used in aerospace, swarm robotics, automotive, video gaming, dynamics simulation and other domains. As many applications of collision detection run online, timing requirements are imposed on the algorithm runtime: algorithms must, at a minimum, keep up with the passage of time. In practice, this places a limit on the number of objects, n, that can be tracked at the same time. In this paper, we improve the scalability of collision detection, effectively raising the limit n for online object tracking. The key to our approach is the use of a four-dimensional axis-aligned bounding box (AABB) tree, which stores each object's three-dimensional occupancy region in space during a one-dimensional interval of time. This improves efficiency by permitting per-object variable times steps. Further, we describe partitioning strategies that can decompose the 4D AABB tree search into several smaller-dimensional problems that can be solved in parallel. We formalize the collision detection problem and prove our algorithm's correctness. We demonstrate the feasibility of online collision detection for an orbital space debris application, using publicly available data on the full catalog of n=16848 objects provided by this http URL.\",\n",
       "  'len': 1276},\n",
       " {'abstract': 'Advances in robotics, artificial intelligence, and machine learning are ushering in a new age of automation, as machines match or outperform human performance. Machine intelligence can enable businesses to improve performance by reducing errors, improving sensitivity, quality and speed, and in some cases achieving outcomes that go beyond current resource capabilities. Relevant applications include new product architecture design, rapid material characterization, and life-cycle management tied with a digital strategy that will enable efficient development of products from cradle to grave. In addition, there are also challenges to overcome that must be addressed through a major, sustained research effort that is based solidly on both inferential and computational principles applied to design tailoring of functionally optimized structures. Current applications of structural materials in the aerospace industry demand the highest quality control of material microstructure, especially for advanced rotational turbomachinery in aircraft engines in order to have the best tailored material property. In this paper, deep convolutional neural networks were developed to accurately predict processing-structure-property relations from materials microstructures images, surpassing current best practices and modeling efforts. The models automatically learn critical features, without the need for manual specification and/or subjective and expensive image analysis. Further, in combination with generative deep learning models, a framework is proposed to enable rapid material design space exploration and property identification and optimization. The implementation must take account of real-time decision cycles and the trade-offs between speed and accuracy.',\n",
       "  'len': 1774},\n",
       " {'abstract': 'This paper focuses on the detection and classification of micro-unmanned aerial vehicles (UAVs) using radio frequency (RF) fingerprints of the signals transmitted from the controller to the micro-UAV. In the detection phase, raw signals are split into frames and transformed into the wavelet domain. A Markov models-based naive Bayes approach is used to check for the presence of a UAV in each frame. In the classification phase, unlike the traditional approaches that rely solely on time-domain signals and corresponding features, the proposed technique uses the energy transient signal. This approach is more robust to noise and can cope with different modulation techniques. First, the normalized energy trajectory is generated from the energy-time-frequency distribution of the raw control signal. Next, the start and end points of the energy transient are detected by searching for the most abrupt changes in the mean of the energy trajectory. Then, a set of statistical features is extracted from the energy transient. Significant features are selected by performing neighborhood component analysis (NCA) to keep the computational cost of the algorithm low. Finally, selected features are fed to several machine learning algorithms for classification. The algorithms are evaluated experimentally using a database containing 100 RF signals from each of 14 different UAV controllers. The signals are recorded wirelessly using a high-frequency oscilloscope. The data set is randomly partitioned into training and test sets for validation with the ratio 4:1. Ten Monte Carlo simulations are run and results are averaged to assess the performance of the methods. All the micro-UAVs are detected correctly and an average accuracy of 96.3% is achieved using the k-nearest neighbor (kNN) classification. Proposed methods are also tested for different signal-to-noise ratio (SNR) levels and results are reported.',\n",
       "  'len': 1920},\n",
       " {'abstract': 'The key innovation in this paper is an open-source, high-performance iterative solver for high contrast, strongly anisotropic elliptic partial differential equations implemented within dune-pdelab. The iterative solver exploits a robust, scalable two-level additive Schwarz preconditioner, GenEO (Spillane et al. 2014). The development of this solver has been motivated by the need to overcome the limitations of commercially available modeling tools for solving structural analysis simulations in aerospace composite applications. Our software toolbox dune-composites encapsulates the mathematical complexities of the underlying packages within an efficient C++ framework, providing an application interface to our new high-performance solver. We illustrate its use on a range of industrially motivated examples, which should enable other scientists to build on and extend dune-composites and the GenEO preconditioner for use in their own applications. We demonstrate the scalability of the solver on more than 15,000 cores of the UK national supercomputer Archer, solving an aerospace composite problem with over 200 million degrees of freedom in a few minutes. This scale of computation brings composites problems that would otherwise be unthinkable into the feasible range. To demonstrate the wider applicability of the new solver, we also confirm the robustness and scalability of the solver on SPE10, a challenging benchmark in subsurface flow/reservoir simulation.',\n",
       "  'len': 1482},\n",
       " {'abstract': 'Planetary bodies such as asteroids, comets, and planetary moons are high-value science targets as they hold important information about the formation and evolution of our solar system. However, due to their low-gravity, variable sizes and shapes, dedicated orbiting spacecraft missions around these target bodies is difficult. Therefore, many planetary bodies are observed during flyby encounters, and consequently, the mapping coverage of the target body is limited. In this work, we propose the use of a spacecraft swarm to provide complete surface maps of a planetary body during a close encounter flyby. With the advancement of low-cost spacecraft technology, such a swarm can be realized by using multiple miniature spacecraft. The design of a swarm mission is a complex multi-disciplinary problem. To get started, we propose the Integrated Design Engineering & Automation of Swarms (IDEAS) software. In this work, we will introduce the development of the Automated Swarm Designer module of the software and apply it to total surface mapping of asteroid 433 Eros through flybys.',\n",
       "  'len': 1094},\n",
       " {'abstract': 'RodFIter is a promising method of attitude reconstruction from inertial measurements based on the functional iterative integration of Rodrigues vector. The Rodrigues vector is used to encode the attitude in place of the popular rotation vector because it has a polynomial-like rate equation and could be cast into theoretically sound and exact integration. This paper further applies the approach of RodFIter to the unity-norm quaternion for attitude reconstruction, named QuatFIter, and shows that it is identical to the previous Picard-type quaternion method. The Chebyshev polynomial approximation and truncation techniques from the RodFIter are exploited to speed up its implementation. Numerical results demonstrate that the QuatFIter is comparable in accuracy to the RodFIter, although its convergence rate is relatively slower with respect to the number of iterations. Notably, the QuatFIter has about two times better computational efficiency, thanks to the linear quaternion kinematic equation.',\n",
       "  'len': 1014},\n",
       " {'abstract': 'In the preliminary trajectory design of the multi-target rendezvous problem, a model that can quickly estimate the cost of the orbital transfer is essential. The estimation of the transfer time using solar sail between two arbitrary orbits is difficult and usually requires to solve an optimal control problem. Inspired by the successful applications of the deep neural network in nonlinear regression, this work explores the possibility and effectiveness of mapping the transfer time for solar sail from the orbital characteristics using the deep neural network. Furthermore, the Monte Carlo Tree Search method is investigated and used to search the optimal sequence considering a multi-asteroid exploration problem. The obtained sequences from preliminary design will be solved and verified by sequentially solving the optimal control problem. Two examples of different application backgrounds validate the effectiveness of the proposed approach.',\n",
       "  'len': 959},\n",
       " {'abstract': 'We present an approach for robust high-order mesh generation specially tailored to streamlined bodies. The method is based on a semi-sructured approach which combines the high quality of structured meshes in the near-field with the flexibility of unstructured meshes in the far-field. We utilise medial axis technology to robustly partition the near-field into blocks which can be meshed coarsely with a linear swept mesher. A high-order mesh of the near-field is then generated and split using an isoparametric approach which allows us to obtain highly stretched elements aligned with the flow field. Special treatment of the partition is performed on the wing root juntion and the trailing edge --- into the wake --- to obtain an H-type mesh configuration with anisotropic hexahedra ideal for the strong shear of high Reynolds number simulations. We then proceed to discretise the far-field using traditional robust tetrahedral meshing tools. This workflow is made possible by two sets of tools: CADfix, focused on CAD system, the block partitioning of the near-field and the generation of a linear mesh; and NekMesh, focused on the curving of the high-order mesh and the generation of highly-stretched boundary layer elements. We demonstrate this approach on a NACA0012 wing attached to a wall and show that a gap between the wake partition and the wall can be inserted to remove the dependency of the partitioning procedure on the local geometry.',\n",
       "  'len': 1461},\n",
       " {'abstract': 'Multiple-stage adaptive architectures are conceived to face with the problem of target detection buried in noise, clutter, and intentional interference. First, a scenario where the radar system is under the electronic attack of noise-like interferers is considered. In this context, two sets of training samples are jointly exploited to devise a novel two-step estimation procedure of the interference covariance matrix. Then, this estimate is plugged in the adaptive matched filter to mitigate the deleterious effects of the noise-like jammers on radar sensitivity. Besides, a second scenario, which also includes the presence of coherent jammers, is addressed. Specifically, the sparse nature of data is brought to light and the compressive sensing paradigm is applied to estimate target response and coherent jammers amplitudes. The likelihood ratio test, where the unknown parameters are replaced by previous estimates, is designed and assessed. Remarkably, the sparse approach allows for echo classification and estimation of both angles of arrival and number of the interfering sources. The performance analysis, conducted resorting to simulated data, highlights the effectiveness of the newly proposed architectures also in comparison with suitable competing architectures (when they exist).',\n",
       "  'len': 1309},\n",
       " {'abstract': 'We describe a semi-structured method for the generation of high-order hybrid meshes suited for the simulation of high Reynolds number flows. This is achieved through the use of highly stretched elements in the viscous boundary layers near the wall surfaces. CADfix is used to first repair any possible defects in the CAD geometry and then generate a medial object based decomposition of the domain that wraps the wall boundaries with partitions suitable for the generation of either prismatic or hexahedral elements. The latter is a novel distinctive feature of the method that permits to obtain well-shaped hexahedral meshes at corners or junctions in the boundary layer. The medial object approach allows greater control on the \"thickness\" of the boundary-layer mesh than is generally achievable with advancing layer techniques. CADfix subsequently generates a hybrid straight sided mesh of prismatic and hexahedral elements in the near-field region modelling the boundary layer, and tetrahedral elements in the far-field region covering the rest of the domain. The mesh in the near-field region provides a framework that facilitates the generation, via an isoparametric technique, of layers of highly stretched elements with a distribution of points in the direction normal to the wall tailored to efficiently and accurately capture the flow in the boundary layer. The final step is the generation of a high-order mesh using NekMesh, a high-order mesh generator within the Nektar++ framework. NekMesh uses the CADfix API as a geometry engine that handles all the geometrical queries to the CAD geometry required during the high-order mesh generation process. We will describe in some detail the methodology using a simple geometry, a NACA wing tip, for illustrative purposes. Finally, we will present two examples of application to reasonably complex geometries proposed by NASA as CFD validation cases.',\n",
       "  'len': 1917},\n",
       " {'abstract': 'Mixing characteristics of a supersonic jet influenced by a Downstream Microjet Fluidic Injection (DMFI) system are numerically investigated. The DMFI system is built on the observation of a previous experimental study that utilized transverse fluidic injection from four equally spaced injection ports placed on an injection tube at a distance downstream of a 1.5 Mach number nozzle. The measurements from these previous experiments demonstrated thickening and mixing enhancement of the jet shear layer as a result of fluidic injection. The current numerical study examines the underlying physics of the flow field, as well as the effectiveness of the DMFI system at smaller mass flow rate ratios compared to those utilized in the previous experiments. Results indicate good agreement with the trend observed by the experimental study, and considerable improvement in enhancement of the jet mixing is observed. The observed mixing enhancement is attributed to the presence of the microjet tube and fluidic injection and the consequent generation of streamwise vortices, as well as the natural separating bow shock due to the transverse flow injection. DMFI system is shown to enhance the early mixing, resulting in attenuation of the downstream turbulence production. Furthermore, the DMFI system is demonstrated to be an effective method of mixing enhancement for supersonic jets with a potential for reducing the jet noise radiation.',\n",
       "  'len': 1446},\n",
       " {'abstract': \"There is an every-growing need to construct large space telescopes and structures for observation of exo-planets, main-belt asteroids and NEOs. Space observation capabilities can significant enhanced by large-aperture structures. Structures extending to several meters in size could potentially revolutionize observation enabling technologies. These include star-shades for imaging distant objects such as exo-planets and high-resolution large aperture telescopes. In addition to size, such structures require controllable precision surfaces and high packing efficiencies. A promising approach to achieving high compaction for large surface areas is by incorporating compliant materials or gossamers. Gossamer structures on their own do not meet stiffness requirements for controlled deployment. Supporting stiffening mechanisms are required to fully realize their structural potential. The accuracy of the 'active' surface constructed out of a gossamer additionally also depends on the load bearing structure that supports it. This paper investigates structural assemblies constructed from modular inflatable membranes stiffened pneumatically using inflation gas. These units assembled into composites can yield desirable characteristics. We present the design of large assemblies of these modular elements.\",\n",
       "  'len': 1319},\n",
       " {'abstract': 'There are thousands of asteroids in near-Earth space and millions expected in the Main Belt. They are diverse in their physical properties and compositions. They are also time capsules of the early Solar System making them valuable for planetary science, and are strategic for resource mining, planetary defense/security and as interplanetary depots. But we lack direct knowledge of the geophysical behavior of an asteroid surface under milligravity conditions, and therefore landing on an asteroid and manipulating its surface material remains a daunting challenge. Towards this goal we are putting forth plans for a 12U CubeSat that will be in Low Earth Orbit and that will operate as a spinning centrifuge on-orbit. In this paper, we will present an overview of the systems engineering and instrumentation design on the spacecraft. Parts of this 12U CubeSat will contain a laboratory that will recreate asteroid surface conditions by containing crushed meteorite. The laboratory will spin at 1 to 2 RPM during the primary mission to simulate surface conditions of asteroids 2 km and smaller, followed by an extended mission where the spacecraft will spin at even higher RPM. The result is a bed of realistic regolith, the environment that landers and diggers and maybe astronauts will interact with. The CubeSat is configured with cameras, lasers, actuators and small mechanical instruments to both observe and manipulate the regolith at low simulated gravity conditions. A series of experiments will measure the general behavior, internal friction, adhesion, dilatancy, coefficients of restitution and other parameters that can feed into asteroid surface dynamics simulations. Effective gravity can be varied, and external mechanical forces can be applied.',\n",
       "  'len': 1771},\n",
       " {'abstract': \"Exploration of small bodies, namely comets and asteroids remain a challenging endeavor due to their low gravity. The risk is so high that missions such as Hayabusa II and OSIRIS-REx will be performing touch and go missions to obtain samples. The next logical step is to perform longer-term mobility on the surface of these asteroid. This can be accomplished by sending small landers of a 1 kg or less with miniature propulsion systems that can just offset the force of asteroid gravity. Such a propulsion system would ideally be used to hop on the surface of the asteroid. Hopping has been found to be most efficient form of mobility on low-gravity. Use of wheels for rolling presents substantial challenges as the wheel can't gain traction to roll. The Asteroid Mobile Imager and Geologic Observer (AMIGO) utilizes 1 kg landers that are stowed in a 1U CubeSat configuration and deployed, releasing an inflatable that is 1 m in diameter. The inflatable is attached to the top of the 1U lander, enabling high speed communications and a means of easily tracking lander from an overhead mothership. Milligravity propulsion is required for the AMIGO landers to perform ballistic hops on the asteroid surface. The propulsion system is used to navigate the lander across the surface of the asteroid under the extremely low gravity while taking care to not exceed escape velocity.Although the concept for AMIGO missions is to use multiple landers, the more surface area evaluated by each lander the better. Without a propulsion system, each AMIGO will have a limited range of observable area. The propulsion system also serves as a rough attitude control system (ACS), as it enables pointing and regulation over where the lander is positioned via an array of MEMS thrusters.\",\n",
       "  'len': 1778},\n",
       " {'abstract': \"Deep reinforcement learning (deep RL) has achieved superior performance in complex sequential tasks by learning directly from image input. A deep neural network is used as a function approximator and requires no specific state information. However, one drawback of using only images as input is that this approach requires a prohibitively large amount of training time and data for the model to learn the state feature representation and approach reasonable performance. This is not feasible in real-world applications, especially when the data are expansive and training phase could introduce disasters that affect human safety. In this work, we use a human demonstration approach to speed up training for learning features and use the resulting pre-trained model to replace the neural network in the deep RL Deep Q-Network (DQN), followed by human interaction to further refine the model. We empirically evaluate our approach by using only a human demonstration model and modified DQN with human demonstration model included in the Microsoft AirSim car simulator. Our results show that (1) pre-training with human demonstration in a supervised learning approach is better and much faster at discovering features than DQN alone, (2) initializing the DQN with a pre-trained model provides a significant improvement in training time and performance even with limited human demonstration, and (3) providing the ability for humans to supply suggestions during DQN training can speed up the network's convergence on an optimal policy, as well as allow it to learn more complex policies that are harder to discover by random exploration.\",\n",
       "  'len': 1643},\n",
       " {'abstract': 'This paper presents an experimental study of the air-to-ground (AG) propagation channel through ultrawideband(UWB) measurements in an open area using unmanned-aerial-vehicles (UAVs). Measurements were performed using UWB radios operating at a frequency range of 3.1 GHz - 4.8 GHz and UWB planar elliptical dipole antennas having an omni-directional pattern in the azimuth plane and typical donut shaped pattern in the elevation plane. Three scenarios were considered for the channel measurements: (i)two receivers (RXs) at different heights above the ground and placed close to each other in line-of-sight (LOS) with the transmitter (TX) on the UAV and the UAV is hovering; (ii) RXs are in obstructed line-of-sight (OLOS) with the UAV TX due to foliage, and the UAV is hovering; and, (iii) UAV is moving in a circular path. Different horizontal and vertical distances between the RXs and TX were used in the measurements. In addition, two different antenna orientations were used on the UAV antennas (vertical and horizontal) to analyze the effects of antenna radiation patterns on the UWB AG propagation. From the empirical results, it was observed that the received power depends mainly on the antenna radiation pattern in the elevation plane when the antennas are oriented in the same direction, as expected for these omni-azimuth antennas. Moreover, the overall antenna gain at the TX and RX can be approximated using trigonometric functions of the elevation angle.',\n",
       "  'len': 1480},\n",
       " {'abstract': 'Deep Neural Networks (DNN) will emerge as a cornerstone in automotive software engineering. However, developing systems with DNNs introduces novel challenges for safety assessments. This paper reviews the state-of-the-art in verification and validation of safety-critical systems that rely on machine learning. Furthermore, we report from a workshop series on DNNs for perception with automotive experts in Sweden, confirming that ISO 26262 largely contravenes the nature of DNNs. We recommend aerospace-to-automotive knowledge transfer and systems-based safety approaches, e.g., safety cage architectures and simulated system test cases.',\n",
       "  'len': 649},\n",
       " {'abstract': 'Structural health monitoring is a condition-based field of study utilised to monitor infrastructure, via sensing systems. It is therefore used in the field of aerospace engineering to assist in monitoring the health of aerospace structures. A difficulty however is that in structural health monitoring the data input is usually from sensor arrays, which results in data which are highly redundant and correlated, an area in which traditional two-way matrix approaches have had difficulty in deconstructing and interpreting. Newer methods involving tensor analysis allow us to analyse this multi-way structural data in a coherent manner. In our approach, we demonstrate the usefulness of tensor-based learning coupled with for damage detection, on a novel $N$-DoF Lagrangian aeroservoelastic model.',\n",
       "  'len': 808},\n",
       " {'abstract': 'The rapid developments of Artificial Intelligence in the last decade are influencing Aerospace Engineering to a great extent and research in this context is proliferating. We share our observations on the recent developments in the area of Spacecraft Guidance Dynamics and Control, giving selected examples on success stories that have been motivated by mission designs. Our focus is on evolutionary optimisation, tree searches and machine learning, including deep learning and reinforcement learning as the key technologies and drivers for current and future research in the field. From a high-level perspective, we survey various scenarios for which these approaches have been successfully applied or are under strong scientific investigation. Whenever possible, we highlight the relations and synergies that can be obtained by combining different techniques and projects towards future domains for which newly emerging artificial intelligence techniques are expected to become game changers.',\n",
       "  'len': 1005},\n",
       " {'abstract': 'Drones, unmanned aerial vehicles (UAVs), or unmanned aerial systems (UAS) are expected to be an important component of 5G/beyond 5G (B5G) communications. This includes their use within cellular architectures (5G UAVs), in which they can facilitate both wireless broadcast and point-to-point transmissions, usually using small UAS (sUAS). Allowing UAS to operate within airspace along with commercial, cargo, and other piloted aircraft will likely require dedicated and protected aviation spectrum at least in the near term, while regulatory authorities adapt to their use. The command and control (C2), or control and non-payload communications (CNPC) link provides safety critical information for the control of the UAV both in terrestrial-based line of sight (LOS) conditions and in satellite communication links for so-called beyond LOS (BLOS) conditions. In this paper, we provide an overview of these CNPC links as they may be used in 5G and satellite systems by describing basic concepts and challenges. We review new entrant technologies that might be used for UAV C2 as well as for payload communication, such as millimeter wave (mmWave) systems, and also review navigation and surveillance challenges. A brief discussion of UAV-to-UAV communication and hardware issues are also provided.',\n",
       "  'len': 1307},\n",
       " {'abstract': \"With the great advancements of convolution neural networks(CNN), CNN accelerators are increasingly developed and deployed in the major computing this http URL make use of the CNN accelerators, CNN models are trained via the off-line training systems such as Caffe, Pytorch and Tensorflow on multi-core CPUs and GPUs first and then compiled to the target accelerators. Although the two-step process seems to be natural and has been widely applied, it assumes that the accelerators' behavior can be fully modeled on CPUs and GPUs. This does not hold true and the behavior of the CNN accelerators is un-deterministic when the circuit works at 'unstable' mode when it is overclocked or is affected by the environment like fault-prone aerospace. The exact behaviors of the accelerators are determined by both the chip fabrication and the working environment or status. In this case, applying the conventional off-line training result to the accelerators directly may lead to considerable accuracy loss. To address this problem, we propose to train for the 'unstable' CNN accelerator and have the 'un-determined behavior' learned together with the data in the same framework. Basically, it starts from the off-line trained model and then integrates the uncertain circuit behaviors into the CNN models through additional accelerator-specific training. The fine-tuned training makes the CNN models less sensitive to the circuit uncertainty. We apply the design method to both an overclocked CNN accelerator and a faulty accelerator. According to our experiments on a subset of ImageNet, the accelerator-specific training can improve the top 5 accuracy up to 3.4% and 2.4% on average when the CNN accelerator is at extreme overclocking. When the accelerator is exposed to a faulty environment, the top 5 accuracy improves up to 6.8% and 4.28% on average under the most severe fault injection.\",\n",
       "  'len': 1894},\n",
       " {'abstract': 'A novel trajectory design methodology is proposed in the current work to minimize the state uncertainty in the crucial mission of spacecraft rendezvous. The trajectory is shaped under constraints utilizing a multiple-impulse approach. State uncertainty is characterized in terms of covariance, and the impulse time as the only affective parameter in uncertainty propagation is selected to minimize the trace of the covariance matrix. Further, the impulse location is also adopted as the other design parameter to satisfy various translational constraints of the space mission. Efficiency and viability of the proposed idea have been investigated through some scenarios that include constraints on final time, control effort, and maximum thruster limit addition to considering safe corridors. The obtained results show that proper selection of the impulse time and impulse position fulfils a successful feasible rendezvous mission with minimum uncertainty.',\n",
       "  'len': 966},\n",
       " {'abstract': 'Recently, deep neural networks (DNNs) have been the subject of intense research for the classification of radio frequency (RF) signals, such as synthetic aperture radar (SAR) imagery or micro-Doppler signatures. However, a fundamental challenge is the typically small amount of data available due to the high costs and resources required for measurements. Small datasets limit the depth of DNNs implementable, and limit performance. In this work, a novel method for generating diversified radar micro-Doppler signatures using Kinect-based motion capture simulations is proposed as a training database for transfer learning with DNNs. In particular, it is shown that together with residual learning, the proposed DivNet approach allows for the construction of deeper neural networks and offers improved performance in comparison to transfer learning from optical imagery. Furthermore, it is shown that initializing the network using diversified synthetic micro-Doppler signatures enables not only robust performance for previously unseen target profiles, but also class generalization. Results are presented for 7-class and 11-class human activity recognition scenarios using a 4-GHz continuous wave (CW) software-defined radar.',\n",
       "  'len': 1238},\n",
       " {'abstract': 'A methodology for non-intrusive, projection-based non-linear model reduction originally presented by Renganathan et. al. (2018)~\\\\cite{renganathan2018koopman} is further extended towards parametric systems with focus on application to aerospace design. Specifically, we extend the method for static systems with parametric geometry (that deforms the mesh), in addition to parametric boundary conditions. The main idea is to first perform a transformation on the governing equations such that it is lifted to a higher dimensional but linear under-determined system. This enables one to extract the system matrices easily compared to that of the original non-linear system. The under-determined system is closed with a set of model-dependent non-linear constraints upon which the model reduction is finally performed. The methodology is validated on the subsonic and transonic inviscid flow past the NACA0012 and the RAE2822 airfoils. We further demonstrate the utility of the approach by applying it to two common problems in aerospace design namely, derivative-free global optimization and parametric uncertainty quantification with Monte Carlo sampling. Overall, the methodology is shown to achieve accuracy upto 5\\\\% and computational speed-up of 2-3 orders of magnitude as that of the full-order model. Comparison against another non-intrusive model reduction method revealed that the proposed approach is more robust, accurate and retains the consistency between the state variables.',\n",
       "  'len': 1496},\n",
       " {'abstract': \"Thermal spraying has been present for over a century, being greatly refined and optimised during this time, becoming nowadays a reliable and cost-efficient method to deposit thick coatings with a wide variety of feedstock materials and substrates. Thermal sprayed coatings have been successfully applied in fields such as aerospace or electricity production, becoming an essential component of today's industry. To overpass the traditional capabilities of those coatings, new functionalities and coherent responses are being integrated, opening the field of functional and smart coatings. The aim of this paper is to present a comprehensive review of the current state of functional and smart coatings produced using thermal spraying deposition. It will first describe the different thermal spraying technologies, with a focus on how different techniques achieve the thermal and kinetic energy required to form a coating, as well as the environment to which feedstock particles are exposed in terms of temperature and velocity. It will then deal with the state-of-the-art functional and smart coatings applied using thermal spraying techniques, with a discussion on the fundamentals on which the coatings are designed, the efficiency of its performance and the industrial applications, both current and potential. The inherent designing flexibility of thermal sprayed functional and smart coatings has been exploited to explore exciting new possibilities on many different fields. Applications such as anti-bacterial and anti-fouling coatings, superhydrophobic surfaces, electrical and heating devices for functional coatings and self-healing, self-lubricating and sensors for smart coatings are here presented and discussed. All these exciting developments pave the way for the numerous applications that are to come in the next decade, making the field of thermal sprayed coatings a unique opportunity for research.\",\n",
       "  'len': 1928},\n",
       " {'abstract': 'Wire + arc additive manufacture (WAAM) is an attractive method for manufacturing large-scale aerospace components, however the microstructural changes that occur and the effect of interpass rolling are poorly understood. Therefore two fundamental studies were conducted: the first involved temperature measurement of a wrought dummy wall so that the microstructural changes in the heat affected zone (HAZ) could be related to the thermal cycle. This demonstrated that the white band in the microstructure corresponded to 825 C well below the beta-transus temperature and above this boundary the bi-modal substrate material was converted to lamellar. The second involved peening WAAM material along the side of a deposited wall before applying a typical WAAM thermal heat treatment. This showed that refinement occurred up to the first layer band in the microstructure and the smallest grains were observed just above this boundary at higher temperatures significant grain growth occurred. This study has provided the foundational understanding of microstructural changes that will facilitate future process developments.',\n",
       "  'len': 1131},\n",
       " {'abstract': 'The communication range of wireless networks can be greatly improved by using distributed beamforming from a set of independent radio nodes. One of the key challenges in establishing a beamformed communication link from separate radios is achieving carrier frequency and sample timing synchronization. This paper describes an implementation that addresses both carrier frequency and sample timing synchronization simultaneously using RF signaling between designated master and slave nodes. By using a pilot signal transmitted by the master node, each slave estimates and tracks the frequency and timing offset and digitally compensates for them. A real-time implementation of the proposed system was developed in GNU Radio and tested with Ettus USRP N210 software defined radios. The measurements show that the distributed array can reach a residual frequency error of 5 Hz and a residual timing offset of 1/16 the sample duration for 70 percent of the time. This performance enables distributed beamforming for range extension applications.',\n",
       "  'len': 1052},\n",
       " {'abstract': \"The European Spallation Source (ESS) is the world's next generation spallation-based neutron source. The research conducted at ESS will yield in the discovery and development of new materials including the fields of manufacturing, pharmaceuticals, aerospace, engines, plastics, energy, telecommunications, transportation, information technology and biotechnology. The spallation source will deliver an unprecedented neutron flux. In particular, the reflectometers selected for construction, ESTIA and FREIA, have to fulfill challenging requirements. Local incident peak rate can reach 10$^5$~Hz/mm$^2$. For new science to be addressed, the spatial resolution is aimed to be less than 1 mm with a desired scattering of 10$^{-4}$ (peak-to-tail ratio). The latter requirement is approximately two orders of magnitude better than the current state-of-the-art detectors. The main aim of this work is to quantify the cumulative contribution of various detector components to the scattering of neutrons and to prove that the respective effect is within the requirements set for the Multi-Blade detector by the ESS reflectometers. To this end, different sets of geometry and beam parameters are investigated, with primary focus on the cathode coating and the detector window thickness.\",\n",
       "  'len': 1288},\n",
       " {'abstract': 'This paper tackles the problem of finding the optimal non-coherent detector for the reacquisition of weak Global Navigation Satellite System (GNSS) signals in the presence of bits and phase uncertainty. Two solutions are derived based on using two detection frameworks: the Bayesian approach and the generalized likelihood ratio test (GLRT). We also derive approximate detectors of reduced computation complexity and without noticeable performance degradation. Simulation results reveal a clear improvement of the detection probability for the proposed techniques with respect to the conventional detectors implemented in high sensitivity GNSS (HS-GNSS) receivers to acquire weak GNSS signals. Finally, we draw conclusions on which is the best technique to reacquire weak GNSS signals in practice considering a trade-off between performance and complexity.',\n",
       "  'len': 867},\n",
       " {'abstract': 'The Mott corrections to the higher moments of the heavy ion energy-loss distribution are calculated in a wide range of relative particle velocity on the basis of the Mott exact cross section. It is shown that the relative Mott corrections to the first-order Born central moments and normalized central moments reach a large value over the range under consideration.',\n",
       "  'len': 376},\n",
       " {'abstract': 'Wide bandgap semiconductors have become the most attractive materials in optoelectronics in the last decade. Their wide bandgap and intrinsic properties have advanced the development of reliable photodetectors to selectively detect short wavelengths (i.e., ultraviolet, UV) in high temperature regions (up to 300°C). The main driver for the development of high-temperature UV detection instrumentation is in-situ monitoring of hostile environments and processes found within industrial, automotive, aerospace, and energy production systems that emit UV signatures. In this review, a summary of the optical performance (in terms of photocurrent-to-dark current ratio, responsivity, quantum efficiency, and response time) and uncooled, high-temperature characterization of III-nitride, SiC, and other wide bandgap semiconductor UV photodetectors is presented.',\n",
       "  'len': 868},\n",
       " {'abstract': 'On-board estimation of the pose of an uncooperative target spacecraft is an essential task for future on-orbit servicing and close-proximity formation flying missions. However, two issues hinder reliable on-board monocular vision based pose estimation: robustness to illumination conditions due to a lack of reliable visual features and scarcity of image datasets required for training and benchmarking. To address these two issues, this work details the design and validation of a monocular vision based pose determination architecture for spaceborne applications. The primary contribution to the state-of-the-art of this work is the introduction of a novel pose determination method based on Convolutional Neural Networks (CNN) to provide an initial guess of the pose in real-time on-board. The method involves discretizing the pose space and training the CNN with images corresponding to the resulting pose labels. Since reliable training of the CNN requires massive image datasets and computational resources, the parameters of the CNN must be determined prior to the mission with synthetic imagery. Moreover, reliable training of the CNN requires datasets that appropriately account for noise, color, and illumination characteristics expected in orbit. Therefore, the secondary contribution of this work is the introduction of an image synthesis pipeline, which is tailored to generate high fidelity images of any spacecraft 3D model. The proposed technique is scalable to spacecraft of different structural and physical properties as well as robust to the dynamic illumination conditions of space. Through metrics measuring classification and pose accuracy, it is shown that the presented architecture has desirable robustness and scalable properties.',\n",
       "  'len': 1768},\n",
       " {'abstract': 'A Bow-Tie ITO/BST and Au/BST antennas were designed and analyzed using the finite element, Multiphysics COMSOL program. The study shows that the outputs peak at several modes and ITO antennas allow for the integration of the antenna with optoelectronic devices.',\n",
       "  'len': 272},\n",
       " {'abstract': \"Drone racing is becoming a popular sport where human pilots have to control their drones to fly at high speed through complex environments and pass a number of gates in a pre-defined sequence. In this paper, we develop an autonomous system for drones to race fully autonomously using only onboard resources. Instead of commonly used visual navigation methods, such as simultaneous localization and mapping and visual inertial odometry, which are computationally expensive for micro aerial vehicles (MAVs), we developed the highly efficient snake gate detection algorithm for visual navigation, which can detect the gate at 20HZ on a Parrot Bebop drone. Then, with the gate detection result, we developed a robust pose estimation algorithm which has better tolerance to detection noise than a state-of-the-art perspective-n-point method. During the race, sometimes the gates are not in the drone's field of view. For this case, a state prediction-based feed-forward control strategy is developed to steer the drone to fly to the next gate. Experiments show that the drone can fly a half-circle with 1.5m radius within 2 seconds with only 30cm error at the end of the circle without any position feedback. Finally, the whole system is tested in a complex environment (a showroom in the faculty of Aerospace Engineering, TU Delft). The result shows that the drone can complete the track of 15 gates with a speed of 1.5m/s which is faster than the speeds exhibited at the 2016 and 2017 IROS autonomous drone races.\",\n",
       "  'len': 1521},\n",
       " {'abstract': \"In civil, mechanical, and aerospace engineering, structural dynamics is commonly understood to be a discipline concerned with the analysis and characterization of the vibratory response of structures. Key elements of the response are the amplitude, phase, and damping ratio, which are quantities that vary with the excitation frequency. In this paper, we extend the discipline of structural dynamics to the realm of materials engineering by intrinsically building localized substructures within, or attached to, the material domain itself$-$which is viewed as an extended medium without defined external boundaries. Our system is essentially a locally resonant elastic metamaterial, except here it is viewed from the perspective of unique dissipation characteristics rather than subwavelength effective properties or band gaps, as widely done in the literature. We provide a theory, validated by experiments, for substructurally synthesizing the dissipation under the conditions of free-wave motion, i.e., waves not constrained to a prescribed driving frequency. We use an extended elastic beam with attached pillars as an example of a metamaterial. When compared to an identical infinite beam with no attached substructures, we show that within certain frequency ranges the metamaterial exhibits either enhanced or reduced dissipation$-$which we refer to as positive and negative metadamping, respectively. These regimes are rigorously identified and characterized using the metamaterial's band structure and wavenumber-dependent dissipation diagram. This theory impacts applications that require a combination of high stiffness and high damping or, conversely, applications that benefit from a reduction in loss without the need to change the backbone constituent material.\",\n",
       "  'len': 1786},\n",
       " {'abstract': \"The thermal subsystem of the Mars Express (MEX) spacecraft keeps the on-board equipment within its pre-defined operating temperatures range. To plan and optimize the scientific operations of MEX, its operators need to estimate in advance, as accurately as possible, the power consumption of the thermal subsystem. The remaining power can then be allocated for scientific purposes. We present a machine learning pipeline for efficiently constructing accurate predictive models for predicting the power of the thermal subsystem on board MEX. In particular, we employ state-of-the-art feature engineering approaches for transforming raw telemetry data, in turn used for constructing accurate models with different state-of-the-art machine learning methods. We show that the proposed pipeline considerably improve our previous (competition-winning) work in terms of time efficiency and predictive performance. Moreover, while achieving superior predictive performance, the constructed models also provide important insight into the spacecraft's behavior, allowing for further analyses and optimal planning of MEX's operation.\",\n",
       "  'len': 1132},\n",
       " {'abstract': 'Many real-world objects are designed by smooth curves, especially in the domain of aerospace and ship, where aerodynamic shapes (e.g., airfoils) and hydrodynamic shapes (e.g., hulls) are designed. To facilitate the design process of those objects, we propose a deep learning based generative model that can synthesize smooth curves. The model maps a low-dimensional latent representation to a sequence of discrete points sampled from a rational Bézier curve. We demonstrate the performance of our method in completing both synthetic and real-world generative tasks. Results show that our method can generate diverse and realistic curves, while preserving consistent shape variation in the latent space, which is favorable for latent space design optimization or design space exploration.',\n",
       "  'len': 798},\n",
       " {'abstract': 'This paper investigates the problem of aerial vehicle recognition using a text-guided deep convolutional neural network classifier. The network receives an aerial image and a desired class, and makes a yes or no output by matching the image and the textual description of the desired class. We train and test our model on a synthetic aerial dataset and our desired classes consist of the combination of the class types and colors of the vehicles. This strategy helps when considering more classes in testing than in training.',\n",
       "  'len': 536},\n",
       " {'abstract': \"Nature provides many paradigms for the design and fabrication of artificial composite materials. Inspired by the relationship between the well-ordered architecture and biopolymers found in natural nacre, we present a facile strategy to construct large-scale organic/inorganic nacre-mimetics with hierarchical structure via a water-evaporation self-assembly process. Through hydrogen bonding, we connect Laponite-nanoclay platelets with each other using naturally abundant cellulose creating thin, flexible films with a local brick-and-mortar architecture. While the aqueous solution displays liquid crystalline textures, the dried films show a pronounced Young's modulus (9.09 GPa) with a maximum strength of 298.02 MPa and toughness of 16.63 MJm-3. In terms of functionalities, we report excellent glass-like transparency along with exceptional shape-persistent flame shielding. We also demonstrate that through metal ion-coordination we can further strengthen the interactions between the polymers and the nanoclays. These ion-treated hybrid films exhibit further enhanced mechanical, and thermal properties as well as resistance against swelling and dissolution in aqueous environments. We believe that our simple pathway to fabricate such versatile polymer/clay nanocomposites can open avenues for inexpensive production of environmentally friendly, biomimetic materials in aerospace, wearable electrical devices, artificial muscle, and food packaging industry.\",\n",
       "  'len': 1476},\n",
       " {'abstract': \"Attitude computation is of vital importance for a variety of applications. Based on the functional iteration of the Rodrigues vector integration equation, the RodFIter method can be advantageously applied to analytically reconstruct the attitude from discrete gyroscope measurements over the time interval of interest. It is promising to produce ultra-accurate attitude reconstruction. However, the RodFIter method imposes high computational load and does not lend itself to onboard implementation. In this paper, a fast approach to significantly reduce RodFIter's computation complexity is presented while maintaining almost the same accuracy of attitude reconstruction. It reformulates the Rodrigues vector iterative integration in terms of the Chebyshev polynomial iteration. Due to the excellent property of Chebyshev polynomials, the fast RodFIter is achieved by means of appropriate truncation of Chebyshev polynomials, with provably guaranteed convergence. Moreover, simulation results validate the speed and accuracy of the proposed method.\",\n",
       "  'len': 1059},\n",
       " {'abstract': 'Estimating the probability of failures or accidents with aerospace systems is often necessary when new concepts or designs are introduced, as it is being done for Autonomous Aircraft. If the design is safe, as it is supposed to be, accident cases are hard to find. Such analysis needs some variance reduction technique and several algorithms exist for that, however specific model features may cause difficulties in practice, such as the case of system models where independent agents have to autonomously accomplish missions within finite time, and likely with the presence of human agents. For handling these scenarios, this paper presents a novel estimation approach, based on the combination of the well-established variation reduction technique of Interacting Particles System (IPS) with the long-standing optimization algorithm denominated DIviding RECTangles (DIRECT). When combined, these two techniques yield statistically significant results for extremely low probabilities. In addition, this novel approach allows the identification of intermediate events and simplifies the evaluation of sensitivity of the estimated probabilities to certain system parameters.',\n",
       "  'len': 1183},\n",
       " {'abstract': 'Micro-Doppler-based target classification capabilities of the automotive radars can provide high reliability and short latency to the future active safety automotive features. A large number of pedestrians surrounding vehicle in practical urban scenarios mandate prioritization of their treat level. Classification between relevant pedestrians that cross the street or are within the vehicle path and those that are on the sidewalks and move along the vehicle rout can significantly minimize a number of vehicle-to-pedestrian accidents. This work proposes a novel technique for a pedestrian direction of motion estimation which treats pedestrians as complex distributed targets and utilizes their micro-Doppler (MD) radar signatures. The MD signatures are shown to be indicative of pedestrian direction of motion, and the supervised regression is used to estimate the mapping between the directions of motion and the corresponding MD signatures. In order to achieve higher regression performance, the state of the art sparse dictionary learning based feature extraction algorithm was adopted from the field of computer vision by drawing a parallel between the Doppler effect and the video temporal gradient. The performance of the proposed approach is evaluated in a practical automotive scenario simulations, where a walking pedestrian is observed by a multiple-input-multiple-output (MIMO) automotive radar with a 2D rectangular array. The simulated data was generated using the statistical Boulic-Thalman human locomotion model. Accurate direction of motion estimation was achieved by using a support vector regression (SVR) and a multilayer perceptron (MLP) based regression algorithms. The results show that the direction estimation error is less than $10^{\\\\circ}$ in $95\\\\%$ of the tested cases, for pedestrian at the range of $100$m from the radar.',\n",
       "  'len': 1865},\n",
       " {'abstract': 'The problem of multisensor multitarget state estimation in the presence of constant but unknown sensor biases is investigated. The classical approach to this problem is to augment the state vector to include the states of all the targets and the sensor biases, and then implement an augmented state Kalman filter (ASKF). In this paper, we propose a novel decoupled Kalman filtering algorithm. The decoupled Kalman filtering first processes each target in a separate branch, namely the single-target Kalman filtering branch, where the single-target states and the sensor biases are estimated. Then the bias estimate is refined by fusing the former bias estimates across all the single-target Kalman filtering branches. Finally, the refined bias estimate is fed back to each single-target Kalman filtering branch to improve the target state estimation. We prove that the proposed decoupled Kalman filtering is exactly equivalent to the ASKF in terms of the estimation results under a usual initial condition. The equivalence is also confirmed via the numerical example. Moreover, we further validate the proposed algorithm using the field experimental data of a multistatic passive radar.',\n",
       "  'len': 1197},\n",
       " {'abstract': '\"Auxetic\" materials have the counter-intuitive property of expanding rather than contracting perpendicular to an applied stretch, formally they have negative Poisson\\'s Ratios (PRs).[1,2] This results in properties such as enhanced energy absorption and indentation resistance, which means that auxetics have potential for applications in areas from aerospace to biomedical industries.[3,4] Existing synthetic auxetics are all created by carefully structuring porous geometries from positive PR materials. Crucially, their geometry causes the auxeticity.[3,4] The necessary porosity weakens the material compared to the bulk and the structure must be engineered, for example, by using resource-intensive additive manufacturing processes.[1,5] A longstanding goal for researchers has been the development of a synthetic material that has intrinsic auxetic behaviour. Such \"molecular auxetics\" would avoid porosity-weakening and their very existence implies chemical tuneability.[1,4-9] However molecular auxeticity has never previously been proven for a synthetic material.[6,7] Here we present a synthetic molecular auxetic based on a monodomain liquid crystal elastomer (LCE). When stressed perpendicular to the alignment direction, the LCE becomes auxetic at strains greater than approximately 0.8 with a minimum PR of -0.8. The critical strain for auxeticity coincides with the occurrence of a negative liquid crystal order parameter (LCOP). We show the auxeticity agrees with theoretical predictions derived from the Warner and Terentjev theory of LCEs.[10] This demonstration of a synthetic molecular auxetic represents the origin of a new approach to producing molecular auxetics with a range of physical properties and functional behaviours. Further, it demonstrates a novel feature of LCEs and a route for realisation of the molecular auxetic technologies that have been proposed over the years.',\n",
       "  'len': 1913},\n",
       " {'abstract': 'Wireless links are characterized by fluctuating quality leading to variable packet error rates which are orders of magnitude higher than the ones of wired links. Therefore, it is of paramount importance to investigate the limitations of using 5G wireless links for internet of things (IoT) applications. 5G wireless links in IoT need to assure determinism of process flows via realtime communication anytime and anywhere, which is an utmost requirement for multiple verticals like automotive, industrial automation and aerospace. Based on a space-time approach, in this work, we provide novel definitions of wireless link availability and reliability, assuming a number of access points (APs) and end points (EPs) deployed over a fixed area. Our objective is to analyze the availability of a service in both domains. In the space domain, we characterize spatially available areas consisting of all locations that meet a performance requirement with confidence. In the time domain, we propose a channel allocation scheme accounting for the spatial availability of a cell. To emphasize the incurred space-time performance trade-offs, numerical results are presented, also highlighting the effect of different system parameters on the achievable link availability and reliability.',\n",
       "  'len': 1288},\n",
       " {'abstract': 'In computational fluid dynamics simulations of industrial flows, models based on the Reynolds-averaged Navier--Stokes (RANS) equations are expected to play an important role in decades to come. However, model uncertainties are still a major obstacle for the predictive capability of RANS simulations. This review examines both the parametric and structural uncertainties in turbulence models. We review recent literature on data-free (uncertainty propagation) and data-driven (statistical inference) approaches for quantifying and reducing model uncertainties in RANS simulations. Moreover, the fundamentals of uncertainty propagation and Bayesian inference are introduced in the context of RANS model uncertainty quantification. Finally, the literature on uncertainties in scale-resolving simulations is briefly reviewed with particular emphasis on large eddy simulations.',\n",
       "  'len': 884},\n",
       " {'abstract': 'Pyramidal lattice sandwich structure (PLSS) exhibits high stiffness and strength-to-weight ratio which can be effectively utilized for designing light-weight load bearing structures for ranging from ground to aerospace vehicles. While these structures provide superior strength to weigh ratio, their sound insulation capacity has not been well understood. The aim of this study is to develop numerical and experimental methods to fundamentally investigate the sound insulation property of the pyramidal lattice sandwich structure with solid trusses (PLSSST). A finite element model has been developed to predict the sound transmission loss (STL) of PLSSST and simulation results have been compared with those obtained experimentally. Parametric studies is then performed using the validated finite element model to investigate the effect of different parameters in pyramidal lattice sandwich structure with hollow trusses (PLSSHT), revealing that the pitching angle, the uniform thickness and the length of the hollow truss and the lattice constant have considerable effects on the sound transmission loss. Finally a design optimization strategy has been formulated to optimize PLSSHT in order to maximize STL while meeting mechanical property requirements. It has been shown that STL of the optimal PLSSHT can be increased by almost 10% at the low-frequency band. The work reported here provides useful information for the noise reduction design of periodic lattice structures.',\n",
       "  'len': 1489},\n",
       " {'abstract': 'Reynolds-Averaged Navier-Stokes(RANS) method will still play a vital role in the following several decade in aerospace engineering. Although RANS models are widely used, empiricism and large discrepancies between models reduce the reliability of simulating complex flows. Therefore, in recent years, data-driven turbulence model has aroused widespread concern in fluid mechanics. Based on the experimental/numerical simulation results, this approach aims to modify or construct the turbulence model for specific purposes by machine learning technologies. In this paper, we take the results calculated by SA model as training data. Different from low Reynolds number turbulent flows, the data from high Reynolds number flows shows an apparent scaling effect, thus leading to difficulties in the data-driven modeling. In order to improve the fitting accuracy, we divided the flow field into near-wall region, wake region, and far-field region, and built individual model for every region. In this paper, we adopted the radial basis function neural network (RBFNN) and some auxiliary optimization algorithms to reconstruct a mapping function between mean variables and the eddy viscosity. Since this model reflects the relationship between local flow characteristics and turbulent eddy viscosity, it is independent on the airfoil shape and flow condition. The training data in this paper is generated from only three subsonic flow calculations of NACA0012 airfoil. By coupling the proposed approach with N-S equations, we calculated various flow cases as well as two different airfoils and showed the eddy viscosity contours, velocity profiles along the normal direction of wall and skin friction coefficient distributions, etc. Compared with the SA model, the results show a reasonable accuracy and better efficiency, which indicates the positive prospect of data-driven methods in turbulence modeling.',\n",
       "  'len': 1911},\n",
       " {'abstract': 'Due to the increasing demand for high performance and cost reduction within the framework of complex system design, numerical optimization of computationally costly problems is an increasingly popular topic in most engineering fields. In this paper, several variants of the Efficient Global Optimization algorithm for costly constrained problems depending simultaneously on continuous decision variables as well as on quantitative and/or qualitative discrete design parameters are proposed. The adaptation that is considered is based on a redefinition of the Gaussian Process kernel as a product between the standard continuous kernel and a second kernel representing the covariance between the discrete variable values. Several parameterizations of this discrete kernel, with their respective strengths and weaknesses, are discussed in this paper. The novel algorithms are tested on a number of analytical test-cases and an aerospace related design problem, and it is shown that they require fewer function evaluations in order to converge towards the neighborhoods of the problem optima when compared to more commonly used optimization algorithms.',\n",
       "  'len': 1160},\n",
       " {'abstract': 'Future space missions will be driven by factors such as the need for reduced cost of spacecraft without diminished performance, new services and capabilities including reconfigurability, autonomous operations, target observation with improved resolution and servicing (or proximity) operations. Small satellites, deployed as a sensor network in space, can through inter-satellite communication (ISC) enable the realization of these future goals. Developing the communication subsystem that can facilitate ISC within this distributed network of small satellites require a complex range of design trade-offs. For small satellites, the general design parameters that are to be optimized for ISC are size, mass, and power, as well as cost (SMaP-C). Novel and efficient design techniques for implementing the communication subsystem are crucial for building multiple small satellite networks with capability for achieving significant data-rates along the inter-satellite links (ISLs). In this paper, we propose an alternative approach to RF and laser ISLs for ISC among small satellites deployed as a sensor network in low Earth orbit (LEO). For short to medium range ISLs, we present an LED-based visible light communication (VLC) system that addresses the SMaP constraints, including capability for achieving significant data rates. Our research is focused on the development of the physical layer for pico-/nano class of satellites with prime consideration for the impact of solar background illumination on link performance. We develop an analytical model of the inter-satellite link (ISL) in MATLAB and evaluate its feasibility and performance for different intensity modulation and direct detection (IM/DD) schemes.',\n",
       "  'len': 1727},\n",
       " {'abstract': 'CCD was born in Bell Laboratories in 1969 and has been widely used in various fields. Its ultra-low noise and high quantum efficiency make it work well in particle physics, high energy physics, nuclear physics and astrophysics. Nowadays, more and more CCD cameras have been developed for medical diagnosis, scientific experiments, aerospace, military exploration and other fields. For the wide range of CCD cameras, a Non-vacuum-cooling compact (NVCC) scientific CCD camera has been developed, including FPGA-based low noise clock and bias driver circuit, data acquisition circuit, STM32-based temperature control design. At the same time, the readout noise of the imaging system is studied emphatically. The scheme to generate the CCD clock and the bias driving circuit through ultralow noise LDOs is proposed. The camera was tested in a variety of environments, and the test results show that the system can run at a maximum rate of 5M pixels/s and readout noise is as low as 9.29e^- when the CCD readout speed is 500K pixels/s. Finally, a series of stability tests were carried out on the camera system.',\n",
       "  'len': 1117},\n",
       " {'abstract': 'Graph-based design languages in UML (Unified Modeling Language) are presented as a method to encode and automate the complete design process and the final optimization of the product or complex system. A design language consists of a vocabulary (digital building blocks) and a set of rules (digital composition knowledge) along with an executable sequence of the rules (digital encoding of the design process). The rule-based mechanism instantiates a central and consistent global product data structure (the so-called design graph). Upon the generation of the abstract central model, the domain-specific engineering models are automatically generated, remotely executed and their results are fed-back into the central design model for subsequent design decisions or optimizations. The design languages are manually modeled and automatically executed in a so-called design compiler. Up to now, a variety of product designs in the areas of aerospace, automotive, machinery and consumer products have been successfully accelerated and automated using graph-based design languages. Different design strategies and mechanisms have been identified and applied in the automation of the design processes. Approaches ranging from the automated and declarative processing of constraints, through fractal nested design patterns, to mathematical dimension-based derivation of the sequence of design actions are used. The existing knowledge for a design determines the global design strategy (top-down vs. bottom-up). Similarity-mechanics in the form of dimensionless invariants are used for evaluation to downsize the solution for an overall complexity reduction. Design patterns, design paradigms (form follows function) and design strategies (divide and conquer) from information science are heavily used to structure, manage and handle complexity.',\n",
       "  'len': 1850},\n",
       " {'abstract': 'We study the problem of mixing between core and annular flow in a pipe, examining the effect of a swirling core flow. Such flows are important across a range of applications, including jet pumps, combustion chambers and aerospace engineering. Previous studies show that swirl can increase shear layer growth rates and, in the case of confining walls, reduce flow separation. However, the effect of swirl on pressure loss in a confined flow is uncertain. To address this, we develop a simplified model that approximates the axial flow profile as a linear shear layer separating uniform-velocity core and annular streams. The azimuthal flow profile is approximated as a solid body rotation within the core region, and a parabolic mixing profile within the shear layer. This model shows good agreement with computational turbulence modelling, whilst its simplicity and low computational cost make it ideal for benchmark predictions and design purposes. Using this model, we confirm that a swirling core is useful for increasing shear layer growth rates, but find that it is detrimental to pressure recovery. This has important implications for the design of diffusers that incorporate swirling flows. We use the model to describe the slow recirculation region that can form along the pipe axis for sufficiently large swirl, by approximating it as a stagnant zone with zero velocity. The criteria for the development of such a region are established in terms of the pipe expansion angle and inflow velocity profile.',\n",
       "  'len': 1522},\n",
       " {'abstract': 'Landing-gear noise is an increasing issue for transport aircraft. A key determinant of the phenomenon is the surface pressure field. Previous studies have described this field when the oncoming flow is perfectly aligned with the gear. In practice, there may be a cross-flow component; here its influence is investigated experimentally for a generic, two-wheel, landing-gear model. It is found that yaw angles as small as 5° cause significant changes in both overall flow topology and unsteady surface pressures. Most notably, on the outboard face of the leeward wheel, large-scale separation replaces predominantly attached flow behind a leading-edge separation bubble. The effect on unsteady surface pressures includes marked shifts in the content at frequencies in the audible range, implying that yaw is an important parameter for landing-gear noise, and that purely unyawed studies may not be fully representative of the problem.',\n",
       "  'len': 944},\n",
       " {'abstract': 'A fault detection and isolation method for satellite rate gyros is proposed based on using the satellite-to-satellite measurements such as relative position beside orbit parameters of the primary satellite. By finding a constant of motion, it is shown that the dynamic states in a relative motion are restricted in such a way that the angular velocity vector of primary satellite lies on a quadratic surface. This constant of motion is then used to detect the gyroscope faults and estimate the corresponding scale factor or bias values of the rate gyros of the primary satellite. The proposed algorithm works even in time variant fault situations as well, and does not impose any additional subsystems to formation flying satellites. Monte-Carlo simulations are used to ensure that the algorithm retains its performance in the presence of uncertainties. In presence of only measurement noise, the isolation process performs well by selecting a proper threshold. However, the isolation performance degrades as the scale factor approaches unity or bias approaches zero. Finally, the effect of orbital perturbations on isolation process is investigated by including the effect of zonal harmonics as well as drag and without loss of generality, it is shown that the perturbation effects are negligible.',\n",
       "  'len': 1309},\n",
       " {'abstract': 'The advantages introduced by carbon fiber reinforced polymer (CFRP) composites has made them an appropriate choice in many applications and an ideal replacement for conventional materials. The benefits using CFRP composites are due to their lightweight, high stiffness, as well as corrosion resistance. For this reason, there is a fast growing trend in using CFRP composites for aircraft and wind turbine structural applications. The replacement of the conventional aerospace-grade metal alloys (aluminum, titanium, magnesium, etc.) with CFRP composites results in new challenges. For example, an aircraft during flight is prone to be struck by lightning. To withstand the injection of such massive amount of energy, adequate electrical properties, mainly electrical conductivity, is required. In fact, electrical conductance (or its reciprocal, resistance) is a critical parameter representing any material change and it can be considered an index for health monitoring. In this paper, AS4/8552 carbon/epoxy laminated composites were injected with two types of electrical currents, impulse current and direct current. The change in measured electrical resistance was recorded. A significant resistance drop occurred after electrical current injections. Furthermore, four-point flexural tests were performed on these composites to correlate an electrical resistance change with a potential flexural property change. There was no clear trend between a resistance change and flexural strength/modulus change of the test coupons, regardless of current injection. However, it was observed that the injection of the current affects the contact resistance such that its resistance decreases.',\n",
       "  'len': 1696},\n",
       " {'abstract': 'The usage of complex Microcontroller Units (MCUs) in avionic systems constitutes a challenge in assuring their safety. They are not developed according to the development requirements accepted by the aerospace industry. These Commercial off-the-shelf (COTS) hardware components usually target other domains like the telecommunication branch. In the last years MCUs developed in compliance to the ISO 26262 have been released on the market for safety-related automotive applications. The avionic assurance process could profit from these safety MCUs. In this paper we present evaluation results based on the current assurance practice that demonstrates expected assurance activities benefit from ISO 26262 compliant MCUs.',\n",
       "  'len': 731},\n",
       " {'abstract': 'Research on automated vehicles has experienced an explosive growth over the past decade. A main obstacle to their practical realization, however, is a convincing safety concept. This question becomes ever more important as more sophisticated algorithms are used and the vehicle automation level increases. The field of functional safety offers a systematic approach to identify possible sources of risk and to improve the safety of a vehicle. It is based on practical experience across the aerospace, process and other industries over multiple decades. This experience is compiled in the functional safety standard for the automotive domain, ISO 26262, which is widely adopted throughout the automotive industry. However, its applicability and relevance for highly automated vehicles is subject to a controversial debate. This paper takes a critical look at the discussion and summarizes the main steps of ISO 26262 for a safe control design for automated vehicles.',\n",
       "  'len': 976},\n",
       " {'abstract': \"A GPS-denied UAV (Agent B) is localised through INS alignment with the aid of a nearby GPS-equipped UAV (Agent A), which broadcasts its position at several time instants. Agent B measures the signals' direction of arrival with respect to Agent B's inertial navigation frame. Semidefinite programming and the Orthogonal Procrustes algorithm are employed, and accuracy is improved through maximum likelihood estimation. The method is validated using flight data and simulations. A three-agent extension is explored.\",\n",
       "  'len': 524},\n",
       " {'abstract': 'Asteroid (162173) Ryugu is the target object of Hayabusa2, an asteroid exploration and sample return mission led by Japan Aerospace Exploration Agency (JAXA). Ground-based observations indicate that Ryugu is a C-type near-Earth asteroid with a diameter of less than 1 km, but the knowledge of its detailed properties is still very limited. This paper summarizes our best understanding of the physical and dynamical properties of Ryugu based on remote sensing and theoretical modeling. This information is used to construct a design reference model of the asteroid that is used for formulation of mission operations plans in advance of asteroid arrival. Particular attention is given to the surface properties of Ryugu that are relevant to sample acquisition. This reference model helps readers to appropriately interpret the data that will be directly obtained by Hayabusa2 and promotes scientific studies not only for Ryugu itself and other small bodies but also for the Solar System evolution that small bodies shed light on.',\n",
       "  'len': 1038},\n",
       " {'abstract': 'In this paper, a novel image moments based model for shape estimation and tracking of an object moving with a complex trajectory is presented. The camera is assumed to be stationary looking at a moving object. Point features inside the object are sampled as measurements. An ellipsoidal approximation of the shape is assumed as a primitive shape. The shape of an ellipse is estimated using a combination of image moments. Dynamic model of image moments when the object moves under the constant velocity or coordinated turn motion model is derived as a function for the shape estimation of the object. An Unscented Kalman Filter-Interacting Multiple Model (UKF-IMM) filter algorithm is applied to estimate the shape of the object (approximated as an ellipse) and track its position and velocity. A likelihood function based on average log-likelihood is derived for the IMM filter. Simulation results of the proposed UKF-IMM algorithm with the image moments based models are presented that show the estimations of the shape of the object moving in complex trajectories. Comparison results, using intersection over union (IOU), and position and velocity root mean square errors (RMSE) as metrics, with a benchmark algorithm from literature are presented. Results on real image data captured from the quadcopter are also presented.',\n",
       "  'len': 1338},\n",
       " {'abstract': \"Beginning in 2012, NASA utilized a strategic process to identify broad societal questions, or grand challenges, that are well suited to the aerospace sector and align with national priorities. This effort generated NASA's first grand challenge, the Asteroid Grand Challenge, a large scale effort using multidisciplinary collaborations and innovative engagement mechanisms focused on finding and addressing asteroid threats to human populations. In April 2010, President Barack Obama announced a mission to send humans to an asteroid by 2025. This resulted in the agency's Asteroid Redirect Mission to leverage and maximize existing robotic and human efforts to capture and reroute an asteroid, with the goal of eventual human exploration. The AGC, initiated in 2013, complemented ARM by expanding public participation, partnerships, and other approaches to find, understand, and overcome these potentially harmful asteroids. This paper describes a selection of AGC activities implemented from 2013 to 2017 and their results, excluding those conducted by NASA's Near Earth Object Observations Program and other organizations. The strategic development of the initiative is outlined as well as initial successes, strengths, and weaknesses resulting from the first four years of AGC activities and approaches. Finally, we describe lesson learned and areas for continued work and study. The AGC lessons learned and strategies could inform the work of other agencies and organizations seeking to conduct a global scientific investigation with matrixed organizational support, multiple strategic partners, and numerous internal and external open innovation approaches and audiences.\",\n",
       "  'len': 1687},\n",
       " {'abstract': 'We consider a land mobile satellite communication system using spread spectrum techniques where the uplink is exposed to MT jamming attacks, and the downlink is corrupted by multi-path fading channels. We proposes an anti-jamming receiver, which exploits inherent low-dimensionality of the received signal model, by formulating a robust principal component analysis (Robust PCA)-based recovery problem. Simulation results verify that the proposed receiver outperforms the conventional receiver for a reasonable rank of the jamming signal.',\n",
       "  'len': 549},\n",
       " {'abstract': \"Laser altimetry is a powerful tool for addressing the major objectives of planetary physics and geodesy, and have been applied in planetary explorations of the Moon, Mars, Mercury, and the asteroids Eros, and Itokawa. The JUpiter Icy Moons Explorer (JUICE), led by European Space Agency (ESA), has started development to explore the emergence of habitable worlds around gas giants. The Ganymede Laser Altimeter (GALA) will be the first laser altimeter for icy bodies, and will measure the shape and topography of the large icy moons of Jupiter, (globally for Ganymede, and using flyby ground-tracks for Europa and Callisto). Such information is crucial for understanding the formation of surface features and can tremendously improve our understanding of the icy tectonics. In addition, the GALA will infer the presence or absence of a subsurface ocean by measuring the tidal and rotational responses. Furthermore, it also improves the accuracy of gravity field measurements reflecting the interior structure, collaborating with the radio science experiment. In addition to range measurements, the signal strength and the waveform of the laser pulses reflected from the moon's surface contain information about surface reflectance at the laser wavelength and small scale roughness. Therefore we can infer the degrees of chemical and physical alterations, e.g., erosion, space weathering, compaction and deposition of exogenous materials, through GALA measurements without being affected by illumination conditions. JUICE spacecraft carries ten science payloads including GALA. They work closely together in a synergistic way with GALA being one of the key instruments for understanding the evolution of the icy satellites Ganymede, Europa, and Callisto.\",\n",
       "  'len': 1764},\n",
       " {'abstract': 'With the advent of improved computational resources, aerospace design has testing-based process to a simulation-driven procedure, wherein uncertainties in design and operating conditions are explicitly accounted for in the design under uncertainty methodology. A key source of such uncertainties in design are the closure models used to account for fluid turbulence. In spite of their importance, no reliable and extensively tested modules are available to estimate this epistemic uncertainty. In this article, we outline the EQUiPS uncertainty estimation module developed for the SU2 CFD suite that focuses on uncertainty due to turbulence models. The theoretical foundations underlying this uncertainty estimation and its computational implementation are detailed. Thence, the performance of this module is presented for a range of test cases, including both benchmark problems and flows relevant to aerospace design. Across the range of test cases, the uncertainty estimates of the module were able to account for a significant portion of the discrepancy between RANS predictions and high fidelity data.',\n",
       "  'len': 1117},\n",
       " {'abstract': \"This paper describes a novel communication-spare cooperative localization algorithm for a team of mobile unmanned robotic vehicles. Exploiting an event-based estimation paradigm, robots only send measurements to neighbors when the expected innovation for state estimation is high. Since agents know the event-triggering condition for measurements to be sent, the lack of a measurement is thus also informative and fused into state estimates. The robots use a Covariance Intersection (CI) mechanism to occasionally synchronize their local estimates of the full network state. In addition, heuristic balancing dynamics on the robots' CI-triggering thresholds ensure that, in large diameter networks, the local error covariances remains below desired bounds across the network. Simulations on both linear and nonlinear dynamics/measurement models show that the event-triggering approach achieves nearly optimal state estimation performance in a wide range of operating conditions, even when using only a fraction of the communication cost required by conventional full data sharing. The robustness of the proposed approach to lossy communications, as well as the relationship between network topology and CI-based synchronization requirements, are also examined.\",\n",
       "  'len': 1270},\n",
       " {'abstract': 'A recent trend in distributed multi-sensor fusion is to use random finite set filters at the sensor nodes and fuse the filtered distributions algorithmically using their exponential mixture densities (EMDs). Fusion algorithms which extend the celebrated covariance intersection and consensus based approaches are such examples. In this article, we analyse the variational principle underlying EMDs and show that the EMDs of finite set distributions do not necessarily lead to consistent fusion of cardinality distributions. Indeed, we demonstrate that these inconsistencies may occur with overwhelming probability in practice, through examples with Bernoulli, Poisson and independent identically distributed (IID) cluster processes. We prove that pointwise consistency of EMDs does not imply consistency in global cardinality and vice versa. Then, we redefine the variational problems underlying fusion and provide iterative solutions thereby establishing a framework that guarantees cardinality consistent fusion.',\n",
       "  'len': 1025},\n",
       " {'abstract': \"On February 6th, 2018 SpaceX launched a Tesla Roadster on a Mars-crossing orbit. We perform N-body simulations to determine the fate of the object over the next 15 Myr. The orbital evolution is initially dominated by close encounters with the Earth. While a precise orbit can not be predicted beyond the next several centuries due to these repeated chaotic scatterings, one can reliably predict the long-term outcomes by statistically analyzing a large suite of possible trajectories with slightly perturbed initial conditions. Repeated gravitational scatterings with Earth lead to a random walk. Collisions with the Earth, Venus and the Sun represent primary sinks for the Roadster's orbital evolution. Collisions with Mercury and Mars, or ejections from the Solar System by Jupiter, are highly unlikely. We calculate a dynamical half-life of the Tesla of approximately 15 Myr, with some 22%, 12% and 12% of Roadster orbit realizations impacting the Earth, Venus, and the Sun within one half-life, respectively. Because the eccentricities and inclinations in our ensemble increase over time due to mean-motion and secular resonances, the impact rates with the terrestrial planets decrease beyond a few million years, whereas the impact rate on the Sun remains roughly constant.\",\n",
       "  'len': 1289},\n",
       " {'abstract': \"Over the last decades quaternions have become a crucial and very successful tool for attitude representation in robotics and aerospace. However, there is a major problem that is continuously causing trouble in practice when it comes to exchanging formulas or implementations: there are two quaternion multiplications in common use, Hamilton's original multiplication and its flipped version, which is often associated with NASA's Jet Propulsion Laboratory. We believe that this particular issue is completely avoidable and only exists today due to a lack of understanding. This paper explains the underlying problem for the popular passive world to body usage of rotation quaternions, and derives an alternative solution compatible with Hamilton's multiplication. Furthermore, it argues for entirely discontinuing the flipped multiplication. Additionally, it provides recipes for efficiently detecting relevant conventions and migrating formulas or algorithms between them.\",\n",
       "  'len': 984},\n",
       " {'abstract': 'Low-density, highly porous graphene/graphene oxide (GO) based-foams have shown high performance in energy absorption applications, even under high compressive deformations. In general, foams are very effective as energy dissipative materials and have been widely used in many areas such as automotive, aerospace and biomedical industries. In the case of graphene-based foams, the good mechanical properties are mainly attributed to the intrinsic graphene and/or GO electronic and mechanical properties. Despite the attractive physical properties of graphene/GO based-foams, their structural and thermal stabilities are still a problem for some applications. For instance, they are easily degraded when placed in flowing solutions, either by the collapsing of their layers or just by structural disintegration into small pieces. Recently, a new and scalable synthetic approach to produce low-density 3D macroscopic GO structure interconnected with polydimethylsiloxane (PDMS) polymeric chains (pGO) was proposed. A controlled amount of PDMS is infused into the freeze-dried foam resulting into a very rigid structure with improved mechanical properties, such as tensile plasticity and toughness. The PDMS wets the graphene oxide sheets and acts like a glue bonding PDMS and GO sheets. In order to obtain further insights on mechanisms behind the enhanced mechanical pGO response we carried out fully atomistic molecular dynamics (MD) simulations. Based on MD results, we build up a structural model that can explain the experimentally observed mechanical behavior.',\n",
       "  'len': 1574},\n",
       " {'abstract': \"Gaussian process priors are commonly used in aerospace design for performing Bayesian optimization. Nonetheless, Gaussian processes suffer two significant drawbacks: outliers are a priori assumed unlikely, and the posterior variance conditioned on observed data depends only on the locations of those data, not the associated sample values. Student's-T processes are a generalization of Gaussian processes, founded on the Student's-T distribution instead of the Gaussian distribution. Student's-T processes maintain the primary advantages of Gaussian processes (kernel function, analytic update rule) with additional benefits beyond Gaussian processes. The Student's-T distribution has higher Kurtosis than a Gaussian distribution and so outliers are much more likely, and the posterior variance increases or decreases depending on the variance of observed data sample values. Here, we describe Student's-T processes, and discuss their advantages in the context of aerospace optimization. We show how to construct a Student's-T process using a kernel function and how to update the process given new samples. We provide a clear derivation of optimization-relevant quantities such as expected improvement, and contrast with the related computations for Gaussian processes. Finally, we compare the performance of Student's-T processes against Gaussian process on canonical test problems in Bayesian optimization, and apply the Student's-T process to the optimization of an aerostructural design problem.\",\n",
       "  'len': 1512},\n",
       " {'abstract': 'SRAM-based FPGAs are popular in the aerospace industry for their field programmability and low cost. However, they suffer from cosmic radiation-induced Single Event Upsets (SEUs). Triple Modular Redundancy (TMR) is a well-known technique to mitigate SEUs in FPGAs that is often used with another SEU mitigation technique known as configuration scrubbing. Traditional TMR provides protection against a single fault at a time, while partitioned TMR provides improved reliability and availability. In this paper, we present a methodology to analyze TMR partitioning at early design stage using probabilistic model checking. The proposed formal model can capture both single and multiple-cell upset scenarios, regardless of any assumption of equal partition sizes. Starting with a high-level description of a design, a Markov model is constructed from the Data Flow Graph (DFG) using a specified number of partitions, a component characterization library and a user defined scrub rate. Such a model and exhaustive analysis captures all the considered failures and repairs possible in the system within the radiation environment. Various reliability and availability properties are then verified automatically using the PRISM model checker exploring the relationship between the scrub frequency and the number of TMR partitions required to meet the design requirements. Also, the reported results show that based on a known voter failure rate, it is possible to find an optimal number of partitions at early design stages using our proposed method.',\n",
       "  'len': 1554},\n",
       " {'abstract': 'This paper discusses translation and attitude control in spacecraft rendezvous and soft docking. The target spacecraft orbit can be either circular or elliptic. The high fidelity model for this problem is intrinsically a nonlinear system but can be viewed as a linear time varying system (LTV). Therefore, a model predictive control (MPC) based design is proposed to deal with the time-varying feature of the problem. A robust pole assignment method is used in the MPC-based design because of the following merits and/or considerations: (a) no overshoot of the relative position and attitude between the target and the chaser to achieve soft docking by placing all closed-loop poles in the negative real axis of the complex plan, which avoids oscillation of the relative position and attitude, in particular, in the final stage, (b) fast on-line computation, (c) modeling error tolerance, and (d) disturbance rejection. We will discuss these considerations and merits, and use a design simulation to demonstrate that the desired performance is indeed achieved.',\n",
       "  'len': 1071},\n",
       " {'abstract': 'An upcoming industrial IoT revolution, supposedly led by the introduction of embedded sensing and computing, seamless communication and massive data analytics within industrial processes [1], seems unquestionable today. Multiple technologies are being developed, and huge marketing efforts are being made to position solutions in this industrial landscape. However, we have observed that industrial wireless technologies are hardly being adopted by the manufacturing industry. In this article, we try to understand the reasons behind this current lack of wireless technologies adoption by means of conducting visits to the manufacturing industry and interviews with the maintenance and engineering teams in these industries. The manufacturing industry is very diverse and specialized, so we have tried to cover some of the most representative cases: the automotive sector, the pharmaceutical sector (blistering), machine-tool industries (both consumer and aerospace sectors) and robotics. We have analyzed the technology of their machinery, their application requirements and restrictions, and identified a list of obstacles for wireless technology adoption. The most immediate obstacles we have found are the need to strictly follow standards and certifications processes, as well as their prudence. But the less obvious and perhaps even more limiting obstacles are their apparent lack of concern regarding low energy consumption or cost which, in contrast, are believed to be of utmost importance by wireless researchers and practitioners. In this reality-check article, we analyze the causes of this different perception, we identify these obstacles and devise complementary paths to make wireless adoption by the industrial manufacturing sector a reality in the coming years.',\n",
       "  'len': 1790},\n",
       " {'abstract': 'In this work, methods for the evaluation of LES-quality and LES-accuracy are presented, which include the Lyapunov exponent for the analysis of short-time predictability of LES-calculation and the Wasserstein metric for the quantitative assessment of simulation results. Both methods are derived and evaluated in application to the Volvo test case. Both the non-reacting and reacting cases are calculated. For the non- reacting cases, good agreement with the experimental data is achieved by solvers at high numerical resolution. The reacting cases are more challenging due to the small length scale of the flame and the suppression of sinuous mode of absolute instability by the density ratio. The analysis of the turbulent simulation data using the concept of the Lyapunov exponent and the Wasserstein metric provides a more quantitative approach to assess the mesh dependency of the simulation results. The convergence of the Lyapunov exponent is shown to be a more sensitive and stronger indication of mesh-independence. Though grid convergence for the reacting cases cannot be reached with the chosen resolutions, the Lyapunov exponents and the Wasserstein metric are shown to be capable of identifying quantity-specific sensitivities with respect to the numerical resolution, while requiring significantly less computational resources than acquiring profiles of conventional turbulent statistics.',\n",
       "  'len': 1413},\n",
       " {'abstract': 'The aim of this article is to present a comprehensive methodology for the verification of computational fluid dynamics (CFD) solvers with a special attention to aspects pertinent to discretizations with orders of accuracy (OOAs) higher than two. The method of manufactured solutions (MMS) is adopted and a series of manufactured solutions (MSs) is introduced that examines various components of CFD solvers for free flows (not bounded by walls), including inviscid, laminar and turbulent problems when the latter are modelled by the Reynolds-averaged Navier-Stokes (RANS) equations. The treatment of curved elements is also examined. These MSs are furthermore conceived with demonstrated suitability for the verification of OOAs up to the sixth. Each MS is as well utilized to discuss salient aspects useful to the code verification methodology such as the relative qualities of the most useful norms in measuring the discretization error, the sensitivity analysis of the verification process to forcing function terms, the relation between residual minimization and discretization error convergence in iterative solutions and finally the sensitivity of high-order discretizations to grid stretching and self-similarity. Furthermore, scripts and code are provided as accompanying material to assist the interested reader in reproducing the verification results of each manufactured solution (MS).',\n",
       "  'len': 1407},\n",
       " {'abstract': 'Position-sensitive-detectors (PSDs) based on lateral photoeffect have been widely used in diverse applications, including optical engineering, aerospace and military fields. With increasing demands in long working distance, low energy consumption, and weak signal sensing systems, the poor responsivity of conventional Silicon-based PSDs has become a bottleneck limiting their applications. Herein, we propose a high-performance passive PSD based on graphene-Si heterostructure. The graphene is adapted as a photon absorbing and charge separation layer working together with Si as a junction, while the high mobility provides promising ultra-long carrier diffusion length and facilitates large active area of the device. A PSD with working area of 8 mm x 8 mm is demonstrated to present excellent position sensitivity to weak light at nWs level (much better than the limit of ~{\\\\mu}Ws of Si p-i-n PSDs). More importantly, it shows very fast response and low degree of non-linearity of ~3%, and extends the operating wavelength to the near infrared (IR) region (1319 and 1550 nm). This work therefore provides a new strategy for high performance and broadband PSDs.',\n",
       "  'len': 1175},\n",
       " {'abstract': 'Materials comprising carbon nanotube (CNT) aligned nanowire (NW) polymer nanocomposites (A-PNCs) are emerging as next-generation materials for use in aerospace structures. Enhanced operating regimes, such as operating temperatures, motivate the study of CNT aligned NW ceramic matrix nanocomposites (A-CMNCs). Here we report the synthesis of CNT A-CMNCs through the pyrolysis of CNT A-PNC precursors, thereby creating carbon matrix CNT A-CMNCs. Characterization reveals that the fabrication of high strength, high temperature, lightweight next-generation aerospace materials is possible using this method. Additional characterization and modeling are planned.',\n",
       "  'len': 670},\n",
       " {'abstract': \"We propose an extension to the so-called PD detector. The PD detector jointly monitors received power and correlation profile distortion to detect the presence of GNSS carry-off-type spoofing, jamming, or multipath. We show that classification performance can be significantly improved by replacing the PD detector's symmetric-difference-based distortion measurement with one based on the post-fit residuals of the maximum-likelihood estimate of a single-signal correlation function model. We call the improved technique the PD-ML detector. In direct comparison with the PD detector, the PD-ML detector exhibits improved classification accuracy when tested against an extensive library of recorded field data. In particular, it is (1) significantly more accurate at distinguishing a spoofing attack from a jamming attack, (2) better at distinguishing multipath-afflicted data from interference-free data, and (3) less likely to issue a false alarm by classifying multipath as spoofing. The PD-ML detector achieves this improved performance at the expense of additional computational complexity.\",\n",
       "  'len': 1105},\n",
       " {'abstract': 'Silver (Ag) coatings have been widely used in many industry areas due to their excellent conductivity. However, wider applications of Ag coatings have been hindered by their poor mechanical properties. In this research, to improve the mechanical performance, Ag-Bi nano-composite coatings were prepared by a novel ionic co-discharge method. A systematic study of the microstructure, mechanical properties, electrical conductivity and antibacterial behavior of the resulting coating was performed. The results indicated that after adding an appropriate amount of Bi containing solution into the Ag plating solution, Ag-Bi nano-particles were in-situ formed and distributed uniformly throughout the coating matrix, resulting in a significant improvement in the mechanical properties. The hardness of Ag-Bi coating was increased by 60% compared to that of the pure Ag coating. The corrosion resistance of Ag-Bi coatings was also enhanced. The outcome of this research may find a broader application in electronics, jewelry, aerospace and other industries.',\n",
       "  'len': 1063},\n",
       " {'abstract': 'The resource management of a phase array system capable of multiple target tracking and surveillance is critical for the realization of its full potential. Present work aims to improve the performance of an existing method, time-balance scheduling, by establishing an analogy with a well-known stochastic control problem, the machine replacement problem. With the suggested policy, the scheduler can adapt to the operational scenario without a significant sacrifice from the practicality of the time-balance schedulers. More specifically, the numerical experiments indicate that the schedulers directed with the suggested policy can successfully trade the unnecessary track updates, say of non-maneuvering targets, with the updates of targets with deteriorating tracks, say of rapidly maneuvering targets, yielding an overall improvement in the tracking performance.',\n",
       "  'len': 877},\n",
       " {'abstract': 'We propose a novel consensus notion, called \"partial consensus\", for distributed GM-PHD (Gaussian mixture probability hypothesis density) fusion based on a peer-to-peer (P2P) sensor network, in which only highly-weighted posterior Gaussian components (GCs) are disseminated in the P2P communication for fusion while the insignificant GCs are not involved. The partial consensus does not only enjoy high efficiency in both network communication and local fusion computation, but also significantly reduces the affect of potential false data (clutter) to the filter, leading to increased signal-to-noise ratio at local sensors. Two \"conservative\" mixture reduction schemes are advocated for fusing the shared GCs in a fully distributed manner. One is given by pairwise averaging GCs between sensors based on Hungarian assignment and the other is merging close GCs based a new GM merging scheme. The proposed approaches have a close connection to the conservative fusion approaches known as covariance union and arithmetic mean density. In parallel, average consensus is sought on the cardinality distribution (namely the GM weight sum) among sensors. Simulations for tracking either a single target or multiple targets that simultaneously appear are presented based on a sensor network where each sensor operates a GM-PHD filter, in order to compare our approaches with the benchmark generalized covariance intersection approach. The results demonstrate that the partial, arithmetic average, consensus outperforms the complete, geometric average, consensus.',\n",
       "  'len': 1566},\n",
       " {'abstract': 'Accurate mathematical models of aerodynamic properties play an important role in the aerospace field. In some cases, system parameters of an aircraft can be estimated reliably only via flight tests. In order to obtain meaningful experimental data, the aircraft dynamics need to be excited via suitable maneuvers. In this paper, optimal maneuvers are obtained for an autonomous aircraft by solving a time domain model-based optimum experimental design problem that aims to obtain more accurate parameter estimates while enforcing safety constraints.The optimized inputs are compared with respect to conventional maneuvers widely used in the aerospace field and tested within real experiments.',\n",
       "  'len': 702},\n",
       " {'abstract': 'The rich structures arising from the impingement dynamics of water drops onto solid substrates at high velocities are investigated numerically. Current methodologies in the aircraft industry estimating water collection on aircraft surfaces are based on particle trajectory calculations and empirical extensions thereof in order to approximate the complex fluid-structure interactions. We perform direct numerical simulations (DNS) using the volume-of-fluid method in three dimensions, for a collection of drop sizes and impingement angles. The high speed background air flow is coupled with the motion of the liquid in the framework of oblique stagnation-point flow. Qualitative and quantitative features are studied in both pre- and post-impact stages. One-to-one comparisons are made with experimental data available from the investigations of Sor et al. (Journal of Aircraft 52 (6), pp. 1838-1846, 2015), while the main body of results is created using parameters relevant to flight conditions with droplet sizes in the ranges from tens to several hundreds of microns, as presented by Papadakis et al. (AIAA Aerospace Sciences Meeting and Exhibit 0565, pp. 1-40, 2004). Drop deformation, collision, coalescence and microdrop ejection and dynamics, all typically neglected or empirically modelled, are accurately accounted for. In particular, we identify new morphological features in regimes below the splashing threshold in the modelled conditions. We then expand on the variation in the number and distribution of ejected microdrops as a function of the impacting drop size beyond this threshold. The presented model of drop impact addresses key questions at a fundamental level, however the conclusions of the study extend towards the advancement of understanding of water dynamics on aircraft surfaces, which has important implications in terms of compliance to aircraft safety regulations.',\n",
       "  'len': 1908},\n",
       " {'abstract': 'This paper proposes a feedback guidance law to move the instantaneous impact point (IIP) of a rocket to a desired location. Analytic expressions relating the time derivatives of an IIP with the external acceleration of the rocket are introduced. A near time-optimal feedback-form guidance law to determine the direction of the acceleration for guiding the IIP is developed using the de-rivative expressions. The effectiveness of the proposed guidance law, in comparison with the results of open-loop trajectory optimization, was demonstrated through IIP pointing case studies.',\n",
       "  'len': 587},\n",
       " {'abstract': 'For a group of cooperating UAVs, localizing each other is often a key task. This paper studies the localization problem for a group of UAVs flying in 3D space with very limited information, i.e., when noisy distance measurements are the only type of inter-agent sensing that is available, and when only one UAV knows a global coordinate basis, the others being GPS-denied. Initially for a two-agent problem, but easily generalized to some multi-agent problems, constraints are established on the minimum number of required distance measurements required to achieve the localization. The paper also proposes an algorithm based on semidefinite programming (SDP), followed by maximum likelihood estimation using a gradient descent initialized from the SDP calculation. The efficacy of the algorithm is verified with experimental noisy flight data.',\n",
       "  'len': 855},\n",
       " {'abstract': 'A new analytic formulation to express the time derivatives of the instantaneous impact point (IIP) of a rocket is proposed. The geometric relationship on a plane tangential to the IIP is utilized to decompose the inertial IIP rate vector into the downrange and crossrange components, and a systematic procedure to determine the component values is presented. The new formulation shows significant advantages over the existing formulation such that the procedure and final expressions for the IIP derivatives are easy to understand and more compact. The validity of the proposed formulation was demonstrated through numerical simulation.',\n",
       "  'len': 647},\n",
       " {'abstract': 'Programmable shape-shifting materials can take different physical forms to achieve multifunctionality in a dynamic and controllable manner. Although morphing a shape from 2D to 3D via programmed inhomogeneous local deformations has been demonstrated in various ways, the inverse problem -- programming a sheet to take an arbitrary desired 3D shape -- is much harder yet critical to realize specific functions. Here, we address this inverse problem in thin liquid crystal elastomer (LCE) sheets, where the shape is preprogrammed by precise and local control of the molecular orientation of the liquid crystal monomers. We show how blueprints for arbitrary surface geometries as well as local extrinsic curvatures can be generated using approximate numerical methods. Backed by faithfully alignable and rapidly lockable LCE chemistry, we precisely embed our designs in LCE sheets using advanced top-down microfabrication techniques. We thus successfully produce flat sheets that, upon thermal activation, take an arbitrary desired shape, such as a face. The general design principles presented here for creating an arbitrary 3D shape will allow for exploration of unmet needs in flexible electronics, metamaterials, aerospace and medical devices, and more.',\n",
       "  'len': 1265},\n",
       " {'abstract': 'Despite years of research, understanding of the space radiation environment and the risk it poses to long-duration astronauts remains limited. There is a disparity between research results and observed empirical effects seen in human astronaut crews, likely due to the numerous factors that limit terrestrial simulation of the complex space environment and extrapolation of human clinical consequences from varied animal models. Given the intended future of human spaceflight, with efforts now to rapidly expand capabilities for human missions to the moon and Mars, there is a pressing need to improve upon the understanding of the space radiation risk, predict likely clinical outcomes of interplanetary radiation exposure, and develop appropriate and effective mitigation strategies for future missions. To achieve this goal, the space radiation and aerospace community must recognize the historical limitations of radiation research and how such limitations could be addressed in future research endeavors. We have sought to highlight the numerous factors that limit understanding of the risk of space radiation for human crews and to identify ways in which these limitations could be addressed for improved understanding and appropriate risk posture regarding future human spaceflight.',\n",
       "  'len': 1300},\n",
       " {'abstract': 'This work is a natural extension of the authors previous work, Multiple scattering theory for heterogeneous elastic continua with strong property fluctuation, theoretical fundamentals and applications, which established the foundation for developing multiple scattering model for strongly scattering heterogeneous elastic continua. In this work, the corresponding multiple scattering theory for polycrystalline materials with randomly oriented anisotropic crystallites is developed. As applications in ultrasonic nondestructive evaluation, we calculated the dispersion and attenuation coefficient of one of the most important polycrystalline materials in aeronautics engineering, high temperature titanium alloys. The effects of grain symmetry, grain size, and alloying elements on the dispersion and attenuation behaviors are examined. Key information is obtained which has significant implications for quantitatively evaluating the average grain size, monitoring the phase transition, and even estimating gradual change in chemical composition of titanium components in gas turbine engines. For applications in seismology, the velocities and Q-factors for both hexagonal and cubic polycrystalline iron models for the Earth uppermost inner core are obtained in the whole frequency range. This work provides a universal, quantitative model for characterization of a large variety of polycrystalline materials. It also can be extended to incorporate more complicated microstructures, including ellipsoidal grains with or without textures, and even multiphase polycrystalline materials. The new model demonstrates great potential of applications in ultrasonic nondestructive evaluation and inspection of aerospace and aeronautic structures. It also provides a theoretical framework for quantitative seismic data explanation and inversion for the material composition and structural formations of the Earth inner core.',\n",
       "  'len': 1926},\n",
       " {'abstract': 'We report the design, fabrication and characterization of ultralight highly emissive metaphotonic structures with record-low mass/area that emit thermal radiation efficiently over a broad spectral (2 to 35 microns) and angular (0-60 degrees) range. The structures comprise one to three pairs of alternating nanometer-scale metallic and dielectric layers, and have measured effective 300 K hemispherical emissivities of 0.7 to 0.9. To our knowledge, these structures, which are all subwavelength in thickness are the lightest reported metasurfaces with comparable infrared emissivity. The superior optical properties, together with their mechanical flexibility, low outgassing, and low areal mass, suggest that these metasurfaces are candidates for thermal management in applications demanding of ultralight flexible structures, including aerospace applications, ultralight photovoltaics, lightweight flexible electronics, and textiles for thermal insulation.',\n",
       "  'len': 969},\n",
       " {'abstract': \"NASA's SLS and Orion crew vehicle will launch humans to cislunar space to begin the new era of space exploration. NASA plans to use the Orion crew vehicle to transport humans between Earth and cislunar space where there will be a stationed habitat known as the Deep Space Gateway (DSG). The proximity to the lunar surface allows for direct communication between the DSG and surface assets, which enables low-latency telerobotic exploration. The operational constraints for telerobotics must be fully explored on Earth before being utilized on space exploration missions. We identified two constraints on space exploration using low-latency surface telerobotics and attempts to quantify these constraints. A constraint associated with low-latency surface telerobotics is the bandwidth available between the orbiting command station and the ground assets. The bandwidth available will vary during operation. As a result, it is critical to quantify the operational video conditions required for effective exploration. We designed an experiment to quantify the threshold frame rate required for effective exploration. The experiment simulated geological exploration via low-latency surface telerobotics using a COTS rover in a lunar analog environment. The results from this experiment indicate that humans should operate above a threshold frame rate of 5 frames per second. In a separate, but similar experiment, we introduced a 2.6 second delay in the video system. This delay recreated the latency conditions present when operating rovers on the lunar farside from an Earth-based command station. This time delay was compared to low-latency conditions for teleoperation at the DSG ($\\\\leq$0.4 seconds). The results from this experiment show a 150% increase in exploration time when the latency is increased to 2.6 seconds. This indicates that such a delay significantly complicates real-time exploration strategies.\",\n",
       "  'len': 1924},\n",
       " {'abstract': 'Heterogeneous materials consisting of different phases are ideally suited to achieve a broad spectrum of desirable bulk physical properties by combining the best features of the constituents through the strategic spatial arrangement of the different phases. Disordered hyperuniform heterogeneous materials are new, exotic amorphous matter that behave like crystals in the manner in which they suppress volume-fraction fluctuations at large length scales, and yet are isotropic with no Bragg peaks. In this paper, we formulate for the first time a Fourier-space numerical construction procedure to design at will a wide class of disordered hyperuniform two-phase materials with prescribed spectral densities, which enables one to tune the degree and length scales at which this suppression occurs. We demonstrate that the anomalous suppression of volume-fraction fluctuations in such two-phase materials endow them with novel and often optimal transport and electromagnetic properties. Specifically, we construct a family of phase-inversion-symmetric materials with variable topological connectedness properties that remarkably achieves a well-known explicit formula for the effective electrical (thermal) conductivity. Moreover, we design disordered stealthy hyperuniform dispersion that possesses nearly optimal effective conductivity while being statistically isotropic. Interestingly, all of our designed materials are transparent to electromagnetic radiation for certain wavelengths, which is a common attribute of all hyperuniform materials. Our constructed materials can be readily realized by 3D printing and lithographic technologies. We expect that our designs will be potentially useful for energy-saving materials, batteries and aerospace applications.',\n",
       "  'len': 1774},\n",
       " {'abstract': 'A Global Enhanced Vibrational Kinetic (GEVKM) model is presented for a new High Current Negative Hydrogen Ion Source (HCNHIS) developed by Busek Co. Inc. and Worcester Polytechnic Institute. The HCNHIS consists of a high-pressure radio-frequency discharge (RFD) chamber where high-lying vibrational states of the hydrogen molecules are produced, a bypass system, a nozzle, and a low-pressure negative hydrogen ion production region where $\\\\text{H}^-$ are generated by the dissociative attachment of low energy electrons to rovibrationally excited hydrogen molecules. The GEVKM is developed from moment equations for multi-temperature chemically reacting plasmas derived from the Wang Chang-Uhlenbeck equations for a cylindrical geometry of an inductively coupled RFD chamber. The species included into the model are $\\\\text{H}(n), n=0-3, \\\\text{H}_2(v), v=0-14, \\\\text{H}^+, \\\\text{H}_2^+, \\\\text{H}_3^+, \\\\text{H}^-$, and $e$. The volume-averaged steady-state continuity equations coupled with the electron energy equation, the total energy equation and heat transfer to the chamber walls are solved simultaneously to obtain the volume-averaged number densities of the plasma components, the electron, heavy-particle and wall temperatures. The GEVKM is supplemented by a comprehensive set of surface and volumetric chemical processes governing vibrational and ionization kinetics of hydrogen plasmas. Analytic boundary conditions developed for the flow in the bypass tubes and in the nozzle take into account rarefaction effects and are validated by comparing pressures predictions in the RFD chamber with the pressure measurements. The GEVKM is verified and validated in the wide range of pressures and absorbed powers of different plasma reactors and is used in a parametric investigation of the RFD chamber of the HCNHIS with hydrogen inlet flow rates 5-1000 sccm and absorbed powers 200-1000 W.',\n",
       "  'len': 1904},\n",
       " {'abstract': \"We propose a primal-dual interior-point (PDIP) method for solving quadratic programming problems with linear inequality constraints that typically arise form MPC applications. We show that the solver converges (locally) quadratically to a suboptimal solution of the MPC problem. PDIP solvers rely on two phases: the damped and the pure Newton phases. Compared to state-of-the-art PDIP methods, our solver replaces the initial damped Newton phase (usually used to compute a medium-accuracy solution) with a dual solver based on Nesterov's fast gradient scheme (DFG) that converges with a sublinear convergence rate of order O(1/k^2) to a medium-accuracy solution. The switching strategy to the pure Newton phase, compared to the state of the art, is computed in the dual space to exploit the dual information provided by the DFG in the first phase. Removing the damped Newton phase has the additional advantage that our solver saves the computational effort required by backtracking line search. The effectiveness of the proposed solver is demonstrated on a 2-dimensional discrete-time unstable system and on an aerospace application.\",\n",
       "  'len': 1144},\n",
       " {'abstract': 'It is shown that replacing the sinusoidal chip in Golay complementary code pairs by special classes of waveforms that satisfy two conditions, symmetry/anti-symmetry and quazi-orthogonality in the convolution sense, renders the complementary codes immune to frequency selective fading and also allows for concatenating them in time using one frequency band/channel. This results in a zero-sidelobe region around the mainlobe and an adjacent region of small cross-correlation sidelobes. The symmetry/anti-symmetry property results in the zero-sidelobe region on either side of the mainlobe, while quasi-orthogonality of the two chips keeps the adjacent region of cross-correlations small. Such codes are constructed using discrete frequency-coding waveforms (DFCW) based on linear frequency modulation (LFM) and piecewise LFM (PLFM) waveforms as chips for the complementary code pair, as they satisfy both the symmetry/anti-symmetry and quasi-orthogonality conditions. It is also shown that changing the slopes/chirp rates of the DFCW waveforms (based on LFM and PLFM waveforms) used as chips with the same complementary code pair results in good code sets with a zero-sidelobe region. It is also shown that a second good code set with a zero-sidelobe region could be constructed from the mates of the complementary code pair, while using the same DFCW waveforms as their chips. The cross-correlation between the two sets is shown to contain a zero-sidelobe region and an adjacent region of small cross-correlation sidelobes. Thus, the two sets are quasi-orthogonal and could be combined to form a good code set with twice the number of codes without affecting their cross-correlation properties. Or a better good code set with the same number codes could be constructed by choosing the best candidates form the two sets. Such code sets find utility in multiple input-multiple output (MIMO) radar applications.',\n",
       "  'len': 1919},\n",
       " {'abstract': \"Hyper-viscoelastic polymers have multiple areas of application including aerospace, biomedicine, and automotive. Their mechanical responses are therefore extremely important to understand, particularly because they exhibit strong rate and temperature dependence, including a low temperature brittle transition. Relationships between the response at various strain rates and temperatures are investigated and a framework developed to predict large strain response at rates of c. 1000 s$^{-1}$ and above where experiments are unfeasible. A master curve of the storage modulus's rate dependence at a reference temperature is constructed using a DMA test of the polymer. A frequency sweep spanning two decades and a temperature range from pre-glass transition to pre-melt is used. A fractional derivative model is fitted to the experimental data, and this model's parameters are used to derive stress-strain relationships at a desired strain rate.\",\n",
       "  'len': 954},\n",
       " {'abstract': 'In this work, we consider the detection of manoeuvring small objects with radars. Such objects induce low signal to noise ratio (SNR) reflections in the received signal. We consider both co-located and separated transmitter/receiver pairs, i.e., mono-static and bi-static configurations, respectively, as well as multi-static settings involving both types. We propose a detection approach which is capable of coherently integrating these reflections within a coherent processing interval (CPI) in all these configurations and continuing integration for an arbitrarily long time across consecutive CPIs. We estimate the complex value of the reflection coefficients for integration while simultaneously estimating the object trajectory. Compounded with this is the estimation of the unknown time reference shift of the separated transmitters necessary for coherent processing. Detection is made by using the resulting integration value in a Neyman-Pearson test against a constant false alarm rate threshold. We demonstrate the efficacy of our approach in a simulation example with a very low SNR object which cannot be detected with conventional techniques.',\n",
       "  'len': 1166},\n",
       " {'abstract': 'The problem of detecting changes with multiple sensors has received significant attention in the literature. In many practical applications such as critical infrastructure monitoring and modeling of disease spread, a useful change propagation model is one where change eventually happens at all sensors, but where not all sensors witness change at the same time instant. While prior work considered the case of known change propagation dynamics, this paper studies a more general setting of unknown change propagation pattern (trajectory). A Bayesian formulation of the problem in both centralized and decentralized settings is studied with the goal of detecting the first time instant at which any sensor witnesses a change. Using the dynamic programming (DP) framework, the optimal solution structure is derived and in the rare change regime, several more practical change detection algorithms are proposed. Under certain conditions, the first-order asymptotic optimality of a proposed algorithm called multichart test is shown as the false alarm probability vanishes. To further reduce the computational complexity, change detection algorithms are proposed based on online estimation of the unknown change propagation pattern. Numerical studies illustrate that the proposed detection techniques offer near-optimal performance. Further, in the decentralized setting, it is shown that if an event-triggered sampling scheme called level-crossing sampling with hysteresis (LCSH) is used for sampling and transmission of local statistics, the detection performance can be significantly improved using the same amount of communication resources compared to the conventional uniform-in-time sampling (US) scheme.',\n",
       "  'len': 1719},\n",
       " {'abstract': 'Model rockets have been employed in student projects, but very few papers in aerospace education offer concise summaries of activities at university-course levels. This paper aims to address this gap in the literature. The rockets used by our students reached some 500 m (~1,640 feet) in altitude, deployed a parachute, and spent 2-3 minutes descending to the ground. We present a series of analyses and experiments that students performed in order to predict the flight time, the maximum altitude, and the landing location of these rockets. They wrote computer programs to numerically integrate equations of motion, and experimentally measured input parameters (e.g., the thrust profile and drag coefficients). Once launched, these rockets could not be controlled; targeting the landing location would thus mean tilting the launch rail to a required angle. The largest source of error in landing location came from the difficulty in modeling wind velocities. Also discussed in this paper are the infrared spectroscopy and the extraction experiment as novel additions to model rocket projects.',\n",
       "  'len': 1104},\n",
       " {'abstract': 'Multipath is among the major sources of errors in precise positioning using GPS and continues to be extensively studied. Two Fast Fourier Transform (FFT)-based detectors are presented in this paper as GPS multipath detection techniques. The detectors are formulated as binary hypothesis tests under the assumption that the multipath exists for a sufficient time frame that allows its detection based on the quadrature arm of the coherent Early-minus-Late discriminator (Q EmL) for a scalar tracking loop (STL) or on the quadrature (Q EmL) and/or in-phase arm (I EmL) for a vector tracking loop (VTL), using an observation window of N samples. Performance analysis of the suggested detectors is done on multipath signal data acquired from the multipath environment simulator developed by the German Aerospace Centre (DLR) as well as on multipath data from real GPS signals. Application of the detection tests to correlator outputs of scalar and vector tracking loops shows that they may be used to exclude multipath contaminated satellites from the navigation solution. These detection techniques can be extended to other Global Navigation Satellite Systems (GNSS) such as GLONASS, Galileo and Beidou.',\n",
       "  'len': 1211},\n",
       " {'abstract': 'MIL-STD-1553 is a military standard that defines the physical and logical layers, and a command/response time division multiplexing of a communication bus used in military and aerospace avionic platforms for more than 40 years. As a legacy platform, MIL-STD-1553 was designed for high level of fault tolerance while less attention was taken with regard to security. Recent studies already addressed the impact of successful cyber attacks on aerospace vehicles that are implementing MIL-STD-1553. In this study we present a security analysis of MIL-STD-1553. In addition, we present a method for anomaly detection in MIL-STD-1553 communication bus and its performance in the presence of several attack scenarios implemented in a testbed, as well as results on real system data. Moreover, we propose a general approach towards an intrusion detection system (IDS) for a MIL-STD-1553 communication bus.',\n",
       "  'len': 909},\n",
       " {'abstract': 'Finite element (FE) analysis has the potential to offset much of the expensive experimental testing currently required to certify aerospace laminates. However, large numbers of degrees of freedom are necessary to model entire aircraft components whilst accurately resolving micro-scale defects. The new module dune-composites, implemented within DUNE by the authors, provides a tool to efficiently solve large-scale problems using novel iterative solvers. The key innovation is a preconditioner that guarantees a constant number of iterations regardless of the problem size. Its robustness has been shown rigorously in Spillane et al. (Numer. Math. 126, 2014) for isotropic problems. For anisotropic problems in composites it is verified numerically for the first time in this paper. The parallel implementation in DUNE scales almost optimally over thousands of cores. To demonstrate this, we present an original numerical study, varying the shape of a localised wrinkle and the effect this has on the strength of a curved laminate. This requires a high-fidelity mesh containing at least four layers of quadratic elements across each ply and interface layer, underlining the need for dune-composites, which can achieve run times of just over 2 minutes on 2048 cores for realistic composites problems with 173 million degrees of freedom.',\n",
       "  'len': 1347},\n",
       " {'abstract': 'The gimbal-spacecraft system, that consists of a variable speed control moment gyro (VSCMG) mounted inside a spacecraft, has been employed as an actuator for the attitude control of a spacecraft and has been much studied in the aerospace control community. Employing a Newtonian approach, the equations of motion are derived, and further study focusses on singularity issues and control law synthesis. While the geometric mechanics community has studied many mechanical systems of engineering interest, including spinning rotors (or momentum wheels) that are used as actuators, there has not been a particular effort to model and control the gimbal-spacecraft system in a geometric framework. This article serves two purposes: it presents the gimbal-spacecraft system in a geometric mechanics framework, and in particular, highlights the connection form, that could form the basis for future control design, and secondly, the exposition is of a tutorial nature whereby the willing reader, with minimal prerequisites, is introduced to the tools of differential geometry in this context.',\n",
       "  'len': 1096},\n",
       " {'abstract': 'Ultrahigh-resolution optical strain sensors provide powerful tools in various scientific and engineering fields, ranging from long-baseline interferometers to civil and aerospace industries. Here we demonstrate an ultrahigh-resolution fibre strain sensing method by directly detecting the time-of-flight (TOF) change of the optical pulse train generated from a free-running passively mode-locked laser (MLL) frequency comb. We achieved a local strain resolution of 18 p{\\\\epsilon}/Hz1/2 and 1.9 p{\\\\epsilon}/Hz1/2 at 1 Hz and 3 kHz, respectively, with largedynamic range of >154 dB at 3 kHz. For remote-point sensing at 1-km distance, 80 p{\\\\epsilon}/Hz1/2 (at 1 Hz) and 2.2 p{\\\\epsilon}/Hz1/2 (at 3 kHz) resolution is demonstrated. While attaining both ultrahigh resolution and large dynamic range, the demonstrated method can be readily extended for multiple-point sensing as well by taking advantage of the broad optical comb spectra. These advantages may allow various applications of this sensor in geophysical science, structural health monitoring, and underwater science.',\n",
       "  'len': 1085},\n",
       " {'abstract': 'Position-sensitive-detectors (PSDs) based on lateral photoeffect has been widely used in diverse applications, including optical engineering, aerospace and military fields. With increasing demand in long distance, low energy consumption, and weak signal sensing systems, the poor responsivity of conventional PSDs has become a bottleneck limiting its applications. Herein, we present a high performance graphene based PSDs with revolutionary interfacial amplification mechanism. Signal amplification in the order of ~10^4 has been demonstrated by utilizing the ultrahigh mobility of graphene and long lifetime of photo-induced carriers at the interface of SiO2/Si. This would improve the detection limit of Si-based PSDs from uW to nW level, without sacrificing the spatial resolution and response speed. Such interfacial amplification mechanism is compatible with current Si technology and can be easily extended to other sensing systems.',\n",
       "  'len': 950},\n",
       " {'abstract': 'Autoignition delay experiments for the isomers of butanol, including n-, sec-, tert-, and iso-butanol, have been performed using a heated rapid compression machine. For a compressed pressure of 15 bar, the compressed temperatures have been varied in the range of 725-855 K for all the stoichiometric fuel/oxidizer mixtures. Over the conditions investigated in this study, the ignition delay decreases monotonically as temperature increases and exhibits single-stage characteristics. Experimental ignition delays are also compared to simulations computed using three kinetic mechanisms available in the literature. Reasonable agreement is found for three isomers (tert-, iso-, and n-butanol).',\n",
       "  'len': 702},\n",
       " {'abstract': 'The emergence of small satellites and CubeSats for interplanetary exploration will mean hundreds if not thousands of spacecraft exploring every corner of the solar-system. Current methods for communication and tracking of deep space probes use ground based systems such as the Deep Space Network (DSN). However, the increased communication demand will require radically new methods to ease communication congestion. Networks of communication relay satellites located at strategic locations such as geostationary orbit and Lagrange points are potential solutions. Instead of one large communication relay satellite, we could have scores of small satellites that utilize phase arrays to effectively operate as one large satellite. Excess payload capacity on rockets can be used to warehouse more small satellites in the communication network. The advantage of this network is that even if one or a few of the satellites are damaged or destroyed, the network still operates but with degraded performance. The satellite network would operate in a distributed architecture and some satellites maybe dynamically repurposed to split and communicate with multiple targets at once. The potential for this alternate communication architecture is significant, but this requires development of satellite formation flying and networking technologies. Our research has found neural-network control approaches such as the Artificial Neural Tissue can be effectively used to control multirobot/multi-spacecraft systems and can produce human competitive controllers. We have been developing a laboratory experiment platform called Athena to develop critical spacecraft control algorithms and cognitive communication methods. We briefly report on the development of the platform and our plans to gain insight into communication phase arrays for space.',\n",
       "  'len': 1844},\n",
       " {'abstract': 'Quantitative thermal imaging has the potential of reliable temperature measurement across an entire field-of-view. This non-invasive technique has applications in aerospace, manufacturing and process control. However, robust temperature measurement on the sub-millimetre (30 {\\\\mu}m) length scale has yet to be demonstrated. Here, the temperature performance and size-of-source (source size) effect of a 3 {\\\\mu}m to 5 {\\\\mu}m thermal imaging system have been assessed. In addition a technique of quantifying thermal imager non-uniformity is described. An uncertainty budget is constructed, which describes a measurement uncertainty of 640 mK (k = 2) for a target with a size of 10 mm. The results of this study provide a foundation for developing the capability for confident quantitative sub-millimetre thermal imaging.',\n",
       "  'len': 829},\n",
       " {'abstract': 'The expedient design of precision components in aerospace and other high-tech industries requires simulations of physical phenomena often described by partial differential equations (PDEs) without exact solutions. Modern design problems require simulations with a level of resolution difficult to achieve in reasonable amounts of time---even in effectively parallelized solvers. Though the scale of the problem relative to available computing power is the greatest impediment to accelerating these applications, significant performance gains can be achieved through careful attention to the details of memory communication and access. The swept time-space decomposition rule reduces communication between sub-domains by exhausting the domain of influence before communicating boundary values. Here we present a GPU implementation of the swept rule, which modifies the algorithm for improved performance on this processing architecture by prioritizing use of private (shared) memory, avoiding interblock communication, and overwriting unnecessary values. It shows significant improvement in the execution time of finite-difference solvers for one-dimensional unsteady PDEs, producing speedups of 2--9$\\\\times$ for a range of problem sizes, respectively, compared with simple GPU versions and 7--300$\\\\times$ compared with parallel CPU versions. However, for a more sophisticated one-dimensional system of equations discretized with a second-order finite-volume scheme, the swept rule performs 1.2--1.9$\\\\times$ worse than a standard implementation for all problem sizes.',\n",
       "  'len': 1577},\n",
       " {'abstract': 'During the last decade, additive manufacturing has become increasingly popular for rapid prototyping, but has remained relatively marginal beyond the scope of prototyping when it comes to applications with tight tolerance specifications, such as in aerospace. Despite a strong desire to supplant many aerospace structures with printed builds, additive manufacturing has largely remained limited to prototyping, tooling, fixtures, and non-critical components. There are numerous fundamental challenges inherent to additive processing to be addressed before this promise is realized. One ubiquitous challenge across all AM motifs is to develop processing-property relationships through precise, in situ monitoring coupled with formal methods and feedback control. We suggest a significant component of this vision is a set of semantic layers within 3D printing files relevant to the desired material specifications. This semantic layer provides the feedback laws of the control system, which then evaluates the component during processing and intelligently evolves the build parameters within boundaries defined by semantic specifications. This evaluation and correction loop requires on-the-fly coupling of finite element analysis and topology optimization. The required parameters for this analysis are all extracted from the semantic layer and can be modified in situ to satisfy the global specifications. Therefore, the representation of what is printed changes during the printing process to compensate for eventual imprecision or drift arising during the manufacturing process.',\n",
       "  'len': 1592},\n",
       " {'abstract': 'An extension to the classical FPV model is developed for transcritical real-fluid combustion simulations in the context of finite volume, fully compressible, explicit solvers. A double-flux model is developed for transcritical flows to eliminate the spurious pressure oscillations. A hybrid scheme with entropy-stable flux correction is formulated to robustly represent large density ratios. The thermodynamics for ideal-gas values is modeled by a linearized specific heat ratio model. Parameters needed for the cubic EoS are pre-tabulated for the evaluation of departure functions and a quadratic expression is used to recover the attraction parameter. The novelty of the proposed approach lies in the ability to account for pressure and temperature variations from the baseline table. Cryogenic LOX/GH2 mixing and reacting cases are performed to demonstrate the capability of the proposed approach in multidimensional simulations. The proposed combustion model and numerical schemes are directly applicable for LES simulations of real applications under transcritical conditions.',\n",
       "  'len': 1092},\n",
       " {'abstract': \"Precise (sub-meter level) real-time navigation using a space-capable single-frequency global positioning system (GPS) receiver and ultra-rapid (real-time) ephemerides from the international global navigation satellite systems service is proposed for low-Earth-orbiting (LEO) satellites. The C/A code and L1 carrier phase measurements are combined and single-differenced to eliminate first-order ionospheric effects and receiver clock offsets. A random-walk process is employed to model the phase ambiguities in order to absorb the time-varying and satellite-specific higher-order measurement errors as well as the GPS clock correction errors. A sequential Kalman filter which incorporates the known orbital dynamic model is developed to estimate orbital states and phase ambiguities without matrix inversion. Real flight data from the single-frequency GPS receiver onboard China's SJ-9A small satellite are processed to evaluate the orbit determination accuracy. Statistics from internal orbit assessments indicate that three-dimensional accuracies of better than 0.50 m and 0.55 mm/s are achieved for position and velocity, respectively.\",\n",
       "  'len': 1149},\n",
       " {'abstract': 'In the aerospace industry the trend for light-weight structures and the resulting complex dynamic behaviours currently challenge vibration engineers. In many cases, these light-weight structures deviate from linear behaviour, and complex nonlinear phenomena can be expected. We consider a cyclically symmetric system of coupled weakly nonlinear undamped oscillators that could be considered a minimal model for different cyclic and symmetric aerospace structures experiencing large deformations. The focus is on localised vibrations that arise from wave envelope modulation of travelling waves. For the defocussing parameter range of the approximative nonlinear evolution equation, we show the possible existence of dark solitons and discuss their characteristics. For the focussing parameter range, we characterise modulation instability and illustrate corresponding nonlinear breather dynamics. Furthermore, we show that for stronger nonlinearity or randomness in initial conditions, transient breather-type dynamics and decay into bright solitons appear. The findings suggest that significant vibration localisation may arise due to mechanisms of nonlinear modulation dynamics.',\n",
       "  'len': 1191},\n",
       " {'abstract': 'We present a formal model for a fragmentation and a reassembly protocol running on top of the standardised CAN bus, which is widely used in automotive and aerospace applications. Although the CAN bus comes with an in-built mechanism for prioritisation, we argue that this is not sufficient and provide another protocol to overcome this shortcoming.',\n",
       "  'len': 359},\n",
       " {'abstract': 'We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter for multi-target tracking with the standard point target measurements without using probability generating functionals or functional derivatives. We also establish the connection with the \\\\delta-generalised labelled multi-Bernoulli (\\\\delta-GLMB) filter, showing that a \\\\delta-GLMB density represents a multi-Bernoulli mixture with labelled targets so it can be seen as a special case of PMBM. In addition, we propose an implementation for linear/Gaussian dynamic and measurement models and how to efficiently obtain typical estimators in the literature from the PMBM. The PMBM filter is shown to outperform other filters in the literature in a challenging scenario.',\n",
       "  'len': 753},\n",
       " {'abstract': 'Avionics is one kind of domain where prevention prevails. Nonetheless fails occur. Sometimes due to pilot misreacting, flooded in information. Sometimes information itself would be better verified than trusted. To avoid some kind of failure, it has been thought to add,in midst of the ARINC664 aircraft data network, a new kind of monitoring.',\n",
       "  'len': 353},\n",
       " {'abstract': 'We propose a simple low-cost technique that enables civil Global Positioning System (GPS) receivers and other civil global navigation satellite system (GNSS) receivers to reliably detect carry-off spoofing and jamming. The technique, which we call the Power-Distortion detector, classifies received signals as interference-free, multipath-afflicted, spoofed, or jammed according to observations of received power and correlation function distortion. It does not depend on external hardware or a network connection and can be readily implemented on many receivers via a firmware update. Crucially, the detector can with high probability distinguish low-power spoofing from ordinary multipath. In testing against over 25 high-quality empirical data sets yielding over 900,000 separate detection tests, the detector correctly alarms on all malicious spoofing or jamming attacks while maintaining a <0.6% single-channel false alarm rate.',\n",
       "  'len': 944},\n",
       " {'abstract': \"We performed highly resolved one-dimensional fully compressible Navier-Stokes simulations of heat-release-induced compression waves in near-critical CO2. The computational setup, inspired by the experimental setup of Miura et al., Phys. Rev. E, 2006, is composed of a closed inviscid (one-dimensional) duct with adiabatic hard ends filled with CO2 at three supercritical pressures. The corresponding initial temperature values are taken along the pseudo-boiling line. Thermodynamic and transport properties of CO2 in near-critical conditions are modeled via the Peng-Robinson equation of state and Chung's Method. A heat source is applied at a distance from one end, with heat release intensities spanning the range 10^3-10^11 W/m^2, generating isentropic compression waves for values < 10^9 W/m^2. For higher heat-release rates such compressions are coalescent with distinct shock-like features (e.g. non-isentropicity and propagation Mach numbers measurably greater than unity) and a non-uniform post-shock state is present due to the strong thermodynamic nonlinearities. The resulting compression wave intensities have been collapsed via the thermal expansion coefficient, highly variable in near-critical fluids, used as one of the scaling parameters for the reference energy. The proposed scaling applies to isentropic thermoacoustic waves as well as shock waves up to shock strength 2. Long-term time integration reveals resonance behavior of the compression waves, raising the mean pressure and temperature at every resonance cycle. When the heat injection is halted, expansion waves are generated, which counteract the compression waves leaving conduction as the only thermal relaxation process. In the long term evolution, the decay in amplitude of the resonating waves observed in the experiments is qualitatively reproduced by using isothermal boundary conditions.\",\n",
       "  'len': 1886},\n",
       " {'abstract': 'Communication with a spacecraft is typically performed using Radio Frequency (RF). RF is a well-established and well-regulated technology that enables communication over long distances as proven by the Voyager 1 & II missions. However, RF requires licensing of very limited radio spectrum and this poses a challenge in the future, particularly with spectrum time-sharing. This is of a concern for emergency communication when it is of utmost urgency to contact the spacecraft and maintain contact, particularly when there is a major mission anomaly or loss of contact. For these applications, we propose a backup laser communication system where a laser is beamed towards a satellite and the onboard photovoltaics acts as a laser receiver. This approach enables a laser ground station to broadcast commands to the spacecraft in times of emergency. Adding an actuated reflector to the laser receiver on the spacecraft enables two-way communication between ground and the spacecraft, but without the laser being located on the spacecraft. In this paper, we analyze the feasibility of the concept in the laboratory and develop a benchtop experiment to verify the concept. We have also developed a preliminary design for a 6U CubeSat-based demonstrator to evaluate technology merits',\n",
       "  'len': 1289},\n",
       " {'abstract': 'The momentum conservation law is applied to analyse the dynamics of pulsejet engine in vertical motion in a uniform gravitational field in the absence of friction. The model predicts existence of a terminal speed given frequency of the short pulses. The conditions that the engine does not return to the starting position are identified. The number of short periodic pulses after which the engine returns to the starting position is found to be independent of the exhaust velocity and gravitational field intensity for certain frequency of the pulses. The pulsejet engine and turbojet engine aircraft models of dynamics are compared. Also the octopus dynamics is modelled. The paper is addressed to intermediate undergraduate students of classical mechanics and aerospace engineering.',\n",
       "  'len': 795},\n",
       " {'abstract': 'We have carried out boundary-layer-resolved, unstructured fully-compressible Navier--Stokes simulations of an ultrasonic standing-wave thermoacoustic engine (TAE) model. The model is constructed as a quarter-wavelength engine, approximately 4 mm by 4 mm in size and operating at 25 kHz, and comprises a thermoacoustic stack and a coin-shaped cavity, a design inspired by Flitcroft and Symko (2013). Thermal and viscous boundary layers (order of 10 $\\\\mathrm{\\\\mu}$m) are resolved. Vibrational and rotational molecular relaxation are modeled with an effective bulk viscosity coefficient modifying the viscous stress tensor. The effective bulk viscosity coefficient is estimated from the difference between theoretical and semi-empirical attenuation curves. Contributions to the effective bulk viscosity coefficient can be identified as from vibrational and rotational molecular relaxation. The inclusion of the coefficient captures acoustic absorption from infrasonic ($\\\\sim$10 Hz) to ultrasonic ($\\\\sim$100 kHz) frequencies. The value of bulk viscosity depends on pressure, temperature, and frequency, as well as the relative humidity of the working fluid. Simulations of the TAE are carried out to the limit cycle, with growth rates and limit-cycle amplitudes varying non-monotonically with the magnitude of bulk viscosity, reaching a maximum for a relative humidity level of 5%. A corresponding linear model with minor losses was developed; the linear model overpredicts transient growth rate but gives an accurate estimate of limit cycle behavior. An improved understanding of thermoacoustic energy conversion in the ultrasonic regime based on a high-fidelity computational framework will help to further improve the power density advantages of small-scale thermoacoustic engines.',\n",
       "  'len': 1791},\n",
       " {'abstract': 'Previous results reported in the robotics literature show the relationship between time-delay control (TDC) and proportional-integral-derivative control (PID). In this paper, we show that incremental nonlinear dynamic inversion (INDI) - more familiar in the aerospace community - are in fact equivalent to TDC. This leads to a meaningful and systematic method for PI(D)-control tuning of robust nonlinear flight control systems via INDI. We considered a reformulation of the plant dynamics inversion which removes effector blending models from the resulting control law, resulting in robust model-free control laws like PI(D)-control.',\n",
       "  'len': 645},\n",
       " {'abstract': 'In the context of embedded systems design, two important challenges are still under investigation. First, improve real-time data processing, reconfigurability, scalability, and self-adjusting capabilities of hardware components. Second, reduce power consumption through low-power design techniques as clock gating, logic gating, and dynamic partial reconfiguration (DPR) capabilities. Today, several application, e.g., cryptography, Software-defined radio or aerospace missions exploit the benefits of DPR of programmable logic devices. The DPR allows well defined reconfigurable FPGA region to be modified during runtime. However, it introduces an overhead in term of power consumption and time during the reconfiguration phase. In this paper, we present an investigation of power consumption overhead of the DPR process using a high-speed digital oscilloscope and the shunt resistor method. Results in terms of reconfiguration time and power consumption overhead for Virtex 5 FPGAs are shown.',\n",
       "  'len': 1005},\n",
       " {'abstract': 'We have performed large-eddy simulations of turbulent separation control via impedance boundary conditions (IBCs) on a \\\\nacafft airfoil in near-stalled conditions. The uncontrolled baseline flow is obtained for freestream Mach numbers of $M_\\\\infty=0.3$, chord-Reynolds numbers $Re_c = 1.5\\\\times10^6$ and angle of attack, $\\\\alpha=14^{\\\\circ{}}$. Flow control is applied via imposition of complex IBCs using the time-domain implementation developed by Scalo, Bodart, and Lele, $\\\\textit{Phys. Fluids} $(2015). Separation is delayed due to the enhanced mixing associated with convectively amplified spanwise-oriented Kelvin-Helmholtz (KH) rollers, generated via hydro-acoustic instabilities. The latter are the result of the interaction of the wall-normal transpiration through the impedance panel and the overlying mean background shear. The result is an alteration of the coupled instability between the separating shear layer and the vortex shedding in the wake (already present in the uncontrolled baseline flow) yielding unique wake topologies associated with different intensities for the passively generated KH vortical structures. Specifically, enhancements up to +13\\\\% in the lift coefficients have been obtained. Results show that tuning of the resonant cavities below the natural shedding frequency is required to generate KH rollers structures with a sufficiently large entrainment diameter to encompass the full extent of the separated region, thereby enhancing mixing and promoting reattachment. Overall, the results presented in this work show that the adoption of hydro-acoustically tuned resonant panels is a promising passive control technique for boundary layer separation control.',\n",
       "  'len': 1706},\n",
       " {'abstract': \"We present a numerical method to determine the complex acoustic impedance at the open surface of an arbitrarily shaped cavity, associated to an impinging planar acoustic wave with a given wavenumber vector and frequency. We have achieved this by developing the first inverse Helmholtz Solver (iHS), which implicitly reconstructs the complex acoustic waveform--at a given frequency--up to the unknown impedance boundary, hereby providing the spatial distribution of impedance as a result of the calculation for that given frequency. We show that the algebraic closure conditions required by the inverse Helmholtz problem are physically related to the assignment of the spatial distribution of the pressure phase over the unknown impedance boundary. The iHS is embarrassingly parallelizable over the frequency domain allowing for the straightforward determination of the full broadband impedance at every point of the target boundary. In this paper, we restrict our analysis to two-dimensions only. We first validate our results against Rott's quasi one-dimensional thermoacoustic theory for viscid and inviscid constant-area rectangular ducts, test our iHS in a fully unstructured fashion with a geometrically complex cavity, and finally, present a simplified, two-dimensional analysis of a sample of carbon-carbon ultrasonically absorptive coatings (C/C UACs) manufactured in DLR-Stuttgart, and used in the hypersonic transition delay experiments by Wagner et al. in AIAA 2012-5865. The final goal is to model C/C UACs with multi-oscillator Time Domain Impedance Boundary Conditions (TDIBC) developed by Lin et al. in JFM (2016) to be applied in direct numerical simulations (DNS) focused on the overlying boundary layer, eliminating the need to simultaneously resolve the microscopic porous structure of the C/C UACs.\",\n",
       "  'len': 1829},\n",
       " {'abstract': 'CubeSats and small satellites are emerging as low-cost tools to perform astronomy, exoplanet searches and earth observation. These satellites can be dedicated to pointing at targets for weeks or months at a time. This is typically not possible on larger missions where usage is shared. Current satellites use reaction wheels and where possible magneto-torquers to control attitude. However, these actuators can induce jitter due to various sources. In this work, we introduce a new class of actuators that exploit radiometric forces induced by gasses on surface with a thermal gradient. Our work shows that a CubeSat or small spacecraft mounted with radiometric actuators can achieve precise pointing of few arc-seconds or less and avoid the jitter problem. The actuator is entirely solid-state, containing no moving mechanical components. This ensures high-reliability and long-life in space. A preliminary design for these actuators is proposed, followed by feasibility analysis of the actuator performance.',\n",
       "  'len': 1020},\n",
       " {'abstract': 'Wheeled planetary rovers such as the Mars Exploration Rovers (MERs) and Mars Science Laboratory (MSL) have provided unprecedented, detailed images of the Mars surface. However, these rovers are large and are of high-cost as they need to carry sophisticated instruments and science laboratories. We propose the development of low-cost planetary rovers that are the size and shape of cantaloupes and that can be deployed from a larger rover. The rover named SphereX is 2 kg in mass, is spherical, holonomic and contains a hopping mechanism to jump over rugged terrain. A small low-cost rover complements a larger rover, particularly to traverse rugged terrain or roll down a canyon, cliff or crater to obtain images and science data. While it may be a one-way journey for these small robots, they could be used tactically to obtain high-reward science data. The robot is equipped with a pair of stereo cameras to perform visual navigation and has room for a science payload. In this paper, we analyze the design and development of a laboratory prototype. The results show a promising pathway towards development of a field system.',\n",
       "  'len': 1139},\n",
       " {'abstract': 'Autonomous control systems onboard planetary rovers and spacecraft benefit from having cognitive capabilities like learning so that they can adapt to unexpected situations in-situ. Q-learning is a form of reinforcement learning and it has been efficient in solving certain class of learning problems. However, embedded systems onboard planetary rovers and spacecraft rarely implement learning algorithms due to the constraints faced in the field, like processing power, chip size, convergence rate and costs due to the need for radiation hardening. These challenges present a compelling need for a portable, low-power, area efficient hardware accelerator to make learning algorithms practical onboard space hardware. This paper presents a FPGA implementation of Q-learning with Artificial Neural Networks (ANN). This method matches the massive parallelism inherent in neural network software with the fine-grain parallelism of an FPGA hardware thereby dramatically reducing processing time. Mars Science Laboratory currently uses Xilinx-Space-grade Virtex FPGA devices for image processing, pyrotechnic operation control and obstacle avoidance. We simulate and program our architecture on a Xilinx Virtex 7 FPGA. The architectural implementation for a single neuron Q-learning and a more complex Multilayer Perception (MLP) Q-learning accelerator has been demonstrated. The results show up to a 43-fold speed up by Virtex 7 FPGAs compared to a conventional Intel i5 2.3 GHz CPU. Finally, we simulate the proposed architecture using the Symphony simulator and compiler from Xilinx, and evaluate the performance and power consumption.',\n",
       "  'len': 1643},\n",
       " {'abstract': \"In this paper, cyber attack detection and isolation is studied on a network of UAVs in a formation flying setup. As the UAVs communicate to reach consensus on their states while making the formation, the communication network among the UAVs makes them vulnerable to a potential attack from malicious adversaries. Two types of attacks pertinent to a network of UAVs have been considered: a node attack on the UAVs and a deception attack on the communication between the UAVs. UAVs formation control presented using a consensus algorithm to reach a pre-specified formation. A node and a communication path deception cyber attacks on the UAV's network are considered with their respective models in the formation setup. For these cyber attacks detection, a bank of Unknown Input Observer (UIO) based distributed fault detection scheme proposed to detect and identify the compromised UAV in the formation. A rule based on the residuals generated using the bank of UIOs are used to detect attacks and identify the compromised UAV in the formation. Further, an algorithm developed to remove the faulty UAV from the network once an attack detected and the compromised UAV isolated while maintaining the formation flight with a missing UAV node.\",\n",
       "  'len': 1248},\n",
       " {'abstract': 'This survey article deals with applications of optimal control to aerospace problems with a focus on modern geometric optimal control tools and numerical continuation techniques. Geometric optimal control is a theory combining optimal control with various concepts of differential geometry. The ultimate objective is to derive optimal synthesis results for general classes of control systems. Continuation or homotopy methods consist in solving a series of parameterized problems, starting from a simple one to end up by continuous deformation with the initial problem. They help overcoming the difficult initialization issues of the shooting method. The combination of geometric control and homotopy methods improves the traditional techniques of optimal control theory. A nonacademic example of optimal attitude-trajectory control of (classical and airborne) launch vehicles, treated in details, illustrates how geometric optimal control can be used to analyze finely the structure of the extremals. This theoretical analysis helps building an efficient numerical solution procedure combining shooting methods and numerical continuation. Chattering is also analyzed and it is shown how to deal with this issue in practice.',\n",
       "  'len': 1235},\n",
       " {'abstract': 'Beryllium (Be) is an important material with wide applications ranging from aerospace components to X-ray equipments. Yet a precise understanding of its phase diagram remains elusive. We have investigated the phase stability of Be using a recently developed hybrid free energy computation method that accounts for anharmonic effects by invoking phonon quasiparticles. We find that the hcp to bcc transition occurs near the melting curve at 0<P<11 GPa with a positive Clapeyron slope of 41 K/GPa. The bcc phase exists in a narrow temperature range that shrinks with increasing pressure, explaining the difficulty in observing this phase experimentally. This work also demonstrates the validity of this theoretical framework based on phonon quasiparticle to study structural stability and phase transitions in strongly anharmonic materials.',\n",
       "  'len': 849},\n",
       " {'abstract': 'SRAM-based FPGAs are increasingly popular in the aerospace industry due to their field programmability and low cost. However, they suffer from cosmic radiation induced Single Event Upsets (SEUs). In safety-critical applications, the dependability of the design is a prime concern since failures may have catastrophic consequences. An early analysis of the relationship between dependability metrics, performability-area trade-off, and different mitigation techniques for such applications can reduce the design effort while increasing the design confidence. This paper introduces a novel methodology based on probabilistic model checking, for the analysis of the reliability, availability, safety and performance-area tradeoffs of safety-critical systems for early design decisions. Starting from the high-level description of a system, a Markov reward model is constructed from the Control Data Flow Graph (CDFG) and a component characterization library targeting FPGAs. The proposed model and exhaustive analysis capture all the failure states (based on the fault detection coverage) and repairs possible in the system. We present quantitative results based on an FIR filter circuit to illustrate the applicability of the proposed approach and to demonstrate that a wide range of useful dependability and performability properties can be analyzed using the proposed methodology. The modeling results show the relationship between different mitigation techniques and fault detection coverage, exposing their direct impact on the design for early decisions.',\n",
       "  'len': 1568},\n",
       " {'abstract': 'Separation kernels provide temporal/spatial separation and controlled information flow to their hosted applications. They are introduced to decouple the analysis of applications in partitions from the analysis of the kernel itself. More than 20 implementations of separation kernels have been developed and widely applied in critical domains, e.g., avionics/aerospace, military/defense, and medical devices. Formal methods are mandated by the security/safety certification of separation kernels and have been carried out since this concept emerged. However, this field lacks a survey to systematically study, compare, and analyze related work. On the other hand, high-assurance separation kernels by formal methods still face big challenges. In this paper, an analytical framework is first proposed to clarify the functionalities, implementations, properties and standards, and formal methods application of separation kernels. Based on the proposed analytical framework, a taxonomy is designed according to formal methods application, functionalities, and properties of separation kernels. Research works in the literature are then categorized and overviewed by the taxonomy. In accordance with the analytical framework, a comprehensive analysis and discussion of related work are presented. Finally, four challenges and their possible technical directions for future research are identified, e.g. specification bottleneck, multicore and concurrency, and automation of full formal verification.',\n",
       "  'len': 1506},\n",
       " {'abstract': \"System relevant embedded software needs to be reliable and, therefore, well tested, especially for aerospace systems. A common technique to verify programs is the analysis of their abstract syntax tree (AST). Tree structures can be elegantly analyzed with the logic programming language Prolog. Moreover, Prolog offers further advantages for a thorough analysis: On the one hand, it natively provides versatile options to efficiently process tree or graph data structures. On the other hand, Prolog's non-determinism and backtracking eases tests of different variations of the program flow without big effort. A rule-based approach with Prolog allows to characterize the verification goals in a concise and declarative way. In this paper, we describe our approach to verify the source code of a flash file system with the help of Prolog. The flash file system is written in C++ and has been developed particularly for the use in satellites. We transform a given abstract syntax tree of C++ source code into Prolog facts and derive the call graph and the execution sequence (tree), which then are further tested against verification goals. The different program flow branching due to control structures is derived by backtracking as subtrees of the full execution sequence. Finally, these subtrees are verified in Prolog. We illustrate our approach with a case study, where we search for incorrect applications of semaphores in embedded software using the real-time operating system RODOS. We rely on computation tree logic (CTL) and have designed an embedded domain specific language (DSL) in Prolog to express the verification goals.\",\n",
       "  'len': 1645},\n",
       " {'abstract': 'How to implement an impeccable space system-of-systems (SoS) internetworking architecture has been a significant issue in system engineering for years. Reliable data transmission is considered one of the most important technologies of space SoS internetworking systems. Due to the high bit error rate (BER), long time delay and asymmetrical channel in the space communication environment, the congestion control mechanism of classic transport control protocols (TCP) shows unsatisfying performances. With the help of existing TCP modifications, this paper contributes an aggressive congestion control mechanism. The proposed mechanism is characterized with a fast start procedure, as well as the feedback information to analyze network traffic and with a link terminating processing mechanism, which can help to reveal the real reason of packet loss, and maintain the size of congestion window at a high level. Simulation results are shown in the end to verify the proposed scheme.',\n",
       "  'len': 992},\n",
       " {'abstract': 'This paper discusses spacecraft control using variable-speed CMGs. A new operational concept for VSCMGs is proposed. This new concept makes it possible to approximate the complex nonlinear system by a linear time-varying system (LTV). As a result, an effective control system design method, Model Predictive Control (MPC) using robust pole assignment, can be used to design the spacecraft control system using VSCMGs. A nice feature of this design is that the control system does not have any singular point. A design example is provided. The simulation result shows the effectiveness of the proposed method.',\n",
       "  'len': 619},\n",
       " {'abstract': 'Magnesium (Mg) and its alloys hold great potential as an energy-saving structural material for automative, aerospace applications. However, the use of Mg alloys has been limited due to poor ductility and formability. Poor mechanical properties of Mg alloys origin from the insufficient number of slip systems, and deformation twinning plays an important role to accommodate plastic deformation. Here, we report a comprehensive experimental and modeling study to understand crystal size effect on the transformation in deformation modes in twin oriented Mg single crystals. The experiments reveal two regimes of size effects: (1) single twin propagation, where a typical \"smaller the stronger\" behavior was dominant in pillars {\\\\le} 18 {\\\\mu}m in diameter, and (2) twin-twin interaction, which results in anomalous strain hardening in pillars > 18 {\\\\mu}m. Molecular dynamics simulations further indicate a transition from twinning to dislocation mediated plasticity for crystal sizes below a few hundred nanometers. Our results provide new understanding of the fundamental deformation modes of twin oriented Mg from nano-scale to bulk, and insights to design Mg alloys with superior mechanical properties through dimensional refinement. This subsequently can materialize into more utilization of Mg alloys as a structural material in technologically relevant applications.',\n",
       "  'len': 1381},\n",
       " {'abstract': 'In high reliability standards fields such as automotive, avionics or aerospace, the detection of anomalies is crucial. An efficient methodology for automatically detecting multivariate outliers is introduced. It takes advantage of the remarkable properties of the Invariant Coordinate Selection (ICS) method. Based on the simultaneous spectral decomposition of two scatter matrices, ICS leads to an affine invariant coordinate system in which the Euclidian distance corresponds to a Mahalanobis Distance (MD) in the original coordinates. The limitations of MD are highlighted using theoretical arguments in a context where the dimension of the data is large. Unlike MD, ICS makes it possible to select relevant components which removes the limitations. Owing to the resulting dimension reduction, the method is expected to improve the power of outlier detection rules such as MD-based criteria. It also greatly simplifies outliers interpretation. The paper includes practical guidelines for using ICS in the context of a small proportion of outliers which is relevant in high reliability standards fields. The choice of scatter matrices together with the selection of relevant invariant components through parallel analysis and normality tests are addressed. The use of the regular covariance matrix and the so called matrix of fourth moments as the scatter pair is recommended. This choice combines the simplicity of implementation together with the possibility to derive theoretical results. A simulation study confirms the good properties of the proposal and compares it with other scatter pairs. This study also provides a comparison with Principal Component Analysis and MD. The performance of our proposal is also evaluated on several real data sets using a user-friendly R package accompanying the paper.',\n",
       "  'len': 1822},\n",
       " {'abstract': \"In this work, a novel data-based stochastic global identification framework is introduced for air vehicles operating under varying flight states and uncertainty. In this context, the term global refers to the identification of a model that is capable of representing the system dynamics under any admissible flight state based on data recorded from sample states. The proposed framework is based on stochastic time-series models for representing the system dynamics and aeroelastic response under multiple flight states, with each state characterized by several variables, such as the airspeed and angle of attack, forming a flight state vector. The method's cornerstone lies in the new class of Vector-dependent Functionally Pooled (VFP) models which allow the explicit analytical inclusion of the flight state vector into the model parameters and, hence, system dynamics. The experimental evaluation is based on a prototype bio-inspired self-sensing composite wing that is subjected to a series of wind tunnel experiments. Distributed micro-sensors in the form of stretchable sensor networks are embedded in the composite layup of the wing to provide the sensing capabilities. Data collected from piezoelectric sensors are employed for the identification of a stochastic global VFP model. The estimated VFP model parameters constitute two-dimensional functions of the flight state vector defined by the airspeed and angle of attack. The identified model is able to successfully represent the aeroelastic response of the wing under the admissible flight states via a minimum number of estimated parameters compared to standard identification approaches. The obtained results demonstrate the high accuracy and effectiveness of the proposed global identification framework, thus constituting a first step towards the next generation of fly-by-feel aerospace vehicles with state awareness capabilities.\",\n",
       "  'len': 1911},\n",
       " {'abstract': 'Advances in robotics and additive manufacturing have become game-changing for the prospects of space industry. It has become feasible to bootstrap a self-sustaining, self-expanding industry at reasonably low cost. Simple modeling was developed to identify the main parameters of successful bootstrapping. This indicates that bootstrapping can be achieved with as little as 12 metric tons (MT) landed on the Moon during a period of about 20 years. The equipment will be teleoperated and then transitioned to full autonomy so the industry can spread to the asteroid belt and beyond. The strategy begins with a sub-replicating system and evolves it toward full self-sustainability (full closure) via an in situ technology spiral. The industry grows exponentially due to the free real estate, energy, and material resources of space. The mass of industrial assets at the end of bootstrapping will be 156 MT with 60 humanoid robots, or as high as 40,000 MT with as many as 100,000 humanoid robots if faster manufacturing is supported by launching a total of 41 MT to the Moon. Within another few decades with no further investment, it can have millions of times the industrial capacity of the United States. Modeling over wide parameter ranges indicates this is reasonable, but further analysis is needed. This industry promises to revolutionize the human condition.',\n",
       "  'len': 1372},\n",
       " {'abstract': 'Simulations of physical phenomena are essential to the expedient design of precision components in aerospace and other high-tech industries. These phenomena are often described by mathematical models involving partial differential equations (PDEs) without exact solutions. Modern design problems require simulations with a level of resolution that is difficult to achieve in a reasonable amount of time even in effectively parallelized solvers. Though the scale of the problem relative to available computing power is the greatest impediment to accelerating these applications, significant performance gains can be achieved through careful attention to the details of memory accesses. Parallelized PDE solvers are subject to a trade-off in memory management: store the solution for each timestep in abundant, global memory with high access costs or in a limited, private memory with low access costs that must be passed between nodes. The GPU implementation of swept time-space decomposition presented here mitigates this dilemma by using private (shared) memory, avoiding internode communication, and overwriting unnecessary values. It shows significant improvement in the execution time of the PDE solvers in one dimension achieving speedups of 6-2x for large and small problem sizes respectively compared to naive GPU versions and 7-300x compared to parallel CPU versions.',\n",
       "  'len': 1386},\n",
       " {'abstract': 'Atmospheric flight phase of a launch vehicle is utilized to evaluate the performance of an H-infinity controller in the presence of disturbances. Dynamics of the vehicle is linearly modeled using time-varying parameters. An operating point was found to design a robust command tracker using H-infinity control theory that guarantees a stable maneuver. At the end, the controller was employed on the launch vehicle to assess the capability of control design on the linearized aerospace vehicle. Experimental results illustrate the excellent performance of the H-infinity controller and accurate tracking implemented by the autopilot. Also the robustness of the entire system against disturbances is demonstrated to be acceptable.',\n",
       "  'len': 739},\n",
       " {'abstract': 'Sliding mode control of a launch vehicle during its atmospheric flight phase is studied in the presence of unmatched disturbances. Linear time-varying dynamics of the aerospace vehicle is converted into a systematic formula and then dynamic sliding manifold as an advanced method is used in order to overcome the limited capability of conventional sliding manifolds in minimizing the undesired effects of unmatched perturbations on the control system. At the end, simulation results are evaluated and the performance of two approaches are compared in terms of stability and robustness of the autopilot.',\n",
       "  'len': 613},\n",
       " {'abstract': 'This study concerns the effectiveness of several techniques and methods of signals processing and data interpretation for the diagnosis of aerospace structure defects. This is done by applying different known feature extraction methods, in addition to a new CBIR-based one; and some soft computing techniques including a recent HPC parallel implementation of the U-BRAIN learning algorithm on Non Destructive Testing data. The performance of the resulting detection systems are measured in terms of Accuracy, Sensitivity, Specificity, and Precision. Their effectiveness is evaluated by the Matthews correlation, the Area Under Curve (AUC), and the F-Measure. Several experiments are performed on a standard dataset of eddy current signal samples for aircraft structures. Our experimental results evidence that the key to a successful defect classifier is the feature extraction method - namely the novel CBIR-based one outperforms all the competitors - and they illustrate the greater effectiveness of the U-BRAIN algorithm and the MLP neural network among the soft computing methods in this kind of application. Keywords- Non-destructive testing (NDT); Soft Computing; Feature Extraction; Classification Algorithms; Content-Based Image Retrieval (CBIR); Eddy Currents (EC).',\n",
       "  'len': 1285},\n",
       " {'abstract': 'Premise of the Study: Impatiens is a commonly seen garden flower, renowned for its strong adaptability and long history of cultivation. However, seldom has any research touched on its physiological resistance mechanism. In this experiment, the impatiens is selected from those which experienced aerospace mutation and thereafter 12 years of cultivation and breeding. Therefore, it is superior to the non-mutagenized impatiens in terms of drought resistance and demonstrates tremendous differences from the normal impatiens in physiology, which intrigues scholars to search for the underlying reasons . Methods: By reference to Impatiens balsamina L,this experiment uses mutagenized impatiens seeds, processed by PEG-6000 in different solution concentration, to measure the germination rate of impatiens, its relative enzymatic activity and expression differences between gene SoS2 and gene RD29b in the drought lower reaches. Key results: Under simulated drought stress, there is no distinct difference between the mutagenized impatiens and the normal impatiens in terms of germination rate. But by measuring the root tillers and the length, the relative enzymatic activity, MDA, and the expression differences between gene SoS2 and gene RD29b in the drought lower reaches, it is verified that the mutagenized impatiens has more advantages than the normal impatiens, and it Can further cultivate become drought resistance varieties impatiens. Conclusions: In this experiment,which is a positive mutation, the mutagenized impatiens improves drought resistance through radiative mutation. The so obtained impatiens is more pleasing in sight in terms of color and shape and has higher application value in garden virescence.',\n",
       "  'len': 1732},\n",
       " {'abstract': 'Facilitating the coexistence of radar systems with communication systems has been a major area of research in radar engineering. The current work presents a new way to sense the environment using the channel equalization block of existing communication systems. We have named this system CommSense. In the current paper we demonstrate the feasibility of the system using Global System for Mobile Communications (GSM) signals. The implementation has been done using open-source Software Defined Radio (SDR) environment. In the preliminary results obtained in our work we show that it is possible to distinguish environmental changes using the proposed system. The major advantage of the system is that it is inexpensive as channel estimation is an inherent block in any communication system and hence the added cost to make it work as an environment sensor is minimal. The major challenge, on which we are continuing our work, is how to characterize the features in the environmental changes. This is an acute challenge given the fact that the bandwidth available is narrow and the system is inherently a forward looking radar. However the initial results, as shown in this paper, are encouraging and we intend to use an application specific instrumentation (ASIN) scheme to distinguish the environmental changes.',\n",
       "  'len': 1323},\n",
       " {'abstract': 'This paper gives a summary on the system concept and design of the focal plane assembly of AsteroidFinder/SSB, a small satellite mission which is currently under development at the German Aerospace Center (DLR). An athermal design concept has been developed in accordance to the requirements of the instrument and spacecraft. Key aspects leading to this approach have been a trade-off study of the mechanical telescope interface, the definition of electrical and thermal interfaces and a material selection which minimizes thermally induced stresses. As a novelty, the structure will be manufactured from a machinable AlN-BN composite ceramic. To enable rapid design iterations and development, an integrated modeling approach has been used to conduct a thermo-mechanical analysis of the proposed concept in order to prove its feasibility. The steady-state temperature distribution for various load cases and the resulting stress and strain within the assembly have both been computed using a finite element simulation.',\n",
       "  'len': 1030},\n",
       " {'abstract': \"Solving quaternion kinematical differential equations is one of the most significant problems in the automation, navigation, aerospace and aeronautics literatures. Most existing approaches for this problem neither preserve the norm of quaternions nor avoid errors accumulated in the sense of long term time. We present symplectic geometric algorithms to deal with the quaternion kinematical differential equation by modeling its time-invariant and time-varying versions with Hamiltonian systems by adopting a three-step strategy. Firstly, a generalized Euler's formula for the autonomous quaternion kinematical differential equation are proved and used to construct symplectic single-step transition operators via the centered implicit Euler scheme for autonomous Hamiltonian system. Secondly, the symplecitiy, orthogonality and invertibility of the symplectic transition operators are proved rigorously. Finally, the main results obtained are generalized to design symplectic geometric algorithm for the time-varying quaternion kinematical differential equation which is a non-autonomous and nonlinear Hamiltonian system essentially. Our novel algorithms have simple algorithmic structures and low time complexity of computation, which are easy to be implemented with real-time techniques. The correctness and efficiencies of the proposed algorithms are verified and validated via numerical simulations.\",\n",
       "  'len': 1415},\n",
       " {'abstract': 'This study concerns with the diagnosis of aerospace structure defects by applying a HPC parallel implementation of a novel learning algorithm, named U-BRAIN. The Soft Computing approach allows advanced multi-parameter data processing in composite materials testing. The HPC parallel implementation overcomes the limits due to the great amount of data and the complexity of data processing. Our experimental results illustrate the effectiveness of the U-BRAIN parallel implementation as defect classifier in aerospace structures. The resulting system is implemented on a Linux-based cluster with multi-core architecture.',\n",
       "  'len': 630},\n",
       " {'abstract': 'The aim of this work is to classify the aerospace structure defects detected by eddy current non-destructive testing. The proposed method is based on the assumption that the defect is bound to the reaction of the probe coil impedance during the test. Impedance plane analysis is used to extract a feature vector from the shape of the coil impedance in the complex plane, through the use of some geometric parameters. Shape recognition is tested with three different machine-learning based classifiers: decision trees, neural networks and Naive Bayes. The performance of the proposed detection system are measured in terms of accuracy, sensitivity, specificity, precision and Matthews correlation coefficient. Several experiments are performed on dataset of eddy current signal samples for aircraft structures. The obtained results demonstrate the usefulness of our approach and the competiveness against existing descriptors.',\n",
       "  'len': 936},\n",
       " {'abstract': 'Mobile agent networks, such as multi-UAV systems, are constrained by limited resources. In particular, limited energy affects system performance directly, such as system lifetime. It has been demonstrated in the wireless sensor network literature that the communication energy consumption dominates the computational and the sensing energy consumption. Hence, the lifetime of the multi-UAV systems can be extended significantly by optimizing the amount of communication data, at the expense of increasing computational cost. In this work, we aim at attaining an optimal trade-off between the communication and the computational energy. Specifically, we propose a mixed-integer optimization formulation for a multi-hop hierarchical clustering-based self-organizing UAV network incorporating data aggregation, to obtain an energy-efficient information routing scheme. The proposed framework is tested on two applications, namely target tracking and area mapping. Based on simulation results, our method can significantly save energy compared to a baseline strategy, where there is no data aggregation and clustering scheme.',\n",
       "  'len': 1132},\n",
       " {'abstract': 'Spaceborne gravity gradients are proposed in this paper to provide autonomous orbit determination capabilities for near Earth satellites. The gravity gradients contain useful position information which can be extracted by matching the observations with a precise gravity model. The extended Kalman filter is investigated as the principal estimator. The stochastic model of orbital motion, the measurement equation and the model configuration are discussed for the filter design. An augmented state filter is also developed to deal with unknown significant measurement biases. Simulations are conducted to analyze the effects of initial errors, data-sampling periods, orbital heights, attitude and gradiometer noise levels, and measurement biases. Results show that the filter performs well with additive white noise observation errors. Degraded observability for the along-track position is found for the augmented state filter. Real flight data from the GOCE satellite are used to test the algorithm. Radial and cross-track position errors of less than 100 m have been achieved.',\n",
       "  'len': 1090},\n",
       " {'abstract': 'A new breakthrough in jet propulsion technology since the invention of the jet engine is achieved. The first critical tests for future air-breathing magneto-plasma propulsion systems have been successfully completed. In this regard, it is also the first time that a pinching dense plasma focus discharge could be ignited at one atmosphere and driven in pulse mode using very fast, nanosecond electrostatic excitations to induce self-organized plasma channels for ignition of the propulsive main discharge. Depending on the capacitor voltage (200-600 V) the energy input at one atmosphere varies from 52-320 J/pulse corresponding to impulse bits from 1.2-8.0 mNs. Such a new pulsed plasma propulsion system driven with one thousand pulses per second would already have thrust-to-area ratios (50-150 kN/m2) of modern jet engines. An array of thrusters could enable future aircrafts and airships to start from ground and reach altitudes up to 50km and beyond. The needed high power could be provided by future compact plasma fusion reactors already in development by aerospace companies. The magneto-plasma compressor itself was originally developed by Russian scientists as plasma fusion device and was later miniaturized for supersonic flow control applications. So the first breakthrough is based on a spin-off plasma fusion technology.',\n",
       "  'len': 1347},\n",
       " {'abstract': \"For near-Sun missions, the spacecraft approaches very close to the Sun and space environmental effects become relevant. Strong restrictions on how much close it can get derive from the maximum temperature that the used materials can stand, in order not to compromise the spacecraft's activity and functionalities. In other words, the minimum perihelion distance of a given mission can be determined based on the materials' temperature restrictions. The temperature of an object in space depends on its optical properties: reflectivity, absorptivity, transmissivity, and emissivity. Usually, it is considered as an approximation that the optical properties of materials are constant. However, emissivity depends on temperature. The consideration of the temperature dependence of emissivity and conductivity of materials used in the aerospace industry leads to the conclusion that the temperature dependence on the heliocentric distance is different from the case of constant optical properties [1]. Particularly, taking into account that emissivity is directly proportional to the temperature, the temperature of an object increases as $r{}^{-2/5}$ when the heliocentric distance $r$ decreases. This means that the same temperature will actually be reached at a different distance and, eventually, the spacecraft will be allowed to approach closer to the Sun without compromising its activities. We focused on metals used for aerospace structures (Al, Ti), however our analysis can be extended to all kinds of composite materials, once their optical properties - in particular emissivity - are defined.\",\n",
       "  'len': 1612},\n",
       " {'abstract': 'Avionics networks rely on a set of stringent reliability and safety requirements. In existing deployments, these networks are based on a wired technology, which supports these requirements. Furthermore, this technology simplifies the security management of the network since certain assumptions can be safely made, including the inability of an attacker to access the network, and the fact that it is almost impossible for an attacker to introduce a node into the network. The proposal for Avionics Wireless Networks (AWNs), currently under development by multiple aerospace working groups, promises a reduction in the complexity of electrical wiring harness design and fabrication, a reduction in the total weight of wires, increased customization possibilities, and the capacity to monitor otherwise inaccessible moving or rotating aircraft parts such as landing gear and some sections of the aircraft engines. While providing these benefits, the AWN must ensure that it provides levels of safety that are at minimum equivalent to those offered by the wired equivalent. In this paper, we propose a secure and trusted channel protocol that satisfies the stated security and operational requirements for an AWN protocol. There are three main objectives for this protocol. First, the protocol has to provide the assurance that all communicating entities can trust each other, and can trust their internal (secure) software and hardware states. Second, the protocol has to establish a fair key exchange between all communicating entities so as to provide a secure channel. Finally, the third objective is to be efficient for both the initial start-up of the network and when resuming a session after a cold and/or warm restart of a node. The proposed protocol is implemented and performance measurements are presented based on this implementation. In addition, we formally verify our proposed protocol using CasperFDR.',\n",
       "  'len': 1927},\n",
       " {'abstract': 'Two aspects of conductive heat are focused here (i) the nature of conductive heat, defined as that form of energy that is transferred as a result of a temperature difference and (ii) the nature of the intermolecular potentials that induces both thermal energy flow and the temperature profile at the steady state for a 1-D lattice chain. It is found that the standard presuppositions of people like Benofy and Quay (BQ) following Joseph Fourier do not obtain for at least a certain specified regime of intermolecular potential parameters related to harmonic (quadratic) potentials for nearest neighbor interactions. For these harmonic potentials, it appears from the simulation results that steady state solutions exist utilizing non-synthetic thermostats that couple not just the two particles at the extreme ends of the lattice chain, but to a control volume of $N$ particles located at either ends of the chain that does not accord with the unique analytical solutions that obtains for single particle thermostatting at the ends of the lattice with a different thermostatting algorithm that utilizes coupling coefficients. If the method used here is considered a more \"realistic\" or feasible model of the physical reality, then a re-evaluation of some aspects of the standard theoretical methodology is warranted since the standard model solution profile does not accord with the simulation temperature profile determined here for this related model. We also note that the sinusoidal temperature profile generated suggests that thermal integrated circuits with several thermal P-N junctions may be constructed, opening a way to create more complex thermal transistor circuits. A stationary principle is proposed for regions that violate the Fourier principle $\\\\mathbf{J_q.}\\\\nabla T \\\\le 0 $, where $\\\\mathbf{J_q}$ is the heat current vector and $T$ the temperature.',\n",
       "  'len': 1877},\n",
       " {'abstract': 'Heavy ions induced single event upset (SEU) sensitivity of three-dimensional integrated SRAMs are evaluated by using Monte Carlo sumulation methods based on Geant4. The cross sections of SEUs and Multi Cell Upsets (MCUs) for 3D SRAM are simulated by using heavy ions with different energies and LETs. The results show that the sensitivity of different die of 3D SRAM has obvious discrepancies at low LET. Average percentage of MCUs of 3D SRAMs rises from 17.2% to 32.95% when LET increases from 42.19 MeV cm2/mg to 58.57MeV cm2/mg. As for a certain LET, the percentage of MCUs shows a notable distinction between face-to-face structure and back-to-face structure. For back-to-face structure, the percentage of MCUs increases with the deeper die. However, the face-to-face die presents the relatively low percentage of MCUs. The comparison of SEU cross sections for planar SRAMs and experiment data are conducted to indicate the effectiveness of our simulation method. Finally, we compare the upset cross sections of planar process and 3D integrated SRAMs. Results demonstrate that the sensitivity of 3D SRAMs is not more than that of planar SRAMs and the 3D structure can be become a great potential application for aerospace and military domain.',\n",
       "  'len': 1257},\n",
       " {'abstract': \"We summarize a laser-ranged satellite test of frame-dragging, a prediction of General Relativity, and then concentrate on the estimate of thermal thrust, an important perturbation affecting the accuracy of the test. The frame dragging study analysed 3.5 years of data from the LARES satellite and a longer period of time for the two LAGEOS satellites. Using the gravity field GGM05S obtained via the Grace mission, which measures the Earth's gravitational field, the prediction of General Relativity is confirmed with a 1-$\\\\sigma$ formal error of 0.002, and a systematic error of 0.05. The result for the value of the frame dragging around the Earth is $\\\\mu$ = 0.994, compared to $\\\\mu$ = 1 predicted by General Relativity. The thermal force model assumes heat flow from the sun (visual) and from Earth (IR) to the satellite core and to the fused silica reflectors on the satellite, and reradiation into space. For a roughly current epoch (days 1460 - 1580 after launch) we calculate an average along-track drag of -0.50 $pm/s^{2}$.\",\n",
       "  'len': 1042},\n",
       " {'abstract': 'Cyber-physical systems (CPS) integrate sensing, computing, communication and actuation capabilities to monitor and control operations in the physical environment. A key requirement of such systems is the need to provide predictable real-time performance: the timing correctness of the system should be analyzable at design time with a quantitative metric and guaranteed at runtime with high assurance. This requirement of predictability is particularly important for safety-critical domains such as automobiles, aerospace, defense, manufacturing and medical devices. The work in this dissertation focuses on the challenges arising from the use of modern multi-core platforms in CPS. Even as of today, multi-core platforms are rarely used in safety-critical applications primarily due to the temporal interference caused by contention on various resources shared among processor cores, such as caches, memory buses, and I/O devices. Such interference is hard to predict and can significantly increase task execution time, e.g., up to 12x on commodity quad-core platforms. To address the problem of ensuring timing predictability on multi-core platforms, we develop novel analytical and systems techniques in this dissertation. Our proposed techniques theoretically bound temporal interference that tasks may suffer from when accessing shared resources. Our techniques also involve software primitives and algorithms for real-time operating systems and hypervisors, which significantly reduce the degree of the temporal interference. Specifically, we tackle the issues of cache and memory contention, locking and synchronization, interrupt handling, and access control for computational accelerators such as GPGPUs, all of which are crucial to achieving predictable real-time performance on a modern multi-core platform.',\n",
       "  'len': 1829},\n",
       " {'abstract': 'We propose a simple method for estimating crystal oscillator g-sensitivity in inertially aided Global Navigation Satellite System (GNSS) receivers. It does not require any specific equipment, like GNSS signal simulators or rate tables. The method is based on analyzing closed-loop phase tracking errors. This enables us to utilize the actual GNSS signal as the frequency reference, despite the presence of an unknown Doppler shift in it. The method has been successfully applied to the calibration of an oven-controlled crystal oscillator.',\n",
       "  'len': 550},\n",
       " {'abstract': 'Using 10 monopole antennas reaching into a rectangular multi mode waveguide we shape the incident wave to create specific transport even after scattering events. Each antenna is attached to an IQ-Modulator, which allows the adjustment of the amplitude and phase in a broad band range of 6-18 GHz. All of them are placed in the near field of the other, thus the excitation of an individual antenna is influenced by the presence of the other antennas. Still these 10 antennas are sufficient to generate any combination of the 10 propagating modes in the far field. At the output the propagating modes are extracted using a movable monopole antenna that is scanning the field. If the modes are scattered in a scattering region, the incident wave can be adjusted in such a way, that the outgoing wave can still be adjusted as long as localization is not present.',\n",
       "  'len': 869},\n",
       " {'abstract': 'This paper extends the capabilities of the harmonic potential field (HPF) approach to planning. The extension covers the situation where the workspace of a robot cannot be segmented into geometrical subregions where each region has an attribute of its own. The suggested approach uses a task-centered, probabilistic descriptor of the workspace as an input to the planner. This descriptor is processed, along with a goal point, to yield the navigation policy needed to steer the agent from any point in its workspace to the target. The approach is easily adaptable to planning in a cluttered environment containing a vector drift field. The extension of the HPF approach is based on the physical analogy with an electric current flowing in a nonhomogeneous conducting medium. The resulting potential field is known as the gamma-harmonic potential (GHPF). Proofs of the ability of the modified approach to avoid zero-probability (definite threat) regions and to converge to the goal are provided. The capabilities of the planer are demonstrated using simulation.',\n",
       "  'len': 1071},\n",
       " {'abstract': 'Dependability is an umbrella concept that subsumes many key properties about a system, including reliability, maintainability, safety, availability, confidentiality, and integrity. Various dependability modeling techniques have been developed to effectively capture the failure characteristics of systems over time. Traditionally, dependability models are analyzed using paper-and-pencil proof methods and computer based simulation tools but their results cannot be trusted due to their inherent inaccuracy limitations. The recent developments in probabilistic analysis support using formal methods have enabled the possibility of accurate and rigorous dependability analysis. Thus, the usage of formal methods for dependability analysis is widely advocated for safety-critical domains, such as transportation, aerospace and health. Given the complementary strengths of mainstream formal methods, like theorem proving and model checking, and the variety of dependability models judging the most suitable formal technique for a given dependability model is not a straightforward task. In this paper, we present a comprehensive review of existing formal dependability analysis techniques along with their pros and cons for handling a particular dependability model.',\n",
       "  'len': 1274},\n",
       " {'abstract': 'Target parameter estimation performance is investigated for a radar employing a set of widely separated transmitting and receiving antenna arrays. Cases with multiple extended targets are considered under two signal model assumptions: stochastic and deterministic. The general expressions for the corresponding Cramer-Rao lower bound (CRLB) and the asymptotic properties of the maximum-likelihood (ML) estimator are derived for a radar with $M_t$ arrays of $L_t$ transmitting elements and $M_r$ arrays of $L_r$ receiving elements for both types of signal models. It is shown that for an infinitely large product $M_tM_r$, and a finite $L_r$, the ML estimator is consistent and efficient under the stochastic model, while the deterministic model requires $M_tM_r$ to be finite and $L_r$ to be infinitely large in order to guarantee consistency and efficiency. Monte Carlo simulations further investigate the estimation performance of the proposed radar configuration in practical scenarios with finite $M_tM_r$ and $L_r$, and a fixed total number of available receiving antenna elements, $M_r L_r$. The numerical results demonstrate that grouping receiving elements into properly sized arrays reduces the mean squared error (MSE) and decreases the threshold SNR. In the numerical examples considered, the preferred configurations employ $M_t M_r > 1$. In fact, when $M_t M_r$ becomes too small, due to the loss of the geometric gain, the estimation performance becomes strongly dependent on the particular scenario and can degrade significantly, while the CRLB may become a poor prediction of the MSE even for high SNR. This suggests it may be advantageous to employ approaches where neither $M_tM_r$ nor $L_r$ are too small.',\n",
       "  'len': 1735},\n",
       " {'abstract': 'Increasing sensitivity and signal to noise ratios of conventional wave sensors is an interesting topic in structural health monitoring, medical imaging, aerospace and nuclear instrumentation. Here, we report the concept of a gradient piezoelectric self-sensing system by integrating shunting circuitry into conventional sensors. By tuning circuit elements properly, both the quality and quantity of the flexural wave measurement data can be significantly increased for new adaptive sensing applications. Through analytical, numerical and experimental studies, we demonstrate that the metamaterial-based sensing system (MBSS) with gradient bending stiffness can be designed by connecting gradient negative capacitance circuits to an array of piezoelectric patches (sensors). We further demonstrate that the proposed system can achieve more than two orders of magnitude amplification of flexural wave signals to overcome the detection limit. This research encompasses fundamental advancements in the MBSS with improved performance and functionalities, and will yield significant advances for a range of applications.',\n",
       "  'len': 1125},\n",
       " {'abstract': 'We propose a solution of the multiple target tracking (MTT) problem based on sets of trajectories and the random finite set framework. A full Bayesian approach to MTT should characterise the distribution of the trajectories given the measurements, as it contains all information about the trajectories. We attain this by considering multi-object density functions in which objects are trajectories. For the standard tracking models, we also describe a conjugate family of multitrajectory density functions.',\n",
       "  'len': 517},\n",
       " {'abstract': 'A research complex for aerospace radiation effects research has been proposed in Harbin Institute of Technology. Its core part is a proton accelerator complex, which consists of a 10 MeV injector, a 300 MeV synchrotron and beam transport lines. The proton beam extracted from the synchrotron is utilized for the radiation effects research. Based on the conceptual design [1], the design study for optimizing the synchrotron has been done. A new lattice design was worked out, and the multi-turn injection and slow extraction system were optimized with the new lattice design. In order to improve the time structure of the extracted beam, a RF knock-out method is employed. To meet the requirement of accurate control of dose, the frequency of the RF kicker is well investigated.',\n",
       "  'len': 789},\n",
       " {'abstract': 'Satellite Laser Ranging (SLR) is an established technology used for geodesy, fundamental science and precise orbit determination. This paper reports on the first successful SLR measurement from the German Aerospace Center research observatory in Stuttgart. While many SLR stations are in operation, the experiment described here is unique in several ways: The modular system has been assembled completely from commercial off-the-shelf components, which increases flexibility and significantly reduces hardware costs. To our knowledge it has been the first time that an SLR measurement has been conducted using an optical fibre rather than a coudé path to direct the light from the laser source onto the telescope. The transmitter operates at an output power of about 75 mW and a repetition rate of 3 kHz, and at a wavelength of 1064 nm. Due to its rather small diameter of only 80 {\\\\mu}{\\\\mu}m, the receiver detector features a low noise rate of less than 2 kHz and can be operated without gating in many cases. With this set-up, clear return signals have been received from several orbital objects equipped with retroreflectors. In its current configuration, the system does not yet achieve the same performance as other SLR systems in terms of precision, maximum distance and the capability of daylight ranging; however, plans to overcome these limitations are outlined.',\n",
       "  'len': 1382},\n",
       " {'abstract': 'We present a high-order implicit large-eddy simulation (ILES) approach for simulating transitional turbulent flows. The approach consists of an Interior Embedded Discontinuous Galerkin (IEDG) method for the discretization of the compressible Navier-Stokes equations and a parallel preconditioned Newton-GMRES solver for the resulting nonlinear system of equations. The IEDG method arises from the marriage of the Embedded Discontinuous Galerkin (EDG) method and the Hybridizable Discontinuous Galerkin (HDG) method. As such, the IEDG method inherits the advantages of both the EDG method and the HDG method to make itself well-suited for turbulence simulations. We propose a minimal residual Newton algorithm for solving the nonlinear system arising from the IEDG discretization of the Navier-Stokes equations. The preconditioned GMRES algorithm is based on a restricted additive Schwarz (RAS) preconditioner in conjunction with a block incomplete LU factorization at the subdomain level. The proposed approach is applied to the ILES of transitional turbulent flows over a NACA 65-(18)10 compressor cascade at Reynolds number 250,000 in both design and off-design conditions. The high-order ILES results show good agreement with a subgrid-scale LES model discretized with a second-order finite volume code while using significantly less degrees of freedom. This work shows that high-order accuracy is key for predicting transitional turbulent flows without a SGS model.',\n",
       "  'len': 1480},\n",
       " {'abstract': 'This paper presents an efficient method for estimating the probability of conflict between air traffic within a block of airspace. Autonomous Sense-and-Avoid is an essential safety feature to enable Unmanned Air Systems to operate alongside other (manned or unmanned) air traffic. The ability to estimate probability of conflict between traffic is an essential part of Sense-and-Avoid. Such probabilities are typically very low. Evaluating low probabilities using naive Direct Monte Carlo generates a significant computational load. This paper applies a technique called Subset Simulation. The small failure probabilities are computed as a product of larger conditional failure probabilities, reducing the computational load whilst improving the accuracy of the probability estimates. The reduction in the number of samples required can be one or more orders of magnitude. The utility of the approach is demonstrated by modeling a series of conflicting and potentially conflicting scenarios based on the standard Rules of the Air.',\n",
       "  'len': 1041},\n",
       " {'abstract': 'Bismaleimide (BMI) are thermosetting polymers mainly used in aerospace applications having properties of dimensional stability, low shrinkage, chemical resistance, fire resistance, good mechanical properties and high resistance against various solvents, acids, and water. BMI is commercially available as Homide 250. BMI coating has also been used for the corrosion protection. Metallization (AL) of BMI using vacuum evaporation was done which serves the purpose of prevention of space charge accumulation in aircraft bodies. Addition of inorganic materials like metal oxides can influence the properties of the polymer as an inorganic-organic composite. The organic-ionorganic composites have wide applications in electronics, optics, chemistry and medicine. Titanium dioxide (TiO2, Titania) has a wide range of applications starting from photocatalysis, dye-sensitized solar cells to optical coatings and electronics. A BMI-TiO2 composite was prepared by chemical route. Atmospheric Plasma Jet (APPJ) using Helium gas was also treated on BMI. XRD and FTIR studies of the composite system prepared at different temperatures showed its crystalline and structural configuration.',\n",
       "  'len': 1188},\n",
       " {'abstract': 'When an aircraft is flying and burning fuel the center of gravity (c.g.) of the aircraft shifts slowly. The c.g. can also be shifted abruptly when e.g. a fighter aircraft releases a weapon. The shift in c.g. is difficult to measure or estimate so the flight control systems need to be robustly designed to cope with this variation. However for fighter aircrafts with high manoeuvrability there is room for improvements. In this project we investigate if the use of adaptive control law augmentation can be used to better cope with the change in c.g. We augment a baseline controller with a robust Model Reference Adaptive Control (MRAC) design and analyse its benefits and possible issues.',\n",
       "  'len': 700},\n",
       " {'abstract': \"In this study the detection of the oil spill using synthetic aperture radar (SAR) imagery is considered. Detection of the oil spill is performed using change detection algorithms between imagery acquired at different times. The specific algorithms used are the correlation coefficient change statistic and the intensity ratio change statistic algorithms. Therefore these algorithms and the probabilistic selection of the threshold criteria is reviewed and discussed. A recently offered change detection method which depends on the idea of generating two different final change maps of two images in a sequence, is used. First final change map is obtained by cumulatively adding the sequences of change maps in such a manner that common change areas are excluded and uncommon change areas are included. The second final change map is obtained by comparing the first and the last images in the temporal sequence. This method requires at least three images to be employed and can be generalized to longer temporal image sequences. The purpose of this approach is to provide a double check mechanism to the conventional approach and thus to reduce the probability of false alarm and enhance the change detection. The algorithms mentioned are applied to a 2010 Gulf of Mexico oil spill imagery together with the method described. It is shown that intensity ratio change statistic is a better tool for identification of the changes due to the oil spill compared to the correlation coefficient change statistic. It is also shown that two final change map method can reduce the probability of false alarm. The data used in this study is acquired by the Japanese Aerospace Agency's Advanced Land Observing Satellite (ALOS) through Alaska SAR Facility (ASF) at the University of Alaska, Fairbanks, AK.\",\n",
       "  'len': 1802},\n",
       " {'abstract': 'This paper presents a multitarget tracking particle filter (PF) for general track-before-detect measurement models. The PF is presented in the random finite set framework and uses a labelled multi-Bernoulli approximation. We also present a label switching improvement algorithm based on Markov chain Monte Carlo that is expected to increase filter performance if targets get in close proximity for a sufficiently long time. The PF is tested in two challenging numerical examples.',\n",
       "  'len': 490},\n",
       " {'abstract': 'Two popular types of spacecraft actuators are reaction wheels and magnetic torque coils. Magnetic torque coils are particularly interesting because they can be used for both attitude control and reaction wheel momentum management (desaturation control). Although these two tasks are performed at the same time using the same set of actuators, most design methods deal with only one of the these tasks or consider these two tasks separately. In this paper, a design with these two tasks in mind is formulated as a single problem. A periodic time-varying linear quadratic regulator design method is then proposed to solve this problem. A simulation example is provided to describe the benefit of the new strategy.',\n",
       "  'len': 722},\n",
       " {'abstract': 'N-modular redundancy (NMR) is commonly used to enhance the fault tolerance of a circuit/system, when subject to a fault-inducing environment such as in space or military systems, where upsets due to radiation phenomena, temperature and/or other environmental conditions are anticipated. Triple Modular Redundancy (TMR), which is a 3-tuple version of NMR, is widely preferred for mission-control space, military, and aerospace, and safety-critical nuclear, power, medical, and industrial control and automation systems. The TMR scheme involves the two-times duplication of a simplex system hardware, with a majority voter ensuring correctness provided at least two out of three copies of the hardware remain operational. Thus the majority voter plays a pivotal role in ensuring the correct operation of the TMR scheme. In this paper, a number of standard-cell based majority voter designs relevant to TMR architectures are presented, and their power, delay and area parameters are estimated based on physical realization using a 32/28nm CMOS process.',\n",
       "  'len': 1060},\n",
       " {'abstract': 'Bearing--only estimation is one of the fundamental and challenging problems in target tracking. As in the case of radar tracking, the presence of offset or position biases can exacerbate the challenges in bearing--only estimation. Modeling various sensor biases is not a trivial task and not much has been done in the literature specifically for bearing--only tracking. This paper addresses the modeling of offset biases in bearing--only sensors and the ensuing multitarget tracking with bias compensation. Bias estimation is handled at the fusion node to which individual sensors report their local tracks in the form of associated measurement reports (AMR) or angle-only tracks. The modeling is based on a multisensor approach that can effectively handle a time--varying number of targets in the surveillance region. The proposed algorithm leads to a maximum likelihood bias estimator. The corresponding Cramér--Rao Lower Bound to quantify the theoretical accuracy that can be achieved by the proposed method or any other algorithm is also derived. Finally, simulation results on different distributed tracking scenarios are presented to demonstrate the capabilities of the proposed approach. In order to show that the proposed method can work even with false alarms and missed detections, simulation results on a centralized tracking scenario where the local sensors send all their measurements (not AMRs or local tracks) are also presented.',\n",
       "  'len': 1455},\n",
       " {'abstract': 'Bias estimation or sensor registration is an essential step in ensuring the accuracy of global tracks in multisensor-multitarget tracking. Most previously proposed algorithms for bias estimation rely on local measurements in centralized systems or tracks in distributed systems, along with additional information like covariances, filter gains or targets of opportunity. In addition, it is generally assumed that such data are made available to the fusion center at every sampling time. In practical distributed multisensor tracking systems, where each platform sends local tracks to the fusion center, only state estimates and, perhaps, their covariances are sent to the fusion center at non-consecutive sampling instants or scans. That is, not all the information required for exact bias estimation at the fusion center is available in practical distributed tracking systems. In this paper, a new algorithm that is capable of accurately estimating the biases even in the absence of filter gain information from local platforms is proposed for distributed tracking systems with intermittent track transmission. Through the calculation of the Posterior Cramér--Rao lower bound and various simulation results, it is shown that the performance of the new algorithm, which uses the tracklet idea and does not require track transmission at every sampling time or exchange of filter gains, can approach the performance of the exact bias estimation algorithm that requires local filter gains.',\n",
       "  'len': 1497},\n",
       " {'abstract': 'This paper deals with the problem of output regulation for systems defined on matrix Lie-Groups. Reference trajectories to be tracked are supposed to be generated by an exosystem, defined on the same Lie-Group of the controlled system, and only partial relative error measurements are supposed to be available. These measurements are assumed to be invariant and associated to a group action on a homogeneous space of the state space. In the spirit of the internal model principle the proposed control structure embeds a copy of the exosystem kinematic. This control problem is motivated by many real applications fields in aerospace, robotics, projective geometry, to name a few, in which systems are defined on matrix Lie-groups and references in the associated homogenous spaces.',\n",
       "  'len': 792},\n",
       " {'abstract': 'Optimization of large structures of multiple components is essential to many industries for minimizing mass, especially the design of aerospace vehicles. Optimizing a single primary load member independently of all other primary structures is an incomplete process, due to the redistribution of internal loads, as the stiffness distribution changes. That is, optimizing a component changes joint loads, which then calls for a new optimization - changing internal loads changes the optimum. This is particularly evident under buckling (stability) constraints. The goal is to develop a finite element-based optimization approach which can be used to optimize each component of a large, primary structure assembly. The optimization objective function will be to minimize mass for the system, and the constraints will be both stress constraints as well as buckling constraints. The research aims to improve both the solution and practical usability of these models. The system of interest is a spacecraft fuselage, of which the member components are panels throughout the structure. We present analyses of several main optimization methods, and define a new algorithm to solve this problem, eigenOpt.',\n",
       "  'len': 1207},\n",
       " {'abstract': 'The E and B experiment (EBEX) is a balloon-borne telescope designed to measure the polarization of the cosmic microwave background with 8\\' resolution employing a gondola scanning with speeds of order degree per second. In January 2013, EBEX completed 11 days of observations in a flight over Antarctica covering $\\\\sim$ 6000 square degrees of the sky. The payload is equipped with two redundant star cameras and two sets of three orthogonal gyroscopes to reconstruct the telescope attitude. The EBEX science goals require the pointing to be reconstructed to approximately 10\" in the map domain, and in-flight attitude control requires the real time pointing to be accurate to $\\\\sim$ 0.5$^{\\\\circ}$ . The high velocity scan strategy of EBEX coupled to its float altitude only permits the star cameras to take images at scan turnarounds, every $\\\\sim$ 40 seconds, and thus requires the development of a pointing system with low noise gyroscopes and carefully controlled systematic errors. Here we report on the design of the pointing system and on a simulation pipeline developed to understand and minimize the effects of systematic errors. The performance of the system is evaluated using the 2012/2013 flight data, and we show that we achieve a pointing error with RMS=25\" on 40 seconds azimuth throws, corresponding to an error of $\\\\sim$ 4.6\" in the map domain.',\n",
       "  'len': 1370},\n",
       " {'abstract': \"Co-existence between unlicensed networks that share spectrum spatio-temporally with terrestrial (e.g. Air Traffic Control) and shipborne radars in 3-GHz band is attracting significant interest. Similar to every primary-secondary coexistence scenario, interference from unlicensed devices to a primary receiver must be within acceptable bounds. In this work, we formulate the spectrum sharing problem between a pulsed, search radar (primary) and 802.11 WLAN as the secondary. We compute the protection region for such a search radar for a) a single secondary user (initially) as well as b) a random spatial distribution of multiple secondary users. Furthermore, we also analyze the interference to the WiFi devices from the radar's transmissions to estimate the impact on achievable WLAN throughput as a function of distance to the primary radar.\",\n",
       "  'len': 856},\n",
       " {'abstract': \"Neuromemristive systems (NMSs) currently represent the most promising platform to achieve energy efficient neuro-inspired computation. However, since the research field is less than a decade old, there are still countless algorithms and design paradigms to be explored within these systems. One particular domain that remains to be fully investigated within NMSs is unsupervised learning. In this work, we explore the design of an NMS for unsupervised clustering, which is a critical element of several machine learning algorithms. Using a simple memristor crossbar architecture and learning rule, we are able to achieve performance which is on par with MATLAB's k-means clustering.\",\n",
       "  'len': 693},\n",
       " {'abstract': 'This paper presents a novel framework which combines a non-iterative solution of Real-Time Nonlinear Receding Horizon Control (NRHC) methodology to achieve consensus within complex network topologies with existing time-delays and in presence of switching topologies. In this formulation, we solve the distributed nonlinear optimization problem for multi-agent network systems directly, \\\\emph{in real-time}, without any dependency on iterative processes, where the stability and convergence guarantees are provided for the solution. Three benchmark examples on non-linear chaotic systems provide validated results which demonstrate the significant outcomes of such methodology.',\n",
       "  'len': 687},\n",
       " {'abstract': 'An effective model of single and multilayered thin panels, including those formed using carbon fiber composite (CFC) materials, is incorporated into the Transmission Line Modeling (TLM) method. The thin panel model is a one-dimensional (1D) one based on analytical expansions of cotangent and cosecant functions that are used to describe the admittance matrix in the frequency domain; these are then converted into the time domain by using digital filter theory and an inverse Z transform. The model, which is extended to allow for material anisotropy, is executed within 1D TLM codes. And, for the first time, the two-dimensional (2D) thin surface model is embedded in unstructured three-dimensional (3D) TLM codes. The approach is validated by using it to study some canonical structures with analytic solutions, and against results taken from the literature. It is then used to investigate shielding effectiveness of carbon fiber composite materials in a practical curved aerospace-related structure.',\n",
       "  'len': 1014},\n",
       " {'abstract': 'This paper presents a sampling-based motion planning algorithm for real-time and propellant-optimized autonomous spacecraft trajectory generation in near-circular orbits. Specifically, this paper leverages recent algorithmic advances in the field of robot motion planning to the problem of impulsively-actuated, propellant-optimized rendezvous and proximity operations under the Clohessy-Wiltshire-Hill (CWH) dynamics model. The approach calls upon a modified version of the Fast Marching Tree (FMT*) algorithm to grow a set of feasible trajectories over a deterministic, low-dispersion set of sample points covering the free state space. To enforce safety, the tree is only grown over the subset of actively-safe samples, from which there exists a feasible one-burn collision avoidance maneuver that can safely circularize the spacecraft orbit along its coasting arc under a given set of potential thruster failures. Key features of the proposed algorithm include: (i) theoretical guarantees in terms of trajectory safety and performance, (ii) amenability to real-time implementation, and (iii) generality, in the sense that a large class of constraints can be handled directly. As a result, the proposed algorithm offers the potential for widespread application, ranging from on-orbit satellite servicing to orbital debris removal and autonomous inspection missions.',\n",
       "  'len': 1379},\n",
       " {'abstract': 'Given the variability in student learning it is becoming increasingly important to tailor courses as well as course sequences to student needs. This paper presents a systematic methodology for offering personalized course sequence recommendations to students. First, a forward-search backward-induction algorithm is developed that can optimally select course sequences to decrease the time required for a student to graduate. The algorithm accounts for prerequisite requirements (typically present in higher level education) and course availability. Second, using the tools of multi-armed bandits, an algorithm is developed that can optimally recommend a course sequence that both reduces the time to graduate while also increasing the overall GPA of the student. The algorithm dynamically learns how students with different contextual backgrounds perform for given course sequences and then recommends an optimal course sequence for new students. Using real-world student data from the UCLA Mechanical and Aerospace Engineering department, we illustrate how the proposed algorithms outperform other methods that do not include student contextual information when making course sequence recommendations.',\n",
       "  'len': 1214},\n",
       " {'abstract': 'CALET (CALorimetric Electron Telescope) is a high energy astroparticle physics experiment planned for a long exposure mission aboard the International Space Station (ISS) by the Japanese Aerospace Exploration Agency, in collaboration with the Italian Space Agency (ASI) and NASA. The main science goal is high precision measurements of the inclusive electron (+positron) spectrum below 1 TeV and the exploration of the energy region above 1 TeV, where the shape of the high end of the spectrum might unveil the presence of nearby sources of acceleration. CALET has been designed to achieve a large proton rejection capability (>10$^5$) with a fine grained imaging calorimeter (IMC) followed by a total absorption calorimeter (TASC), for a total thickness of 30 X$_{0}$ and 1.3 proton interaction length. With an excellent energy resolution and a lower background contamination with respect to previous experiments, CALET will search for possible spectral signatures of dark matter with both electrons and gamma rays. CALET will also measure the high energy spectra and relative abundance of cosmic nuclei from proton to iron and detect trans-iron elements up to Z$\\\\sim$40. The charge identification of individual nuclear species is performed by a dedicated module (CHD) and by multiple dE/dx measurements in the IMC. With a large exposure and high energy resolution, CALET will be able to verify and complement the observations of CREAM, PAMELA and AMS-02 on a possible deviation from a pure power-law of proton and He spectra in the few hundred GeV region and to extend the study to the multi-TeV region. CALET will also contribute to clarify the present experimental picture on the energy dependence of the boron/carbon ratio, below and above 1 TeV/n, thereby providing valuable information on cosmic-ray propagation in the galaxy. Gamma-ray transients will be studied with a dedicated Gamma-ray Burst Monitor (GBM).',\n",
       "  'len': 1929},\n",
       " {'abstract': 'Safety and reliability are the main issues for designing assistance wearable virtual environment of technical gesture in aerospace, or health application domains. That needs the integration in the same isomorphic engineering framework of human requirements, systems requirements and the rationale of their relation to the natural and artifactual this http URL explore coupling integration and design functional organization of support technical gesture systems, firstly ecological psychologyprovides usa heuristicconcept: the affordance. On the other hand mathematical theory of integrative physiology provides us scientific concepts: the stabilizing auto-association principle and functional interaction.After demonstrating the epistemological consistence of these concepts, we define an isomorphic framework to describe and model human systems integration dedicated to human in-the-loop system engineering.We present an experimental approach of safe design of assistance wearable virtual environment of gesture based in laboratory and parabolic flights. On the results, we discuss the relevance of our conceptual approach and the applications to future assistance of gesture wearable systems engineering.',\n",
       "  'len': 1217},\n",
       " {'abstract': 'The Kalman filter and its extensions are used in a vast number of aerospace and navigation applications for nonlinear state estimation of time series. In the literature, different approaches have been proposed to exploit the structure of the state and measurement models to reduce the computational demand of the algorithms. In this tutorial, we survey existing code optimization methods and present them using unified notation that allows them to be used with various Kalman filter extensions. We develop the optimization methods to cover a wider range of models, show how different structural optimizations can be combined, and present new applications for the existing optimizations. Furthermore, we present an example that shows that the exploitation of the structure of the problem can lead to improved estimation accuracy while reducing the computational load. This tutorial is intended for persons who are familiar with Kalman filtering and want to get insights for reducing the computational demand of different Kalman filter extensions.',\n",
       "  'len': 1056},\n",
       " {'abstract': 'The mechanical response, serviceability, and load bearing capacity of materials and structural components can be adversely affected due to external stimuli, which include exposure to a corrosive chemical species, high temperatures, temperature fluctuations (i.e., freezing-thawing), cyclic mechanical loading, just to name a few. It is, therefore, of paramount importance in several branches of engineering -- ranging from aerospace engineering, civil engineering to biomedical engineering -- to have a fundamental understanding of degradation of materials, as the materials in these applications are often subjected to adverse environments. As a result of recent advancements in material science, new materials like fiber-reinforced polymers and multi-functional materials that exhibit high ductility have been developed and widely used; for example, as infrastructural materials or in medical devices (e.g., stents). The traditional small-strain approaches of modeling these materials will not be adequate. In this paper, we study degradation of materials due to an exposure to chemical species and temperature under large-strain and large-deformations. In the first part of our research work, we present a consistent mathematical model with firm thermodynamic underpinning. We then obtain semi-analytical solutions of several canonical problems to illustrate the nature of the quasi-static and unsteady behaviors of degrading hyperelastic solids.',\n",
       "  'len': 1460},\n",
       " {'abstract': 'This work investigates the consensus problem for multi-agent nonlinear systems through the distributed real-time nonlinear receding horizon control methodology. With this work, we develop a scheme to reach the consensus for nonlinear multi agent systems under fixed directed/undirected graph(s) without the need of any linearization techniques. For this purpose, the problem of consensus is converted into an optimization problem and is directly solved by the backwards sweep Riccati method to generate the control protocol which results in a non-iterative algorithm. Stability analysis is conducted to provide convergence guarantees of proposed scheme. In addition, an extension to the leader-following consensus of nonlinear multi-agent systems is presented. Several examples are provided to validate and demonstrate the effectiveness of the presented scheme and the corresponding theoretical results.',\n",
       "  'len': 914},\n",
       " {'abstract': \"The pair correlation function is introduced to target tracking filters that use a finite point process target model as a means to investigate interactions in the Bayes posterior target process. It is shown that the Bayes posterior target point process of the probability hypothesis density (PHD) filter-before using the Poisson point process approximation to close the recursion-is a spatially correlated process with weakly repulsive pair interactions. The reduced Palm target point process is introduced to define the conditional target point process given the state of one or more known targets. Using the intensity function of the reduced Palm process, an approximate two-stage pseudo maximum a posteriori track extractor is developed. The proposed track extractor is formulated for the PHD filter and implemented in a numerical study that involves tracking two close-by targets. Numerical results demonstrate improvement in the mean optimal subpattern assignment statistic for the proposed track extractor in comparison to the Gaussian mixture PHD filter's state extractor.\",\n",
       "  'len': 1089},\n",
       " {'abstract': 'Bismaleimide (BMI) resins are a new breed of thermosetting resins used mainly for high temperature applications and have major usage in aerospace. BMI polymer coatings were deposited on aluminum and mild steel substrates. The effect of corrosion on mild steel and aluminum by Ringers Solution and there protection using BMI coatings were observed. X-ray diffraction studies showed crystalline nature of the BMI coatings. Surface contact angle measurements were carried out using goniometer.',\n",
       "  'len': 501},\n",
       " {'abstract': 'Laser gyroscopes making use of the Sagnac effect have been used as highly accurate rotation sensors for many years. First used in aerospace and defense applications, these devices have more recently been used for precision seismology and in other research settings. In particular, mid-sized (~1 m-scale) laser gyros have been under development as tilt sensors to augment the adaptive active seismic isolation systems in terrestrial interferometric gravitational wave detectors. The most prevalent design is the \"active\" gyroscope, in which the optical ring cavity used to measure the Sagnac degeneracy breaking is itself a laser resonator. In this article, we describe another topology: a \"passive\" gyroscope, in which the sensing cavity is not itself a laser but is instead tracked using external laser beams. While subject to its own limitations, this design is free from the deleterious lock-in effects observed in active systems, and has the advantage that it can be constructed using commercially available components. We demonstrate that our device achieves comparable sensitivity to those of similarly sized active laser gyroscopes.',\n",
       "  'len': 1150},\n",
       " {'abstract': 'This paper presents a novel Lagrangian approach to attitude tracking for rigid spacecraft using unit quaternions, where the motion equations of a spacecraft are described by a four degrees of freedom Lagrangian dynamics subject to a holonomic constraint imposed by the norm of a unit quaternion. The basic energy-conservation property as well as some additional useful properties of the Lagrangian dynamics are explored, enabling to develop quaternion-based attitude tracking controllers by taking full advantage of a broad class of tracking control designs for mechanical systems based on energy-shaping methodology. Global tracking of a desired attitude on the unit sphere is achieved by designing control laws that render the tracking error on the four-dimensional Euclidean space to converge to the origin. The topological constraints for globally exponentially tracking by a quaternion-based continuous controller and singularities in controller designs based on any three-parameter representation of the attitude are then avoided. Using this approach, a full-state feedback controller is first developed, and then several important issues, such as robustness to noise in quaternion measurements, unknown on-orbit torque disturbances, uncertainty in the inertial matrix, and lack of angular-velocity measurements are addressed progressively, by designing a hybrid state-feedback controller, an adaptive hybrid state-feedback controller, and an adaptive hybrid attitude-feedback controller. Global asymptotic stability is established for each controller. Simulations are included to illustrate the theoretical results.',\n",
       "  'len': 1633},\n",
       " {'abstract': 'In recent years, deep learning techniques have been introduced into the field of trajectory optimization to improve convergence and speed. Training such models requires large trajectory datasets. However, the convergence of low thrust (LT) optimizations is unpredictable before the optimization process ends. For randomly initialized low thrust transfer data generation, most of the computation power will be wasted on optimizing infeasible low thrust transfers, which leads to an inefficient data generation process. This work proposes a deep neural network (DNN) classifier to accurately identify feasible LT transfer prior to the optimization process. The DNN-classifier achieves an overall accuracy of 97.9%, which has the best performance among the tested algorithms. The accurate low-thrust trajectory feasibility identification can avoid optimization on undesired samples, so that the majority of the optimized samples are LT trajectories that converge. This technique enables efficient dataset generation for different mission scenarios with different spacecraft configurations.',\n",
       "  'len': 1097},\n",
       " {'abstract': \"The velocity of alpha particles relative to protons can vary depending on the solar wind type and distance from the Sun (Marsch 2012). Measurements from the previous spacecraft provided the alpha-proton's differential velocities down to 0.3 au. Parker Solar Probe (PSP) now enables insights into differential flows of newly accelerated solar wind closer to the Sun for the first time. Here, we study the difference between proton and alpha bulk velocities near PSP perihelia of Encounters 3-7 when the core solar wind is in the field of view of the Solar Probe Analyzer for Ions (SPAN-I) instrument. As previously reported at larger heliospheric distances, the alpha-proton differential speed observed by PSP is greater for fast wind than the slow solar wind. We compare PSP observations with various spacecraft measurements and present the radial and temporal evolution of the alpha-proton differential speed. The differential flow decreases as the solar wind propagates from the Sun, consistent with previous observations. While Helios showed a small radial dependence of differential flow for the slow solar wind, PSP clearly showed this dependency for the young slow solar wind down to 0.09 au. Our analysis shows that the alpha-proton differential speed's magnitude is mainly below the local Alfvén speed. Moreover, alpha particles usually move faster than protons close to the Sun. PSP crossed the Alfvén surface during its eighth Encounter and may cross it in future Encounters, enabling us to investigate the differential flow very close to the solar wind acceleration source region for the first time.\",\n",
       "  'len': 1621},\n",
       " {'abstract': \"Isotopic abundances in comets are key to understanding the history and origin of material in the Solar System. Deuterium-to-hydrogen (D/H) ratios in water are available for several comets. However, no long-term studies of the D/H ratio in water of a comet during its passage around the Sun have been reported. Linear alkanes are important organic molecules, which have been found on several Solar System bodies, including comets. To date, only upper limits of isotopic ratios for D/H and 13C/12C in linear alkanes are available. The aim of this work is a detailed analysis of the D/H ratio in water during the whole Rosetta mission. In addition, a first determination of the D/H and 13C/12C ratios in the first four linear alkanes in the coma of 67P/Churyumov-Gerasimenko is provided. We analysed in situ measurements from the Rosetta/ROSINA Double Focusing Mass Spectrometer (DFMS). The D/H ratio from HDO/H2O and the 16O/17O ratio from H216O/H217O did not change during 67P's passage around the Sun between 2014 and 2016. All D/H ratio measurements were compatible, within 1$\\\\sigma$, with the mean value of $5.01\\\\times10^{-4}$ and its relative variation of 2.0%. This suggests that the D/H ratio in 67P's coma is independent of heliocentric distance, level of cometary activity, as well as spacecraft location with respect to the nucleus. Additionally, the 16O/17O ratio could be determined with a higher accuracy than previously possible, yielding a value of 2347 with a relative variation of 2.3%. For the alkanes, the D/H ratio is between 4.1 and 4.8 times higher than in H2O, while the 13C/12C ratio is compatible, within uncertainties, with data for other Solar System objects. The relatively high D/H ratio in alkanes is in line with other cometary organic molecules and suggests that these organics may be inherited from the presolar molecular cloud from which the Solar System formed.\",\n",
       "  'len': 1905},\n",
       " {'abstract': 'I propose the Cosmic Hitchhikers hypothesis as follows. Advanced extraterrestrial civilizations may use free-floating planets as interstellar transportation for space exploration and interstellar colonization. Large groups or populations of their biological species, post-biological species, and technologies may become Cosmic Hitchhikers when they ride free-floating planets to reach, explore and colonize planetary systems. To get an interstellar ride, Cosmic Hitchhikers may travel to free-floating planets passing close by their home worlds. Otherwise, they may use astronomical engineering to steer free-floating planets toward their home planetary systems. Cosmic Hitchhikers may also ride objects native to the outer regions of their planetary systems, which become free-floating planets when ejected by astronomical engineering or by their stars during the asymptotic giant branch evolution. During interstellar travel, Cosmic Hitchhikers may apply astronomical engineering to steer their free-floating planets toward the planetary systems of their choice. Whereas riding free-floating planets may not save travel time, it avoids the technical challenges of interstellar spacecraft transporting large populations. Each civilization of Cosmic Hitchhikers may colonize several planetary systems. Its colonies may grow into autonomous civilizations, changing the number of civilizations in the Galaxy. Over the last 4 billion years, Cosmic Hitchhikers or their artifacts riding free-floating planets might have passed by the Solar System. Therefore, their artifacts might exist in the Solar System or in our stellar neighborhood. SETI and SETA should include the search for Cosmic Hitchhikers and their artifacts. Keywords: SETI, SETA, free-floating planet, extraterrestrial civilization, interstellar travel, interstellar colonization, artifact, Cosmic Hitchhikers',\n",
       "  'len': 1881},\n",
       " {'abstract': \"LiteBIRD, the Lite (Light) satellite for the study of B-mode polarization and Inflation from cosmic background Radiation Detection, is a space mission for primordial cosmology and fundamental physics. The Japan Aerospace Exploration Agency (JAXA) selected LiteBIRD in May 2019 as a strategic large-class (L-class) mission, with an expected launch in the late 2020s using JAXA's H3 rocket. LiteBIRD is planned to orbit the Sun-Earth Lagrangian point L2, where it will map the cosmic microwave background (CMB) polarization over the entire sky for three years, with three telescopes in 15 frequency bands between 34 and 448 GHz, to achieve an unprecedented total sensitivity of 2.2$\\\\mu$K-arcmin, with a typical angular resolution of 0.5$^\\\\circ$ at 100 GHz. The primary scientific objective of LiteBIRD is to search for the signal from cosmic inflation, either making a discovery or ruling out well-motivated inflationary models. The measurements of LiteBIRD will also provide us with insight into the quantum nature of gravity and other new physics beyond the standard models of particle physics and cosmology. We provide an overview of the LiteBIRD project, including scientific objectives, mission and system requirements, operation concept, spacecraft and payload module design, expected scientific outcomes, potential design extensions and synergies with other projects.\",\n",
       "  'len': 1383},\n",
       " {'abstract': 'We investigate how the probability of acquiring an optical link between a scanning and a target spacecraft depends on the spectral shape, power and dimensionality of the beam jitter, as well as on the choice of detector integration time, beam detection radius and scan speed. For slow scans and long integration times, the probability of failure (Pfail) is determined by the integrated jitter power up to a critical frequency, which we verify by comparing the results of an analytical model to those of Monte Carlo simulations. Jitter above the critical frequency leads to a loss of correlation between integration windows and decreases Pfail for both, 1d (radial) and 2d (radial and tangential) jitter, as long as the RMS jitter amplitude does not exceed the beam diameter. In the opposite limit of fast scans and short integration times, emergent correlations between jitter fluctuations on two adjacent scanning tracks also decrease Pfail. The analytical model is additionally used to assess the effect of multiple overlapping tracks and the impact of target drifts in the uncertainty plane.',\n",
       "  'len': 1105},\n",
       " {'abstract': \"Events which meet certain criteria from star tracker images onboard the Juno spacecraft have been proposed to be due to interplanetary dust particle impacts on its solar arrays. These events have been suggested to be caused by particles with diameters larger than 10 micrometers. Here, we compare the reported event rates to expected dust impact rates using dynamical meteoroid models for the four most abundant meteoroid/dust populations in the inner solar system. We find that the dust impact rates predicted by dynamical meteoroid models are not compatible with either the Juno observations in terms of the number of star tracker events per day, or with the variations of dust flux on Juno's solar panels with time and position in the solar system. For example, the rate of star tracker events on Juno's anti-sunward surfaces is the largest during a period during which Juno is expected to experience the peak impact fluxes on the opposite, sunward hemisphere. We also investigate the hypothesis of dust leaving the Martian Hill sphere originating either from the surface of Mars itself or from one of its moons. We do not find such a hypothetical source to be able to reproduce the star tracker event rate variations observed by Juno. We conclude that the star tracker events observed by Juno are unlikely to be the result of instantaneous impacts from the Zodiacal Cloud.\",\n",
       "  'len': 1387},\n",
       " {'abstract': 'To quantitatively study the driving mechanisms of magnetospheric convection in the magnetotail lobes on a global scale, we utilize data from the ARTEMIS spacecraft in the deep tail and the Cluster spacecraft in the near tail. Previous work demonstrated that, in the lobes near the Moon, we can estimate the convection by utilizing ARTEMIS measurements of lunar ions velocity. In this paper, we analyze these datasets with machine learning models to determine what upstream factors drive the lobe convection in different magnetotail regions and thereby understand the mechanisms that control the dynamics of the tail lobes. Our results show that the correlations between the predicted and test convection velocities for the machine learning models (>0.75) are much better than those of the multiple linear regression model (~ 0.23 - 0.43). The systematic analysis reveals that the IMF and magnetospheric activity play an important role in influencing plasma convection in the global magnetotail lobes.',\n",
       "  'len': 1011},\n",
       " {'abstract': 'Time-delay interferometry (TDI) is a data processing technique for LISA designed to suppress the otherwise overwhelming laser noise by several orders of magnitude. It is widely believed that TDI can only be applied once all phase or frequency measurements from each spacecraft have been synchronized to a common time frame. We demonstrate analytically, using as an example the commonly-used Michelson combination X, that TDI can be computed using the raw, unsynchronized data, thereby avoiding the need for the initial synchronization processing step and significantly simplifying the initial noise reduction pipeline for LISA. Furthermore, the raw data is free of any potential artifacts introduced by clock synchronization and reference frame transformation algorithms, which allows to operate directly on the MHz beatnotes. As a consequence, in-band clock noise is directly suppressed as part of TDI, in contrast to the approach previously proposed in the literature, in which large trends in the beatnotes are removed before the main laser-noise reduction step and clock noise is suppressed in an extra processing step. We validate our algorithm with fullscale numerical simulations that use LISA Instrument and PyTDI and show that we reach the same performance levels as the previously proposed methods, ultimately limited by the clock sideband stability.',\n",
       "  'len': 1371},\n",
       " {'abstract': 'Progress in space weather research and awareness needs community-wide strategies and procedures to evaluate our modeling assets. Here we present the activities of the Ambient Solar Wind Validation Team embedded in the COSPAR ISWAT initiative. We aim to bridge the gap between model developers and end-users to provide the community with an assessment of the state-of-the-art in solar wind forecasting. To this end, we develop an open online platform for validating solar wind models by comparing their solutions with in situ spacecraft measurements. The online platform will allow the space weather community to test the quality of state-of-the-art solar wind models with unified metrics providing an unbiased assessment of progress over time. In this study, we propose a metadata architecture and recommend community-wide forecasting goals and validation metrics. We conclude with a status update of the online platform and outline future perspectives.',\n",
       "  'len': 964},\n",
       " {'abstract': \"The Extreme-ultraviolet Stellar Characterization for Atmospheric Physics and Evolution (ESCAPE) mission is an astrophysics Small Explorer employing ultraviolet spectroscopy (EUV: 80 - 825 Å and FUV: 1280 - 1650 Å) to explore the high-energy radiation environment in the habitable zones around nearby stars. ESCAPE provides the first comprehensive study of the stellar EUV and coronal mass ejection environments which directly impact the habitability of rocky exoplanets. In a 20 month science mission, ESCAPE will provide the essential stellar characterization to identify exoplanetary systems most conducive to habitability and provide a roadmap for NASA's future life-finder missions. ESCAPE accomplishes this goal with roughly two-order-of-magnitude gains in EUV efficiency over previous missions. ESCAPE employs a grazing incidence telescope that feeds an EUV and FUV spectrograph. The ESCAPE science instrument builds on previous ultraviolet and X-ray instrumentation, grazing incidence optical systems, and photon-counting ultraviolet detectors used on NASA astrophysics, heliophysics, and planetary science missions. The ESCAPE spacecraft bus is the versatile and high-heritage Ball Aerospace BCP Small spacecraft. Data archives will be housed at the Mikulski Archive for Space Telescopes (MAST).\",\n",
       "  'len': 1314},\n",
       " {'abstract': \"System-scale magnetohydrodynamic (MHD) waves within Earth's magnetosphere are often understood theoretically using box models. While these have been highly instructive in understanding many fundamental features of the various wave modes present, they neglect the complexities of geospace such as the inhomogeneities and curvilinear geometries present. Here we show global MHD simulations of resonant waves impulsively-excited by a solar wind pressure pulse. Although many aspects of the surface, fast magnetosonic (cavity/waveguide), and Alfvén modes present agree with the box and axially symmetric dipole models, we find some predictions for large-scale waves are significantly altered in a realistic magnetosphere. The radial ordering of fast mode turning points and Alfvén resonant locations may be reversed even with monotonic wave speeds. Additional nodes along field lines that are not present in the displacement/velocity occur in both the perpendicular and compressional components of the magnetic field. Close to the magnetopause the perpendicular oscillations of the magnetic field have the opposite handedness to the velocity. Finally, widely-used detection techniques for standing waves, both across and along the field, can fail to identify their presence. We explain how all these features arise from the MHD equations when accounting for a non-uniform background field and propose modified methods which might be applied to spacecraft observations.\",\n",
       "  'len': 1475},\n",
       " {'abstract': 'In this paper we describe a machine learning based framework for spacecraft swarm trajectory planning. In particular, we focus on coordinating motions of multi-spacecraft in formation flying through passive relative orbit(PRO) transfers. Accounting for spacecraft dynamics while avoiding collisions between the agents makes spacecraft swarm trajectory planning difficult. Centralized approaches can be used to solve this problem, but are computationally demanding and scale poorly with the number of agents in the swarm. As a result, centralized algorithms are ill-suited for real time trajectory planning on board small spacecraft (e.g. CubeSats) comprising the swarm. In our approach a neural network is used to approximate solutions of a centralized method. The necessary training data is generated using a centralized convex optimization framework through which several instances of the n=10 spacecraft swarm trajectory planning problem are solved. We are interested in answering the following questions which will give insight on the potential utility of deep learning-based approaches to the multi-spacecraft motion planning problem: 1) Can neural networks produce feasible trajectories that satisfy safety constraints (e.g. collision avoidance) and low in fuel cost? 2) Can a neural network trained using n spacecraft data be used to solve problems for spacecraft swarms of differing size?',\n",
       "  'len': 1407},\n",
       " {'abstract': 'Robotic spacecraft have helped expand our reach for many planetary exploration missions. Most ground mobile planetary exploration robots use wheeled or modified wheeled platforms. Although extraordinarily successful at completing intended mission goals, because of the limitations of wheeled locomotion, they have been largely limited to benign, solid terrain and avoided extreme terrain with loose soil/sand and large rocks. Unfortunately, such challenging terrain is often scientifically interesting for planetary geology. Although many animals traverse such terrain at ease, robots have not matched their performance and robustness. This is in major part due to a lack of fundamental understanding of how effective locomotion can be generated from controlled interaction with complex terrain on the same level of flight aerodynamics and underwater vehicle hydrodynamics. Early fundamental understanding of legged and limbless locomotor-ground interaction has already enabled stable and efficient bio-inspired robot locomotion on relatively flat ground with small obstacles. Recent progress in the new field of terradynamics of locomotor-terrain interaction begins to reveal the principles of bio-inspired locomotion on loose soil/sand and over large obstacles. Multi-legged and limbless platforms using terradynamics insights hold the promise for serving as robust alternative platforms for traversing extreme extraterrestrial terrain and expanding our reach in planetary exploration.',\n",
       "  'len': 1498},\n",
       " {'abstract': 'The Spitzer Space Telescope operated for over 16 years in an Earth-trailing solar orbit, returning not only a wealth of scientific data but, as a by-product, spacecraft and instrument engineering data which will be of interest to future mission planners. These data will be particularly useful because Spitzer operated in an environment essentially identical to that at the L2 LaGrange point where many future astrophysics missions will operate. In particular, the radiative cooling demonstrated by Spitzer has been adopted by other infrared space missions, from JWST to SPHEREx. This paper aims to facilitate the utility of the Spitzer engineering data by collecting the more unique and potentially useful portions into a single, readily-accessible publication. We avoid discussion of less unique systems, such as the telecom, flight software, and electronics systems and do not address the innovations in mission and science operations which the Spitzer team initiated. These and other items of potential interest are addressed in references supplied in an appendix to this paper.',\n",
       "  'len': 1093},\n",
       " {'abstract': \"Global coronal models seek to produce an accurate physical representation of the Sun's atmosphere which can be used, for example, to drive space weather models. Assessing their accuracy is a complex task and there are multiple observational pathways to provide constraints and tune model parameters. Here, we combine several such independent constraints, defining a model-agnostic framework for standardized comparison. We require models to predict the distribution of coronal holes at the photosphere, and neutral line topology at the model outer boundary. We compare these predictions to extreme ultraviolet (EUV) observations of coronal hole locations, white-light Carrington maps of the streamer belt and the magnetic sector structure measured \\\\textit{in situ} by Parker Solar Probe and 1AU spacecraft. We study these metrics for Potential Field Source Surface (PFSS) models as a function of source surface height and magnetogram choice, as well as comparing to the more physical Wang-Sheeley-Arge (WSA) and the Magnetohydrodynamics Algorithm outside a Sphere (MAS) models. We find that simultaneous optimization of PFSS models to all three metrics is not currently possible, implying a trade-off between the quality of representation of coronal holes and streamer belt topology. WSA and MAS results show the additional physics they include addresses this by flattening the streamer belt while maintaining coronal hole sizes, with MAS also improving coronal hole representation relative to WSA. We conclude that this framework is highly useful for inter- and intra-model comparisons. Integral to the framework is the standardization of observables required of each model, evaluating different model aspects.\",\n",
       "  'len': 1722},\n",
       " {'abstract': 'We investigate the effects of the evolutionary processes in the internal magnetic structure of two interplanetary coronal mass ejections (ICMEs) detected in situ between 2020 November 29 and December 1 by Parker Solar Probe (PSP). The sources of the ICMEs were observed remotely at the Sun in EUV and subsequently tracked to their coronal counterparts in white light. This period is of particular interest to the community since it has been identified as the first widespread solar energetic particle event of Solar Cycle 25. The distribution of various solar and heliospheric-dedicated spacecraft throughout the inner heliosphere during PSP observations of these large-scale magnetic structures enables a comprehensive analysis of the internal evolution and topology of such structures. By assembling different models and techniques, we identify the signatures of interaction between the two consecutive ICMEs and the implications for their internal structure. We use multispacecraft observations in combination with a remote-sensing forward modeling technique, numerical propagation models, and in-situ reconstruction techniques. The outcome, from the full reconciliations, demonstrates that the two CMEs are interacting in the vicinity of PSP. Thus, we identify the in-situ observations based on the physical processes that are associated with the interaction and collision of both CMEs. We also expand the flux rope modeling and in-situ reconstruction technique to incorporate the aging and expansion effects in a distorted internal magnetic structure and explore the implications of both effects in the magnetic configuration of the ICMEs.',\n",
       "  'len': 1655},\n",
       " {'abstract': 'We present a study of the current state of knowledge concerning spacecraft operations and potential hazards while operating near a comet nucleus. Starting from simple calculations comparing the cometary coma environment to benign conditions on Earth, we progress to sophisticated engineering models of spacecraft behavior, and then confront these models with recent spacecraft proximity operations experience. Finally, we make recommendations from lessons learned for future spacecraft missions that enter into orbit around a comet for long-term operations. All of these considerations indicate that, with a proper spacecraft design and operations planning, the near-nucleus environment can be a relatively safe region in which to operate, even for an active short period comet near perihelion with gas production rates as high as 1e29 molecules/s. With gas densities similar to those found in good laboratory vacuums, dust densities similar to Class 100 cleanrooms, dust particle velocities of 10s of m/s, and microgravity forces that permit slow and deliberate operations, the conditions around a comet are generally more benign than a typical day on Mars. Even in strong dust jets near the nucleus surface, dust densities tend to be only a few grains/cm3, about the same as in a typical interior room on Earth. Stochastic forces on a modern spacecraft with tens of square meters of projected surface area can be accounted for using modern Attitude Control Systems to within tens of meters navigation error; surface contamination issues are only important for spacecraft spending months to years within a few kilometers of the nucleus surface; and the issues the Rosetta spacecraft faced, confusion of celestial star trackers by sunlit dust particles flying past the spacecraft, will be addressed using the next generation of star trackers implementing improved transient rejection algorithms.',\n",
       "  'len': 1906},\n",
       " {'abstract': 'LISA will detect gravitational wave (GW) in the milli-Hz frequency band in space. Time-delay interferometry (TDI) is developed to suppress laser frequency noise beneath the acceleration noise and optical metrology noise. To identify stochastic GW signals, it would be required to characterize these noise components entangled in TDI data streams. In this work, we demonstrate noises characterization by combining the first-generation TDI channels from Michelson and Relay configurations. By assuming stationary Gaussian noise in three-year observation, the combination of Michelson and Relay could effectively break the degeneracy between the different optical path noises on three spacecraft. Based on the characterized noises, we further reconstruct the power spectrum density of noise in the selected TDI channel. Two cases are performed to characterize the spectrum shape of a stochastic GW signal. For a modeled signal, its parameter(s) could be directly estimated from the TDI data, and its spectrum could be recovered from the inferred values. And for an unexpected signal, its spectrum may be recognized and retrieved from noise-subtracted residual in which its power spectrum density surpasses the noise level.',\n",
       "  'len': 1230},\n",
       " {'abstract': 'Various exact laws governing compressible magnetohydrodynamic (MHD) and Hall-MHD (CHMHD) turbulence have been derived in recent years. Other than their fundamental theoretical interest, these laws are generally used to estimate the energy dissipation rate from spacecraft observations in order to address diverse problems related, e.g., to heating of the solar wind (SW) and magnetospheric plasmas. Here we use various $1024^3$ direct numerical simulation (DNS) data of free-decay isothermal CHMHD turbulence obtained with the GHOST code (Geophysical High-Order Suite for Turbulence) to analyze two of the recently derived exact laws. The simulations reflect different intensities of the initial Mach number and the background magnetic field. The analysis demonstrates the equivalence of the two laws in the inertial range and relates the strength of the Hall effect to the amplitude of the cascade rate at sub-ion scales. When taken in their general form (i.e., not limited to the inertial range) some subtleties regarding the validity of the stationarity assumption or the absence of the forcing in the simulations are discussed. We show that the free-decay nature of the turbulence induces a shift from a large scale forcing towards the presence of a scale-dependent reservoir of energy fueling the cascade or dissipation. The reduced form of the exact laws (valid in the inertial range) ultimately holds even if the stationarity assumption is not fully verified.',\n",
       "  'len': 1477},\n",
       " {'abstract': 'Satellite constellation missions, consisting of a large number of spacecraft, are increasingly being launched or planned. Such missions require novel control approaches, in particular for what concerns orbital phasing maneuvers. In this context, we consider the problem of reconfiguration of a satellite constellation in a circular formation. In our scenario, a formation of equally spaced spacecraft need to undergo an autonomous reconfiguration due to the deorbiting of a satellite in the formation. The remaining spacecraft have to reconfigure to form again an equidistant formation. To achieve this goal, we consider two decentralized strategies that rely on different sets of information about the neighboring spacecraft in the formation. In the fully decentralized case, each controller knows only the current states of each spacecraft, i.e. position and velocity, while in the second decentralized strategy with with information sharing, the entire planned nominal trajectory of each spacecraft is available to its neighbors. Our numerical simulation results show that, by increasing the amount of information available to each spacecraft, faster reconfiguration maneuvers with smaller fuel consumption can be achieved.',\n",
       "  'len': 1237},\n",
       " {'abstract': 'The estimate of the change rate of the solar gravitational parameter $\\\\mathrm{d}(GM_{\\\\odot})/\\\\mathrm{d}t$ is obtained from processing modern positional observations of planets and spacecraft. Observations were processed and parameters were determined basing on the numerical planetary ephemeris EPM2019. The obtained annual decrease in solar mass $M_{\\\\odot}$ accounts for the loss through radiation ${\\\\dot M}_{{\\\\odot}\\\\mathrm{rad}}$, through the outgoing solar wind ${\\\\dot M}_{{\\\\odot}{\\\\mathrm{wind}}}$, and for the material falling on the Sun ${\\\\dot M}_{{\\\\odot}\\\\mathrm{fall}}$. The estimated relative value is within $-13.4\\\\cdot 10^{-14} < (\\\\dot M_{\\\\odot} /M_{\\\\odot})_{\\\\mathrm{rad}+\\\\mathrm{wind}+ \\\\mathrm{fall}} < -8.7\\\\cdot 10^{-14}$ per year. The following range for the change rate of the gravitational constant $G$ was obtained: $-2.9 \\\\cdot 10^{-14} < \\\\dot G /G < + 4.6 \\\\cdot 10^{-14}$ per year $(3\\\\sigma$). The new result reduces the interval for the change in $G$ and narrows the limits of possible deviations for alternative gravitational theories from general relativity.',\n",
       "  'len': 1088},\n",
       " {'abstract': 'We examine a sampling of 23 polar-coronal-hole jets. We first identified the jets in soft X-ray (SXR) images from the X-ray telescope (XRT) on the Hinode spacecraft, over 2014-2016. During this period, frequently the polar holes were small or largely obscured by foreground coronal haze, often making jets difficult to see. We selected 23 jets among those adequately visible during this period, and examined them further using Solar Dynamics Observatory (SDO) Atmospheric Imaging Assembly (AIA) 171, 193, 211, and 304 Ang images. In SXRs we track the lateral drift of the jet spire relative to the jet base\\'s jet bright point (JBP). In 22 of 23 jets the spire either moves away from (18 cases) or is stationary relative to (4 cases) the JBP. The one exception where the spire moved toward the JBP may be a consequence of line-of-sight projection effects at the limb. From the AIA images, we clearly identify an erupting minifilament in 20 of the 23 jets, while the remainder are consistent with such an eruption having taken place. We also confirm that some jets can trigger onset of nearby \"sympathetic\" jets, likely because eruption of the minifilament field of the first jet removes magnetic constraints on the base-field region of the second jet. The propensity for spire drift away from the JBP, the identification of the erupting minifilament in the majority of jets, and the magnetic-field topological changes that lead to sympathetic jets, all support or are consistent with the minifilament-eruption model for jets.',\n",
       "  'len': 1535},\n",
       " {'abstract': \"Saturn's C ring contains multiple structures that appear to be density waves driven by time-variable anomalies in the planet's gravitational field. Semi-empirical extensions of density wave theory enable the observed wave properties to be translated into information about how the pattern speeds and amplitudes of these gravitational anomalies have changed over time. Combining these theoretical tools with wavelet-based analyses of data obtained by the Visual and Infrared Mapping Spectrometer (VIMS) onboard the Cassini spacecraft reveals a suite of structures in Saturn's gravity field with azimuthal wavenumber 3, rotation rates between 804 degrees/day and 842 degrees/day and local gravitational potential amplitudes between 30 and 150 cm^2/s^2. Some of these anomalies are transient, appearing and disappearing over the course of a few Earth years, while others persist for decades. Most of these persistent patterns appear to have roughly constant pattern speeds, but there is at least one structure in the planet's gravitational field whose rotation rate steadily increased between 1970 and 2010. This gravitational field structure appears to induce two different asymmetries in the planet's gravity field, one with azimuthal wavenumber 3 that rotates at roughly 810 degrees/day and another with azimuthal wavenumber 1 rotating three times faster. The atmospheric processes responsible for generating the latter pattern may involve solar tides.\",\n",
       "  'len': 1463},\n",
       " {'abstract': \"The AGC was designed with the sole purpose of providing navigational guidance and spacecraft control during the Apollo program throughout the 1960s and early 1970s. The AGC sported 72kb of ROM, 4kb of RAM, and a whopping 14,245 FLOPS, roughly 30 million times fewer than the computer this report is being written on. These limitations are what make the AGC so interesting, as its programmers had to ration each individual word of memory due to the bulk of memory technology of the time. Despite these limitations (or perhaps due to them), the AGC was highly optimized, and arguably the most advanced computer of its time, as its computational power was only matched in the late 1970s by computers like the Apple II. It is safe to say that the AGC had no intended market, and was explicitly designed to enhance control of the Apollo Command Module and Apollo Lunar Module. The AGC was not entirely internal to NASA, however, and was designed in MIT's Instrumentation Laboratory, and manufactured by Raytheon, a weapons and defense contractor.\",\n",
       "  'len': 1052},\n",
       " {'abstract': 'We report on the energy dependence of galactic cosmic rays (GCRs) in the very local interstellar medium (VLISM) as measured by the Low Energy Charged Particle (LECP) instrument on the Voyager 1 (V1) spacecraft. The LECP instrument includes a dual-ended solid state detector particle telescope mechanically scanning through 360 deg across eight equally-spaced angular sectors. As reported previously, LECP measurements showed a dramatic increase in GCR intensities for all sectors of the >=211 MeV count rate (CH31) at the V1 heliopause (HP) crossing in 2012, however, since then the count rate data have demonstrated systematic episodes of intensity decrease for particles around 90° pitch angle. To shed light on the energy dependence of these GCR anisotropies over a wide range of energies, we use V1 LECP count rate and pulse height analyzer (PHA) data from >=211 MeV channel together with lower energy LECP channels. Our analysis shows that while GCR anisotropies are present over a wide range of energies, there is a decreasing trend in the amplitude of second-order anisotropy with increasing energy during anisotropy episodes. A stronger pitch-angle scattering at the higher velocities is argued as a potential cause for this energy dependence. A possible cause for this velocity dependence arising from weak rigidity dependence of the scattering mean free path and resulting velocity-dominated scattering rate is discussed. This interpretation is consistent with a recently reported lack of corresponding GCR electron anisotropies.',\n",
       "  'len': 1550},\n",
       " {'abstract': 'Spacecraft data reveals a nonuniform ambipolar electric field transverse to the magnetic field in a thin magnetotail current sheet that leads to intense ExB velocity shear and non-gyrotropic particle distributions. The ExB drift far exceeds the diamagnetic drift and drives lower hybrid waves localized to the magnetic field reversal region, which is ideally suited for the anomalous dissipation necessary for reconnection. It also reveals substructures embedded in the current density, indicating the formation of a non-ideal current sheet.',\n",
       "  'len': 552},\n",
       " {'abstract': 'Infrared space interferometers can surpass the spatial resolution limitations of single-dish space telescopes. However, stellar interferometers from space have not been realized because of technical difficulties. Two beams coming from individual satellites separated by more than a few tens of meters should precisely interfere such that the optical-path and angular differences between the two beams are reduced at the wavelength level. Herein, we propose a novel beam combiner for space interferometers that records the spectrally-resolved interferometric fringes using the densified pupil spectroscopic technique. As the detector plane is optically conjugated to a plane, on which the two beams interfere, we can directly measure the relative phase difference between the two beams. Additionally, when an object within the field of view is obtained with a modest signal-to-noise ratio, we can extract the true complex amplitude from a continuous broadband fringe (i.e., one exposure measurement), without scanning a delay line and chopping interferometry. We discovered that this spectral imaging method is validated for observing the solar system objects by simulating the reflected light from Europa with a small stellar interferometer. However, because the structure of the object spectrum may cause a systematic error in the measurement, this method may be limited in extracting the true complex amplitude for other astronomical objects. Applying this spectral imaging method to general astrophysics will facilitate further research. The beam combiner and spectral imaging method are applied to a formation-flying stellar interferometer with multiple small satellites in a Sun-synchronous orbit for observation of the solar system objects in visible and near-infrared. We present an overview of SEIRIOS and the optimized optical design for a limited-volume spacecraft.',\n",
       "  'len': 1886},\n",
       " {'abstract': 'Theories of stellar convective core overshoot can be examined through analysis of pulsating stars. Better accuracy can be achieved by obtaining external constraints such as those provided by observing pulsating stars in eclipsing binary systems, but this requires that the binary parameters be identified so photometric variations of the pulsating component may be isolated from the binary periodicity. This study aims to uncover the physical parameters of three binaries observed by the Kepler spacecraft. We also seek to evaluate the feasibility of accurately constraining binaries using only readily available time-series photometry and distance estimates. Binary models were constructed using the Physics of Eclipsing Binaries (PHOEBE) software package. Markov Chain Monte Carlo methods were used to sample the parameter space of these models and provide estimates of the posterior distributions for these systems. An initial run using binned light curve data was performed to identify general parameter trends and provide initializing distributions for a subsequent analysis incorporating the full data set. We present theoretical models for all three binaries, along with posterior distributions from our MCMC analyses. Models for KIC 8314879 and KIC 10727668 produced a good match to the observed data, while the model of KIC 5957123 failed to generate an appropriate synthetic light curve. For the two successful models, we interpret the posterior distributions and discuss confidence in our parameter estimates and uncertainties. We also evaluate the feasibility of this procedure in various contexts, and propose several modifications to improve the success of future studies.',\n",
       "  'len': 1697},\n",
       " {'abstract': 'A resultant gravitational force due to the current estimates of the virial mass of the Milky Way galaxy, dominated by dark matter, is estimated near the Sun and is described in two different analytical models yielding consistent results. One is a two-step Hernquist model, the other is a Navarro-Frenk-White model. The effect of this force is estimated on trajectories for spacecraft sufficiently far from the Sun. The difficulty of detecting this force is studied. It is concluded that its effect should be considered for certain spacecraft missions. Its effect on the Pioneer and New Horizons spacecrafts is discussed. A future mission is discussed that may be able to detect this force. Implications of this force are discussed with its impact for problems in planetary astronomy and astrophysics.',\n",
       "  'len': 811},\n",
       " {'abstract': 'In this comment of the article [arXiv:2002.01150] \"Locating the source field lines of Jovian decametric radio emissions\" by YuMing Wang et al., 2020, we discuss the assumptions used by the authors to compute the beaming angle of Jupiter s decametric emissions induced by the moon Io. Their method, relying on multi-point radio observations, was applied to a single event observed on 14th March 2014 by Wind and both STEREO A/B spacecraft from 5 to 16 MHz, and erroneously identified as a northern emission (Io-B type) instead of a southern one (Io-D type). We encourage the authors to update their results with the right hemisphere of origin and to test their method on a larger sample of Jupiter-Io emissions.',\n",
       "  'len': 721},\n",
       " {'abstract': 'We investigate obstacles of superluminal \"warp drive\" travels from interactions with interstellar matter and from curvature effects. The effect of collision of interstellar dust particles and photons with the spacecraft will all lead to a pressure proportional to the apparent velocity of the spaceship $v_s$. The force exerted on the spacecraft from the curvature effect has two non-trivial components. The radial and longitudinal components scales as $v_s^2$ and $v_s^4$ respectively. The above obstacles become increasingly important when the spaceship travels at high superluminal speeds.',\n",
       "  'len': 603},\n",
       " {'abstract': 'Model-Based Systems Engineering aims at creating a model of a system under development, covering the complete system with a level of detail that allows to define and understand its behavior and enables to define any interface and workpackage based on the model. Once such a model is established, further benefits can be reaped, such as the analysis of complex technical correlations within the system. Various insights can be gained by displaying the model as a formal graph and querying it. To enable such queries, a graph schema needs to be designed, which allows to transfer the model into a graph database. In the course of this paper, we discuss the design of a graph schema and MBSE modelling approach, enabling deep going system analysis and anomaly resolution in complex embedded systems. The schema and modelling approach are designed to answer questions such as what happens if there is an electrical short in a component? Which other components are now offline and which data cannot be gathered anymore? Or if a condition cannot be met, which alternative routes can be established to reach a certain state of the system. We build on the use case of qualification and operations of a small spacecraft. Structural and behavioral elements of the MBSE model are transferred to a graph database where analyses are conducted on the system. The schema is implemented by an adapter for MagicDraw to Neo4j. A selection of complex analyses are shown on the example of the MOVE-II space mission.',\n",
       "  'len': 1506},\n",
       " {'abstract': 'Using the New Horizons LORRI camera, we searched for satellites near five Kuiper belt objects (KBOs): four cold classicals (CCs: 2011 JY31, 2014 OS393, 2014 PN70, 2011 HZ102) and one scattered disk object (SD: 2011 HK103). These objects were observed at distances of 0.092-0.290 au from the New Horizons spacecraft, achieving spatial resolutions of 136-430 km (resolution is ~2 camera pixels), much higher than possible from any other facilities. Here we report that CC 2011 JY31 is a binary system with roughly equal brightness components, CC 2014 OS393 is likely an equal brightness binary system, while the three other KBOs did not show any evidence of binarity. The 2011 JY31 binary has a semi-major axis of 198.6 +/- 2.9 km, an orbital inclination of 61.34 +/- 1.34 deg, and an orbital period of 1.940 +/- 0.002 d. The 2014 OS393 binary objects have an apparent separation of ~150 km, making 2011 JY31 and 2014 OS393 the tightest KBO binary systems ever resolved. Both 2011 HK103 and 2011 HZ102 were detected with SNR~10, and our observations rule out equal brightness binaries with separations larger than ~430 km and ~260 km, respectively. The spatial resolution for 2014 PN70 was ~200 km, but this object had SNR~2.5-3, which limited our ability to probe its binarity. The binary frequency for the CC binaries probed in our small survey (67%, not including 2014 PN70) is consistent with the high binary frequency suggested by larger surveys of CCs (Fraser et al. 2017, Noll et al. 2020) and recent planetesimal formation models (Nesvorny et al. 2021), but we extend the results to smaller orbit semi-major axes and smaller objects than previously possible.',\n",
       "  'len': 1675},\n",
       " {'abstract': 'Space situational awareness typically makes use of physical measurements from radar, telescopes, and other assets to monitor satellites and other spacecraft for operational, navigational, and defense purposes. In this work we explore using textual input for the space situational awareness task. We construct a corpus of 48.5k news articles spanning all known active satellites between 2009 and 2020. Using a dependency-rule-based extraction system designed to target three high-impact events -- spacecraft launches, failures, and decommissionings, we identify 1,787 space-event sentences that are then annotated by humans with 15.9k labels for event slots. We empirically demonstrate a state-of-the-art neural extraction system achieves an overall F1 between 53 and 91 per slot for event extraction in this low-resource, high-impact domain.',\n",
       "  'len': 852},\n",
       " {'abstract': 'BepiColombo ESA/JAXA mission is currently in its 7 year cruise phase towards Mercury. The Mercury orbiter radioscience experiment (MORE), one of the 16 experiments of the mission, will start its scientific investigation during the superior solar conjunction (SSC) in March 2021 with a test of general relativity (GR). Other solar conjunctions will follow during the cruise phase, providing several opportunities to improve the results of the first experiment. MORE radio tracking system allows to establish precise ranging and Doppler measurements almost at all solar elongation angles (up to 7-8 solar radii), thus providing an accurate measurement of the relativistic time delay and frequency shift experienced by a radio signal during an SSC. The final objective of the experiment is to place new limits to the accuracy of the GR as a theory of gravity in the weak-field limit. As in all gravity experiments, non-gravitational accelerations acting on the spacecraft are a major concern. Because of the proximity to the Sun, the spacecraft will undergo severe solar radiation pressure acceleration, and the effect of the random fluctuations of the solar irradiance may become a significant source of spacecraft buffeting. In this paper we address the problem of a realistic estimate of the outcome of the SSC experiments of BepiColombo, by including in the dynamical model the effects of random variations in the solar irradiance. We propose a numerical method to mitigate the impact of the variable solar radiation pressure on the outcome of the experiment. Our simulations show that, with different assumptions on the solar activity and observation coverage, the accuracy attainable in the estimation of $\\\\gamma$ lays in the range $[6, 13]\\\\cdot10^{-6}$.',\n",
       "  'len': 1768},\n",
       " {'abstract': \"To settle the question of the nature of the interstellar object 1I/'Oumuamua requires in-situ observations via a spacecraft, as the object is already out of range of existing telescopes. Most previous proposals for reaching 1I/'Oumuamua using near-term technologies are based on the Solar Oberth Manoeuvre (SOM), as trajectories without the SOM are generally significantly inferior in terms of lower mission duration and higher total velocity requirement. While the SOM allows huge velocity gains, it is also technically challenging and thereby increases programmatic and mission-related risks. In this paper, we identify an alternative route to the interstellar object 1I/'Oumuamua, based on a launch in 2028, which does not require a SOM but has a similar performance as missions with a SOM. It instead employs a Jupiter Oberth Manoeuvre (JOM) with a total time of flight of around 26 years or so. The efficacy of this trajectory is a result of it significantly reducing the $\\\\Delta$V to Jupiter by exploiting the VEEGA sequence. The total $\\\\Delta$V of the trajectory is 15.8 $kms^{-1}$ and the corresponding payload mass is 115 kg for a SLS Block 1B or 241 kg for a Block 2. A further advantage of the JOM is that the arrival speed relative to 1I/'Oumuamua is approximately 18 $kms^{-1}$, much lower than the equivalent for the SOM of around 30 $kms^{-1}$.\",\n",
       "  'len': 1370},\n",
       " {'abstract': 'Tether-net launched from a chaser spacecraft provides a promising method to capture and dispose of large space debris in orbit. This tether-net system is subject to several sources of uncertainty in sensing and actuation that affect the performance of its net launch and closing control. Earlier reliability-based optimization approaches to design control actions however remain challenging and computationally prohibitive to generalize over varying launch scenarios and target (debris) state relative to the chaser. To search for a general and reliable control policy, this paper presents a reinforcement learning framework that integrates a proximal policy optimization (PPO2) approach with net dynamics simulations. The latter allows evaluating the episodes of net-based target capture, and estimate the capture quality index that serves as the reward feedback to PPO2. Here, the learned policy is designed to model the timing of the net closing action based on the state of the moving net and the target, under any given launch scenario. A stochastic state transition model is considered in order to incorporate synthetic uncertainties in state estimation and launch actuation. Along with notable reward improvement during training, the trained policy demonstrates capture performance (over a wide range of launch/target scenarios) that is close to that obtained with reliability-based optimization run over an individual scenario.',\n",
       "  'len': 1446},\n",
       " {'abstract': \"To better understand the physical processes associated with Jovian decametric (DAM) radio emissions, we present the statistical study of DAMs and inferred characteristics of DAM sources based on multi-view observation from Wind and STEREO spacecraft. The distribution of the apparent rotation speed of DAMs derived from multiple spacecraft suggests that the rotation speed of Io-related DAMs is in range of 0.15-0.6{\\\\Omega}_J and that of non-Io-DAMs is between 0.7-1.2{\\\\Omega}_J. Based on the method of Wang et al. (2020), we locate the sources of the DAMs and infer their emission angles and associated electron energies. The statistical results show that the DAM source locations have three preferred regions, two in the southern hemisphere and one in the northern hemisphere, which is probably caused by the non-symmetrical topology of Jupiter's magnetic field. The difference between Io-DAM source footprints and Io auroral spots changes with the Io's position in longitude, consistent with the previous results from Hess et al. (2010), Bonfond et al. (2017) and Hinton et al. (2019). In addition, the emission angles for non-Io-DAMs are smaller than that for Io-DAMs from the same source regions and all the emission angles range from 60° to 85°. Correspondingly, the electron energy is mainly distributed between 0.5 and 20 keV.\",\n",
       "  'len': 1345},\n",
       " {'abstract': \"Relativistic spacecraft, like those proposed by the NASA Starlight program and the Breakthrough Starshot Initiative, will have to survive radiation production that is unique when compared to that experienced by conventional spacecraft. In a relativistic interstellar spacecraft's reference frame, the interstellar medium (ISM) will look like a nearly mono-energetic beam of charged particles which impinges upon the leading edge of the spacecraft. Upon impact, ISM protons and electrons will travel characteristic lengths through the spacecraft shield and come to a stop via electronic and nuclear stopping mechanisms. As a result, bremsstrahlung photons will be produced within the spacecraft shield. In this work, we discuss the interstellar environment and its implications for radiation damage on relativistic spacecraft. We also explore expected radiation doses in terms of on-board device radiation tolerance.\",\n",
       "  'len': 926},\n",
       " {'abstract': 'The slow solar wind is generally believed to result from the interaction of open and closed coronal magnetic flux at streamers and pseudostreamers. We use 3-dimensional magnetohydrodynamic simulations to determine the detailed structure and dynamics of open-closed interactions that are driven by photospheric convective flows. The photospheric magnetic field model includes a global dipole giving rise to a streamer together with a large parasitic polarity region giving rise to a pseudostreamer that separates a satellite coronal hole from the main polar hole. Our numerical domain extends out to 30 solar radii and includes an isothermal solar wind, so that the coupling between the corona and heliosphere can be calculated rigorously. This system is driven by imposing a large set of quasi-random surface flows that capture the driving of coronal flux in the vicinity of streamer and pseudostreamer boundaries by the supergranular motions. We describe the resulting structures and dynamics. Interchange reconnection dominates the evolution at both streamer and pseudostreamer boundaries, but the details of the resulting structures are clearly different from one another. Additionally, we calculate in situ signatures of the reconnection and determine the dynamic mapping from the inner heliosphere back to the Sun for a test spacecraft orbit. We discuss the implications of our results for interpreting observations from inner heliospheric missions, such as Parker Solar Probe and Solar Orbiter, and for space weather modeling of the slow solar wind.',\n",
       "  'len': 1566},\n",
       " {'abstract': 'Magnetospheres are a ubiquitous feature of magnetized bodies embedded in a plasma flow. While large planetary magnetospheres have been studied for decades by spacecraft, ion-scale \"mini\" magnetospheres can provide a unique environment to study kinetic-scale, collisionless plasma physics in the laboratory to help validate models of larger systems. In this work, we present preliminary experiments of ion-scale magnetospheres performed on a unique high-repetition-rate platform developed for the Large Plasma Device (LAPD) at UCLA. The experiments utilize a high-repetition-rate laser to drive a fast plasma flow into a pulsed dipole magnetic field embedded in a uniform magnetized background plasma. 2D maps of magnetic field with high spatial and temporal resolution are measured with magnetic flux probes to examine the evolution of magnetosphere and current density structures for a range of dipole and upstream parameters. The results are further compared to 2D PIC simulations to identify key observational signatures of the kinetic-scale structures and dynamics of the laser-driven plasma. We find that distinct 2D kinetic-scale magnetopause and diamagnetic current structures are formed at higher dipole moments, and their locations are consistent with predictions based on pressure balances and energy conservation.',\n",
       "  'len': 1335},\n",
       " {'abstract': \"DESTINY+ is an upcoming JAXA Epsilon medium-class mission to fly by the Geminids meteor shower parent body (3200) Phaethon. It will be the world's first spacecraft to escape from a near-geostationary transfer orbit into deep space using a low-thrust propulsion system. In doing so, DESTINY+ will demonstrate a number of technologies that include a highly efficient ion engine system, lightweight solar array panels, and advanced asteroid flyby observation instruments. These demonstrations will pave the way for JAXA's envisioned low-cost, high-frequency space exploration plans. Following the Phaethon flyby observation, DESTINY+ will visit additional asteroids as its extended mission. The mission design is divided into three phases: a spiral-shaped apogee-raising phase, a multi-lunar-flyby phase to escape Earth, and an interplanetary and asteroids flyby phase. The main challenges include the optimization of the many-revolution low-thrust spiral phase under operational constraints; the design of a multi-lunar-flyby sequence in a multi-body environment; and the design of multiple asteroid flybys connected via Earth gravity assists. This paper shows a novel, practical approach to tackle these complex problems, and presents feasible solutions found within the mass budget and mission constraints. Among them, the baseline solution is shown and discussed in depth; DESTINY+ will spend two years raising its apogee with ion engines, followed by four lunar gravity assists, and a flyby of asteroids (3200) Phaethon and (155140) 2005 UD. Finally, the flight operations plan for the spiral phase and the asteroid flyby phase are presented in detail.\",\n",
       "  'len': 1665},\n",
       " {'abstract': \"The stellar occultation technique provides competitive accuracy in determining the sizes, shapes, astrometry, etc., of the occulting body, comparable to in-situ observations by spacecraft. With the increase in the number of known Solar System objects expected from the LSST, the highly precise astrometric catalogues, such as Gaia, and the improvement of ephemerides, occultations observations will become more common with a higher number of chords in each observation. In the context of the Big Data era, we developed SORA, an open-source python library to reduce and analyse stellar occultation data efficiently. It includes routines from predicting such events up to the determination of Solar System bodies' sizes, shapes, and positions.\",\n",
       "  'len': 752},\n",
       " {'abstract': 'Modern and future high precision pointing space missions face increasingly high challenges related to the widespread use of large flexible structures. The development of new modeling tools which are able to account for the multidisciplinary nature of this problem becomes extremely relevant in order to meet both structure and control performance criteria. This paper proposes a novel methodology to analytically model large truss structures in a sub-structuring framework. A three dimensional unit cube element has been designed and validated with a Finite Element commercial software. This model is composed by multiple two-dimensional sub-mechanisms assembled using block-diagram models. This constitutes the building block for constructing complex truss structures by repetitions of the element. The accurate vibration description of the system and its minimal representation, as well as the possibility of accounting for parametric uncertainties in its mechanical parameters, make it an appropriate tool to perform robust Structure/Control co-design. In order to demonstrate the strengths of the proposed approach, a co-design study case is proposed by combining a multidisciplinary optimization approach based on particle swarm algorithm and multiple structured robust Hinf-synthesis. This has been used to optimize the pointing performances of an high pointing antenna, minimizing the perturbations coming from the Solar Array Mechanisms (SADM) of two solar panels, performing active control by means of multiple proof mass actuators, and simultaneously reduce the mass of the truss-structure which connects the antenna to the main spacecraft body.',\n",
       "  'len': 1666},\n",
       " {'abstract': \"The DART spacecraft will impact Didymos's secondary, Dimorphos, at the end of 2022 and cause a change in the orbital period of the secondary. For simplicity, most previous numerical simulations of the impact used a spherical projectile geometry to model the DART spacecraft. To investigate the effects of alternative, simple projectile geometries on the DART impact outcome we used the iSALE shock physics code in two and thee-dimensions to model vertical impacts of projectiles with a mass and speed equivalent to the nominal DART impact, into porous basalt targets. We found that the simple projectile geometries investigated here have minimal effects on the crater morphology and momentum enhancement. Projectile geometries modelled in two-dimensions that have similar surface areas at the point of impact, affect the crater radius and the crater volume by less than 5%. In the case of a more extreme projectile geometry (i.e., a rod, modelled in three-dimensions), the crater was elliptical and 50% shallower compared to the crater produced by a spherical projectile of the same momentum. The momentum enhancement factor in these test cases, commonly referred to as beta, was within 7% for the 2D simulations and within 10% for the 3D simulations, of the value obtained for a uniform spherical projectile. The most prominent effects of projectile geometry are seen in the ejection velocity as a function of launch position and ejection angle of the fast ejecta that resides in the so-called `coupling zone'. These results will inform the LICIACube ejecta cone analysis.\",\n",
       "  'len': 1584},\n",
       " {'abstract': \"Context. During the post-perihelion phase of the European Space Agency's Rosetta mission to comet 67P, the Optical, Spectroscopic, and Infrared Remote Imaging System on board the spacecraft took numerous image sequences of the near-nucleus coma, with many showing the motion of individual pieces of debris ejected from active surface areas into space. Aims. We aim to track the motion of individual particles in these image sequences and derive their projected velocities and accelerations. This should help us to constrain their point of origin on the surface, understand the forces that influence their dynamics in the inner coma, and predict whether they will fall back to the surface or escape to interplanetary space. Methods. We have developed an algorithm that tracks the motion of particles appearing as point sources in image sequences. Our algorithm employs a point source detection software to locate the particles and then exploits the image sequences' pair-nature to reconstruct the particle tracks and derive the projected velocities and accelerations. We also constrained the particle size from their brightness. Results. Our algorithm identified 2268 tracks in a sample image sequence. Manual inspection not only found that 1187 (~52%) of them are likely genuine, but in combination with runs on simulated data it also revealed a simple criterion related to the completeness of a track to single out a large subset of the genuine tracks without the need for manual intervention. A tentative analysis of a small (n = 89) group of particles exemplifies how our data can be used, and provides first results on the particles' velocity, acceleration, and radius distributions, which agree with previous work.\",\n",
       "  'len': 1730},\n",
       " {'abstract': 'Reflected light photometry of terrestrial exoplanets could reveal the presence of oceans and continents, hence placing direct constraints on the current and long-term habitability of these worlds. Inferring the albedo map of a planet from its observed light curve is challenging because different maps may yield indistinguishable light curves. This degeneracy is aggravated by changing clouds. It has previously been suggested that disk-integrated photometry spanning multiple days could be combined to obtain a cloud-free surface map of an exoplanet. We demonstrate this technique as part of a Bayesian retrieval by simultaneously fitting for the fixed surface map of a planet and the time-variable overlying clouds. We test this approach on synthetic data then apply it to real disk-integrated observations of the Earth. We find that eight days of continuous synthetic observations are sufficient to reconstruct a faithful low resolution surface albedo map, without needing to make assumptions about cloud physics. For lightcurves with negligible photometric uncertainties, the minimal top-of-atmosphere albedo at a location is a good estimate of its surface albedo. When applied to observations from the Earth Polychromating Imaging Camera aboard the DSCOVR spacecraft, our approach removes only a small fraction of clouds. We attribute this difficulty to the full-phase geometry of observations combined with the short correlation length for Earth clouds. For exoplanets with Earth-like climatology, it may be hard to do much better than a cloud-averaged map. We surmise that cloud removal will be most successful for exoplanets imaged near quarter phase that harbour large cloud systems.',\n",
       "  'len': 1703},\n",
       " {'abstract': 'During its commissioning phase in 2020, the Spectrometer/Telescope for Imaging X-rays (STIX) on board the Solar Orbiter spacecraft observed 69 microflares. The two most significant events from this set (of GOES class B2 and B6) were observed on-disk from the spacecraft as well as from Earth and analysed in terms of the spatial, temporal, and spectral characteristics. We complement the observations from the STIX instrument with EUV imagery from SDO/AIA and GOES soft X-ray data by adding imaging and plasma diagnostics over different temperature ranges for a detailed microflare case study that is focussed on energy release and transport. Spectral fitting of the STIX data shows clear nonthermal emission for both microflares studied here. The deduced plasma parameters from DEM reconstruction as well as spectral fitting roughly agree with the values in the literature for microflares as do the nonthermal fit parameters from STIX. The observed Neupert effects and impulsive and gradual phases indicate that both events covered in this study are consistent with the standard chromospheric evaporation flare scenario. For the B6 event on 7 June 2020, this interpretation is further supported by the temporal evolution seen in the DEM profiles of the flare ribbons and loops. For this event, we also find that accelerated electrons can roughly account for the required thermal energy. The 6 June 2020 event demonstrates that STIX can detect nonthermal emission for GOES class B2 events that is nonetheless smaller than the background rate level. We demonstrate for the first time how detailed multi-instrument studies of solar flares can be performed with STIX.',\n",
       "  'len': 1675},\n",
       " {'abstract': 'The application of directed energy to spacecraft mission design is explored using rapid transit to Mars as the design objective. An Earth-based laser array of unprecedented size (10~m diameter) and power (100~MW) is assumed to be enabled by ongoing developments in photonic laser technology. A phased-array laser of this size and incorporating atmospheric compensation would be able to deliver laser power to spacecraft in cislunar space, where the incident laser is focused into a hydrogen heating chamber via an inflatable reflector. The hydrogen propellant is then exhausted through a nozzle to realize specific impulses of 3000 s. The architecture is shown to be immediately reusable via a burn-back maneuver to return the propulsion unit while still within range of the Earth-based laser. The ability to tolerate much greater laser fluxes enables realizing the combination of high thrust and high specific impulse, making this approach favorable in comparison to laser-electric propulsion and occupying a parameter space similar to gas-core nuclear thermal rockets (without the requisite reactor). The heating chamber and its associated regenerative cooling and propellant handling systems are crucial elements of the design that receive special attention in this study. The astrodynamics and the extreme aerocapture maneuver required at Mars arrival after a 45-day transit are also analyzed in detail. The application of laser-thermal propulsion as an enabling technology for other rapid transit missions in the solar system and beyond is discussed.',\n",
       "  'len': 1566},\n",
       " {'abstract': 'We present analysis of 17,043 proton kinetic-scale current sheets collected over 124 days of Wind spacecraft measurements in the solar wind at 11 Samples/s magnetic field resolution. The current sheets have thickness $\\\\lambda$ from a few tens to one thousand kilometers with typical value around 100 km or from about 0.1 to 10$\\\\lambda_{p}$ in terms of local proton inertial length $\\\\lambda_{p}$. We found that the current density is larger for smaller scale current sheets, $J_0\\\\approx 6\\\\; {\\\\rm nA/m^2} \\\\cdot (\\\\lambda/100\\\\;{\\\\rm km})^{-0.56}$ , but does not statistically exceed critical value $J_A$ corresponding to the drift between ions and electrons of local Alvén speed. The observed trend holds in normalized units, $J_0/J_{A}\\\\approx 0.17\\\\cdot (\\\\lambda/\\\\lambda_{p})^{-0.51}$. The current sheets are statistically force-free with magnetic shear angle correlated with current sheet spatial scale, $\\\\Delta \\\\theta\\\\approx 19^{\\\\circ}\\\\cdot (\\\\lambda/\\\\lambda_{p})^{0.5}$. The observed correlations are consistent with local turbulence being the source of proton kinetic-scale current sheets in the solar wind, while mechanisms limiting the current density remain to be understood.',\n",
       "  'len': 1187},\n",
       " {'abstract': 'Magnetic reconnection has been suggested to play an important role in the dynamics and energetics of plasma turbulence by spacecraft observations, simulations and theory over the past two decades, and recently, by magnetosheath observations of MMS. A new method based on magnetic flux transport (MFT) has been developed to identify reconnection activity in turbulent plasmas. This method is applied to a gyrokinetic simulation of two-dimensional (2D) plasma turbulence. Results on the identification of three active reconnection X-points are reported. The first two X-points have developed bi-directional electron outflow jets. Beyond the category of electron-only reconnection, the third X-point does not have bi-directional electron outflow jets because the flow is modified by turbulence. In all cases, this method successfully identifies active reconnection through clear inward and outward flux transport around the X-points. This transport pattern defines reconnection and produces a new quadrupolar structure in the divergence of MFT. This method is expected to be applicable to spacecraft missions such as MMS, Parker Solar Probe, and Solar Orbiter.',\n",
       "  'len': 1168},\n",
       " {'abstract': 'Understanding the geological modification processes on asteroids can provide information concerning their surface history. Images of small asteroids from spacecraft show a depletion in terms of smaller craters. Seismic shaking was considered to be responsible for crater erasure and the main driver modifying the geology of asteroids via regolith convection or the Brazil nut effect. However, a recent artificial impact experiment on the asteroid Ryugu by the Japanese Hayabusa2 mission revealed minimal seismic activity. To investigate whether a seismic shaking model can reproduce the observed crater record, the crater distribution on Ryugu was analyzed using crater production functions under cohesionless conditions. Crater retention ages were estimated as a function of crater diameter for Ryugu, Itokawa, Eros, and Bennu using the crater size-frequency distribution and crater production function estimated for those asteroids. We found that the power-law indices \"a\" are inconsistent with diffusion processes (e.g., seismic shaking, a=2). This result suggests that seismic shaking models based on diffusion equations cannot explain the crater distribution on small asteroids. Alternative processes include surface flows, possibly at the origin of geomorphological and spectral features of Ryugu. We demonstrate that the vertical mixing of material at depths shallower than 1 m occurs over 10^3-10^5 yr by cratering and obliteration. The young surface age of Ryugu is consistent with the slow space weathering that results from cratering, as suggested in previous studies. The timescale (10^4-10^6 yr) required for resurfacing at depths of 2-4 m can be compared with the cosmic-ray exposure ages of returned samples to constrain the distribution of impactors that collide with Ryugu.',\n",
       "  'len': 1801},\n",
       " {'abstract': 'Large scale directed energy offers the possibility of radical transformation in a variety of areas, including the ability to achieve relativistic flight that will enable the first interstellar missions, as well as rapid interplanetary transit. In addition, the same technology will allow for long-range beamed power for ion, ablation, and thermal engines, as well as long-range recharging of distant spacecraft, long-range and ultra high bandwidth laser communications, and many additional applications that include remote composition analysis, manipulation of asteroids, and full planetary defense. Directed energy relies on photonics which, like electronics, is an exponentially expanding growth area driven by diverse economic interests that allows transformational advances in space exploration and capability. We have made enormous technological progress in the last few years to enable this long-term vision. In addition to the technological challenges, we must face the economic challenges to bring the vision to reality. The path ahead requires a fundamental change in the system designs to allow for the radical cost reductions required. To afford the full-scale realization of this vision we will need to bring to fore integrated photonics and mass production as a path forward. Fortunately, integrated photonics is a technology driven by vast consumer need for high speed data delivery. We outline the fundamental physics that drive the economics and derive an analytic cost model that allows us to logically plan the path ahead.',\n",
       "  'len': 1551},\n",
       " {'abstract': 'Accurate and stable spacecraft pointing is a requirement of many astronomical observations. Pointing particularly challenges nanosatellites because of an unfavorable surface area to mass ratio and proportionally large volume required for even the smallest attitude control systems. This work explores the limitations on astrophysical attitude knowledge and control in a regime unrestricted by actuator precision or actuator-induced disturbances such as jitter. The external disturbances on an archetypal 6U CubeSat are modeled and the limiting sensing knowledge is calculated from the available stellar flux and grasp of a telescope within the available volume. These inputs are integrated using a model-predictive control scheme. For a simple test case at 1 Hz, with an 85 mm telescope and a single 11th magnitude star, the achievable body pointing is predicted to be 0.39 arcseconds. For a more general limit, integrating available star light, the achievable attitude sensing is approximately 1 milliarcsecond, which leads to a predicted body pointing accuracy of 20 milliarcseconds after application of the control model. These results show significant room for attitude sensing and control systems to improve before astrophysical and environmental limits are reached.',\n",
       "  'len': 1282},\n",
       " {'abstract': 'The Laser Interferometer Space Antenna (LISA) aims to observe gravitational waves in the mHz regime over its 10-year mission time. LISA will operate laser interferometers between three spacecrafts. Each spacecraft will utilize independent clocks which determine the sampling times of onboard phasemeters to extract the interferometric phases and, ultimately, gravitational wave signals. To suppress limiting laser frequency noise, signals sampled by each phasemeter need to be combined in post-processing to synthesize virtual equal-arm interferometers. The synthesis in turn requires a synchronization of the independent clocks. This article reports on the experimental verification of a clock synchronization scheme down to LISA performance levels using a hexagonal optical bench. The development of the scheme includes data processing that is expected to be applicable to the real LISA data with minor modifications. Additionally, some noise coupling mechanisms are discussed.',\n",
       "  'len': 990},\n",
       " {'abstract': 'Space biology research aims to understand fundamental effects of spaceflight on organisms, develop foundational knowledge to support deep space exploration, and ultimately bioengineer spacecraft and habitats to stabilize the ecosystem of plants, crops, microbes, animals, and humans for sustained multi-planetary life. To advance these aims, the field leverages experiments, platforms, data, and model organisms from both spaceborne and ground-analog studies. As research is extended beyond low Earth orbit, experiments and platforms must be maximally autonomous, light, agile, and intelligent to expedite knowledge discovery. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration on artificial intelligence, machine learning, and modeling applications which offer key solutions toward these space biology challenges. In the next decade, the synthesis of artificial intelligence into the field of space biology will deepen the biological understanding of spaceflight effects, facilitate predictive modeling and analytics, support maximally autonomous and reproducible experiments, and efficiently manage spaceborne data and metadata, all with the goal to enable life to thrive in deep space.',\n",
       "  'len': 1272},\n",
       " {'abstract': 'Human space exploration beyond low Earth orbit will involve missions of significant distance and duration. To effectively mitigate myriad space health hazards, paradigm shifts in data and space health systems are necessary to enable Earth-independence, rather than Earth-reliance. Promising developments in the fields of artificial intelligence and machine learning for biology and health can address these needs. We propose an appropriately autonomous and intelligent Precision Space Health system that will monitor, aggregate, and assess biomedical statuses; analyze and predict personalized adverse health outcomes; adapt and respond to newly accumulated data; and provide preventive, actionable, and timely insights to individual deep space crew members and iterative decision support to their crew medical officer. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration, on future applications of artificial intelligence in space biology and health. In the next decade, biomonitoring technology, biomarker science, spacecraft hardware, intelligent software, and streamlined data management must mature and be woven together into a Precision Space Health system to enable humanity to thrive in deep space.',\n",
       "  'len': 1289},\n",
       " {'abstract': \"This paper presents a continuous optimization approach to DCOC and its application to spacecraft high-precision attitude control. The approach computes a control input sequence that maximizes the time-before-exit by solving a nonlinear programming problem with an exponentially weighted cost function and purely continuous variables. Based on results from sensitivity analysis and exact penalty method, we prove the optimality guarantee of our approach. The practical application of our approach is demonstrated through a spacecraft high-precision attitude control example. A nominal case with three functional reaction wheels (RWs) and an underactuated case with only two functional RWs were considered. Simulation results illustrate the effectiveness of our approach as a contingency method for extending spacecraft's effective mission time in the case of RW failures.\",\n",
       "  'len': 881},\n",
       " {'abstract': 'Current research focuses on designing fast trajectories to the trans-Neptunian object (TNO) (90377) Sedna to study the surface and composition from a close range. Studying Sedna from a close distance can provide unique data about the Solar System evolution process including protoplanetary disc and related mechanisms. The trajectories to Sedna are determined considering flight time and the total characteristic velocity (${\\\\Delta}V$) constraints. The time of flight for the analysis was limited to 20 years. The direct flight, the use of gravity assist manoeuvres near Venus, the Earth and the giant planets Jupiter and Neptune, and the flight with the Oberth manoeuvre near the Sun are considered. It is demonstrated that the use of flight scheme with ${\\\\Delta}VEGA$ (${\\\\Delta}V$ and Earth Gravity Assist manoeuvre) and Jupiter-Neptune gravity assist leads to the lowest cost of ${\\\\Delta}V$=6.13 km/s for launch in 2041. The maximum payload for schemes with ${\\\\Delta}$VEGA manoeuvre is 500 kg using Soyuz 2.1.b, 2,000 kg using Proton-M and Delta IV Heavy and exceeds $12,000$ kg using SLS. For schemes with only Jupiter gravity assist, payload mass is twice less than for ones with ${\\\\Delta}$VEGA manoeuvre. As a possible expansion of the mission to Sedna, it is proposed to send a small spacecraft to another TNO during the primary flight to Sedna. Five TNOs suitable for this scenario are found, three extreme TNOs 2012 VP113, (541132) Leleākūhonua (former 2015 TG387), 2013 SY99) and two classical Kuiper Belt objects: (90482) Orcus, (20000) Varuna.',\n",
       "  'len': 1566},\n",
       " {'abstract': 'In the last years, a new generation of interplanetary space missions have been designed for the exploration of the solar system. At the same time, radio-science instrumentation has reached an unprecedented level of accuracy, leading to a significant improvement of our knowledge of celestial bodies. Along with this hardware upgrade, software products for interplanetary missions have been greatly refined. In this context, we introduce Orbit14, a precise orbit determination software developed at the University of Pisa for processing the radio-science data of the BepiColombo and Juno missions. Along the years, many tools have been implemented into the software and Orbit14 capitalized the experience coming from simulations and treatment of real data. In this paper, we present a review of orbit determination methods developed at the University of Pisa for radio-science experiments of interplanetary missions. We describe the basic theory of the process of parameters estimation and refined methods necessary to have full control on experiments involving spacecraft orbiting millions of kilometers far from the Earth. Our aim is to give both an extensive description of the treatment of radio-science experiments and step-to-step instructions for those who are approaching the field of orbit determination in the context of space missions. We show also the work conducted on the Juno and BepiColombo missions by means of the Orbit14 software. In particular, we summarize the recent results obtained with the gravity experiment of Juno and the simulations performed so far for the gravimetry-rotation and relativity experiments of BepiColombo.',\n",
       "  'len': 1659},\n",
       " {'abstract': 'Recent advances in optical atomic clocks and optical time transfer have enabled new possibilities in precision metrology for both tests of fundamental physics and timing applications. Here we describe a space mission concept that would place a state-of-the-art optical atomic clock in an eccentric orbit around Earth. A high stability laser link would connect the relative time, range, and velocity of the orbiting spacecraft to earthbound stations. The primary goal for this mission would be to test the gravitational redshift, a classical test of general relativity, with a sensitivity 30,000 times beyond current limits. Additional science objectives include other tests of relativity, enhanced searches for dark matter and drifts in fundamental constants, and establishing a high accuracy international time/geodesic reference.',\n",
       "  'len': 842},\n",
       " {'abstract': \"Accurate gravity field models are essential for safe proximity operations around small bodies. State-of-the-art techniques use spherical harmonics or high-fidelity polyhedron shape models. Unfortunately, these techniques can become inaccurate near the surface of the small body or have high computational costs, especially for binary or heterogeneous small bodies. New learning-based techniques do not encode a predefined structure and are more versatile. In exchange for versatility, learning-based techniques can be less robust outside the training data domain. In deployment, the spacecraft trajectory is the primary source of dynamics data. Therefore, the training data domain should include spacecraft trajectories to accurately evaluate the learned model's safety and robustness. We have developed a novel method for learning-based gravity models that directly uses the spacecraft's past trajectories. We further introduce a method to evaluate the safety and robustness of learning-based techniques via comparing accuracy within and outside of the training domain. We demonstrate this safety and robustness method for two learning-based frameworks: Gaussian processes and neural networks. Along with the detailed analysis provided, we empirically establish the need for robustness verification of learned gravity models when used for proximity operations.\",\n",
       "  'len': 1372},\n",
       " {'abstract': 'This study presents a C3.0 flare observed by the BBSO/GST and IRIS, on 2018-May-28 around 17:10 UT. The Near Infrared Imaging Spectropolarimeter (NIRIS) of GST was set to spectral imaging mode to scan five spectral positions at $\\\\pm$ 0.8 Å, $\\\\pm$ 0.4 Åand line center of He I 10830. At the flare ribbon\\'s leading edge the line is observed to undergo enhanced absorption, while the rest of the ribbon is observed to be in emission. When in emission, the contrast compared to the pre-flare ranges from about $30~\\\\%$ to nearly $100~\\\\%$ at different spectral positions. Two types of spectra, \"convex\" shape with higher intensity at line core and \"concave\" shape with higher emission in the line wings, are found at the trailing and peak flaring areas, respectively. On the ribbon front, negative contrasts, or enhanced absorption, of about $\\\\sim 10\\\\% - 20\\\\%$ appear in all five wavelengths. This observation strongly suggests that the negative flares observed in He I 10830 with mono-filtergram previously were not caused by pure Doppler shifts of this spectral line. Instead, the enhanced absorption appears to be a consequence of flare energy injection, namely non-thermal collisional ionization of helium caused by the precipitation of high energy electrons, as found in our recent numerical modeling results. In addition, though not strictly simultaneous, observations of Mg II from the IRIS spacecraft, show an obvious central reversal pattern at the locations where enhanced absorption of He I 10830 is seen, which is in consistent with previous observations.',\n",
       "  'len': 1572},\n",
       " {'abstract': 'Active visual tracking of space non-cooperative object is significant for future intelligent spacecraft to realise space debris removal, asteroid exploration, autonomous rendezvous and docking. However, existing works often consider this task into different subproblems (e.g. image preprocessing, feature extraction and matching, position and pose estimation, control law design) and optimize each module alone, which are trivial and sub-optimal. To this end, we propose an end-to-end active visual tracking method based on DQN algorithm, named as DRLAVT. It can guide the chasing spacecraft approach to arbitrary space non-cooperative target merely relied on color or RGBD images, which significantly outperforms position-based visual servoing baseline algorithm that adopts state-of-the-art 2D monocular tracker, SiamRPN. Extensive experiments implemented with diverse network architectures, different perturbations and multiple targets demonstrate the advancement and robustness of DRLAVT. In addition, We further prove our method indeed learnt the motion patterns of target with deep reinforcement learning through hundreds of trial-and-errors.',\n",
       "  'len': 1159},\n",
       " {'abstract': 'Sheaths ahead of coronal mass ejections (CMEs) are large heliospheric structures that form with CME expansion and propagation. Turbulent and compressed sheaths contribute to the acceleration of particles in the corona and in interplanetary space, but the relation of their internal structures to particle energization is still relatively little studied. In particular, the role of sheaths in accelerating particles when the shock Mach number is low is a significant open problem. This work seeks to provide new insights on the internal structure of CME sheaths with regard to energetic particle enhancements. A good opportunity to achieve this aim was provided by observations of a sheath made by radially aligned spacecraft at 0.8 and $\\\\sim$ 1 AU (Solar Orbiter, Wind, ACE and BepiColombo) on 19-21 April 2020. The sheath was preceded by a weak shock. Energetic ion enhancements occurred at different locations within the sheath structure at Solar Orbiter and L1. Magnetic fluctuation amplitudes at inertial-range scales increased in the sheath relative to the upstream wind. However, when normalised to the local mean field, fluctuation amplitudes did not increase significantly; magnetic compressibility of fluctuation also did not increase. Various substructures were embedded within the sheath at the different spacecraft, including multiple heliospheric current sheet (HCS) crossings and a small-scale flux rope. At L1, the ion flux enhancement was associated with the HCS crossings, while at Solar Orbiter, the enhancement occurred within the rope. Substructures that are swept from the upstream solar wind and compressed in the sheath can act as particularly effective acceleration sites. A possible acceleration mechanism is betatron acceleration associated with the small-scale flux rope and the warped HCS in the sheath.',\n",
       "  'len': 1842},\n",
       " {'abstract': 'Hayabusa2, a Japanese sample-return mission to a C-type asteroid, arrived at its target 162173 Ryugu in June 2018. The optical navigation cameras (ONC-T, ONC-W1, ONC-W2) successfully obtained numerous images of Ryugu. ONC-T is a telescopic framing camera with a charge-coupled device (CCD), has seven filter bands in ultraviolet, visible and near infrared wavelength ranges, and were used to map the spectral distribution of the Ryugu surface. Since the locations of a target seen in ONC-T images are slightly different among different wavelength images in one multi-band observation sequence due to changes in spacecraft positions and attitudes during the filter-changing sequence, one of the image processing issues is image co-registration among images for different wavelength bands. To quickly complete the image co-registration to meet a limited mission schedule, we combined conventional image co-registration techniques with several improvements based on previous planetary missions. The results of our analysis using actual ONC-T images indicate that image co-registration can reach accuracy on the order of 0.1 pixels, which is sufficient for many spectral mapping applications for Ryugu analyses.',\n",
       "  'len': 1218},\n",
       " {'abstract': \"Results. We found that gas-phase silicon was present throughout the Rosetta mission. Furthermore, the presence of sodium and iron atoms near the comet's perihelion confirms that sputtering cannot be the sole release process for refractory elements into the gas phase. Nickel was found to be below the detection limit. The search for parent species of any of the identified gas phase refractories has not been successful. Upper limits for a suite of possible fragment species (SiH, SiC, NaH, etc.) of larger parent and daughter species have been obtained. Furthermore, Si did not exhibit the same drop in signal as do common cometary gases when the spacecraft is pointed away from the nucleus. The combined results suggest that a direct release of elemental species from small grains on the surface of the nucleus or from small grains in the surrounding coma is a more likely explanation than the previous assumption of release via the dissociation of gaseous parent molecules.\",\n",
       "  'len': 987},\n",
       " {'abstract': \"A model that combines celestial geometry and atmospheric physics is used to calculate the dimming of artificial satellites as they enter and exit the Earth's shadow. Refraction of sunlight by the terrestrial atmosphere can illuminate a satellite while it is inside the eclipse region determined from geometry alone. Meanwhile, refraction combines with atmospheric absorption to dim the satellites for tens of km outside of that region. Spacecraft brightness is reduced more in blue light than in red because absorption of sunlight is stronger at shorter wavelengths. Observations from the MMT-9 robotic observatory are consistent with the model predictions. Tables of satellite brightness as functions of their location in the eclipse region are provided.\",\n",
       "  'len': 766},\n",
       " {'abstract': 'Afterglow light curves of gamma-ray bursts (GRBs) exhibit very complex temporal and spectral features, such as a sudden intensity jump about one hour after the prompt emission in the optical band. We model this feature through the late collision of two relativistic shells and investigate the corresponding high-energy neutrino emission within a multi-messenger framework, while contrasting our findings with the ones from the classic fireball model. For a constant density circumburst medium, the total number of emitted neutrinos can increase by about an order of magnitude within a dynamical time when an optical jump occurs with respect to the self-similar afterglow scenario. By exploring the detection prospects with the IceCube Neutrino Observatory and future radio arrays such as IceCube-Gen2 radio, RNO-G and GRAND200k, as well as the POEMMA spacecraft, we conclude that the detection of neutrinos with IceCube-Gen2 radio could enable us to constrain the fraction of GRB afterglows with a jump as well as the properties of the circumburst medium. We also investigate the neutrino signal expected for the afterglows of GRB 100621A and a GRB 130427A-like burst with an optical jump. The detection of neutrinos from GRB afterglows could be crucial to explore the yet-to-be unveiled mechanism powering the optical jumps.',\n",
       "  'len': 1336},\n",
       " {'abstract': 'Context. Solar Orbiter and PSP jointly observed the solar wind for the first time in June 2020, capturing data from very different solar wind streams, calm and Alfvénic wind as well as many dynamic structures. Aims. The aim here is to understand the origin and characteristics of the highly dynamic solar wind observed by the two probes, in particular in the vicinity of the heliospheric current sheet (HCS). Methods. We analyse the plasma data obtained by PSP and Solar Orbiter in situ during the month of June 2020. We use the Alfvén-wave turbulence MHD solar wind model WindPredict-AW, and perform two 3D simulations based on ADAPT solar magnetograms for this period. Results. We show that the dynamic regions measured by both spacecraft are pervaded with flux ropes close to the HCS. These flux ropes are also present in the simulations, forming at the tip of helmet streamers, i.e. at the base of the heliospheric current sheet. The formation mechanism involves a pressure driven instability followed by a fast tearing reconnection process, consistent with the picture of Réville et al. (2020a). We further characterize the 3D spatial structure of helmet streamer born flux ropes, which seems, in the simulations, to be related to the network of quasi-separatrices.',\n",
       "  'len': 1281},\n",
       " {'abstract': 'Magnetic reconnection plays an important role in the release of magnetic energy and consequent energization of particles in collisionless plasmas. Energy transfer in collisionless magnetic reconnection is inherently a two-step process: reversible, collisionless energization of particles by the electric field, followed by collisional thermalization of that energy, leading to irreversible plasma heating. Gyrokinetic numerical simulations are used to explore the first step of electron energization, and we generate the first examples of field-particle correlation (FPC) signatures of electron energization in 2D strong-guide-field collisionless magnetic reconnection. We determine these velocity space signatures at the x-point and in the exhaust, the regions of the reconnection geometry in which the electron energization primarily occurs. Modeling of these velocity-space signatures shows that, in the strong-guide-field limit, the energization of electrons occurs through bulk acceleration of the out-of-plane electron flow by parallel electric field that drives the reconnection, a non-resonant mechanism of energization. We explore the variation of these velocity-space signatures over the plasma beta range $0.01 \\\\le \\\\beta_i \\\\le 1$. Our analysis goes beyond the fluid picture of the plasma dynamics and exploits the kinetic features of electron energization in the exhaust region to propose a single-point diagnostic which can potentially identify a reconnection exhaust region using spacecraft observations.',\n",
       "  'len': 1528},\n",
       " {'abstract': 'Detecting anomalies in time-varying multivariate data is crucial in various industries for the predictive maintenance of equipment. Numerous machine learning (ML) algorithms have been proposed to support automated anomaly identification. However, a significant amount of human knowledge is still required to interpret, analyze, and calibrate the results of automated analysis. This paper investigates current practices used to detect and investigate anomalies in time series data in industrial contexts and identifies corresponding needs. Through iterative design and working with nine experts from two industry domains (aerospace and energy), we characterize six design elements required for a successful visualization system that supports effective detection, investigation, and annotation of time series anomalies. We summarize an ideal human-AI collaboration workflow that streamlines the process and supports efficient and collaborative analysis. We introduce MTV (Multivariate Time Series Visualization), a visual analytics system to support such workflow. The system incorporates a set of novel visualization and interaction designs to support multi-faceted time series exploration, efficient in-situ anomaly annotation, and insight communication. Two user studies, one with 6 spacecraft experts (with routine anomaly analysis tasks) and one with 25 general end-users (without such tasks), are conducted to demonstrate the effectiveness and usefulness of MTV.',\n",
       "  'len': 1477},\n",
       " {'abstract': 'Wakes behind spacecraft caused by supersonic drifting positive ions are common in plasmas and disturb in situ measurements. We review the impact of wakes on observations by the Electric Field and Wave double-probe instruments on the Cluster satellites. In the solar wind, the equivalent spacecraft charging is small compared to the ion drift energy and the wake effects are caused by the spacecraft body and can be compensated for. We present statistics of the direction, width, and electrostatic potential of wakes, and we compare with an analytical model. In the low-density magnetospheric lobes, the equivalent positive spacecraft charging is large compared to the ion drift energy and an enhanced wake forms. In this case observations of the geophysical electric field with the double-probe technique becomes extremely challenging. Rather, the wake can be used to estimate the flux of cold (eV) positive ions. For an intermediate range of parameters, when the equivalent charging of the spacecraft is similar to the drift energy of the ions, also the charged wire booms of a double-probe instrument must be taken into account. We discuss an example of these effects from the MMS spacecraft near the magnetopause. We find that many observed wake characteristics provide information that can be used for scientific studies. An important example is the enhanced wakes used to estimate the outflow of ionospheric origin in the magnetospheric lobes to about ${10}^{26}$ cold (eV) ions/s, constituting a large fraction of the mass outflow from planet Earth.',\n",
       "  'len': 1566},\n",
       " {'abstract': 'The gravitational harmonics measured from Juno and Cassini spacecrafts help us to specify the internal structure and chemical elements of Jupiter and Saturn, respectively. However, we still do not know much about the impact of rotation on the planetary internal structure as well as their formation. The centrifugal force induced by rotation deforms the planetary shape and partially counteracts the gravitational force. Thus, rotation will affect the critical core mass of the exoplanet. Once the atmospheric mass becomes comparable to the critical core mass, the planet will enter the runaway accretion phase and becomes a gas giant. We have confirmed that the critical core masses of rotating planets depend on the stiffness of the polytrope, the outer boundary conditions, and the thickness of the isothermal layer. The critical core mass with Bondi boundary condition is determined by the surface properties. The critical core mass of a rotating planet will increase with the core gravity (i.e., the innermost density). For the Hill boundary condition, the soft polytrope shares the same properties as planets with Bondi boundary condition. Since the total mass for planets with Hill boundary condition increases with the decrease of the polytropic index, higher core gravity is required for rotating planets. As a result, the critical core mass in the stiff Hill model sharply increases. The rotation effects become more important when the radiative and convective regions coexist. Besides, the critical core mass of planets with Hill (Bondi) boundary increases noticeably as the radiative layer becomes thinner (thicker).',\n",
       "  'len': 1639},\n",
       " {'abstract': \"At least two active plumes were observed on Neptune's moon Triton during the Voyager 2 flyby in 1989. Models for Triton's plumes have previously been grouped into five hypotheses, two of which are primarily atmospheric phenomena and are generally considered unlikely, and three of which include eruptive processes and are plausible. These hypotheses are compared, including new arguments, such as comparisons based on current understanding of Mars, Enceladus, and Pluto. An eruption model based on a solar-powered, solid-state greenhouse effect was previously considered the leading hypothesis for Triton's plumes, in part due to the proximity of the plumes to the subsolar latitude during the Voyager 2 flyby and the distribution of Triton's fans that are putatively deposits from former plumes. The other two eruption hypotheses are powered by internal heat, not solar insolation. Based on new analyses of the ostensible relation between the latitude of the subsolar point on Triton and the geographic locations of the plumes and fans, we argue that neither the locations of the plumes nor fans are strong evidence in favor of the solar-powered hypothesis. We conclude that all three eruption hypotheses should be considered further. Five tests are presented that could be implemented with remote sensing observations from future spacecraft to confidently distinguish among the eruption hypotheses for Triton's plumes. The five tests are based on the: (1) composition and thickness of Triton's southern hemisphere terrains, (2) composition of fan deposits, (3) distribution of active plumes, (4) distribution of fans, and (5) surface temperature at the locations of plumes and/or fans. The tests are independent, but complementary, and implementable with a single flyby mission such as the Trident mission concept. We note that, in the case of the solar-driven hypothesis, the 2030s and 2040s may be the last ...\",\n",
       "  'len': 1925},\n",
       " {'abstract': \"Asteroidal impact threats to the Earth will be predicted a century or more in advance. Changing an asteroid's albedo changes the force of Solar radiation on it, and hence its orbit. Albedo may be changed by applying a thin ($\\\\sim 0.1\\\\,\\\\mu$) reflective coat of alkali metal, dispensed as vapor by an orbiting spacecraft. A complete coat reduces the effective Solar gravity, changing the orbital period. A Tunguska-class (50 m diameter) asteroid in a nominal orbit with perihelion 1 AU and aphelion 3 AU ($a = 2\\\\,$AU, $e = 0.5$) may be displaced along its path by $\\\\sim 1000\\\\,$km in 100 years, sufficient to avoid impact in a populated area, by application of one kg of lithium or sodium metal over its entire surface. Alternatively, coating one hemisphere of an asteroid in an elliptical orbit may produce a Solar radiation torque, analogous to but distinct from the Yarkovsky effect, displacing it by an Earth radius in $\\\\sim 200$ years. The time required scales as the square root of the asteroid's diameter (the 1/6 power of its mass) because the displacement increases quadratically with time, making it possible to prevent the catastrophic impact of a km-sized asteroid with a minimal mass.\",\n",
       "  'len': 1205},\n",
       " {'abstract': \"Magnetic reconnection can explosively release magnetic energy when opposing magnetic fields merge and annihilate through a current sheet, driving plasma jets and accelerating non-thermal particle populations to high energy, in plasmas ranging from space and astrophysical to laboratory scales. Through laboratory experiments and spacecraft observations, significant experimental progress has been made in demonstrating how fast dissipation and reconnection occurs in narrow, kinetic-scale current sheets. However, a challenge has been to demonstrate what triggers reconnection and how it proceeds rapidly and efficiently as part of a global system much larger than these kinetic scales. Here we show experimentally the full development of a process where the current sheet forms and then breaks up into multiple current-carrying structures at the ion kinetic scale. The results are consistent with tearing of the current sheet, however modified by collisionless kinetic ion effects, which leads to a larger growth rate and number of plasmoids than observed in previous experiments or compared to predictions from standard tearing instability theory and previous non-linear kinetic reconnection simulations. This effect will increase the role of plasmoid instabilities in many natural reconnection systems and should be considered in triggering rapid reconnection in a broad range of natural plasmas with collisionless, compressible flows, including at the Earth's magnetosheath and magnetotail and at the heliopause, in accretion disks, and in turbulent high-Mach-number collisionless shocks.\",\n",
       "  'len': 1603},\n",
       " {'abstract': 'Atmospheric drag calculation error greatly reduce the low-earth orbit spacecraft trajectory prediction fidelity. To solve the issue, the \"correction - prediction\" strategy is usually employed. In the method, one parameter is fixed and other parameters are revised by inverting spacecraft orbit data. However, based on a single spacecraft data, the strategy usually performs poorly as parameters in drag force calculation are coupled with each other, which result in convoluted errors. A gravity field recovery and atmospheric density detection satellite, Q-Sat, developed by xxxxx Lab at xxx University, is launched on August 6th, 2020. The satellite is designed to be spherical for a constant drag coefficient regardless of its attitude. An orbit prediction method for low-earth orbit spacecraft with employment of Q-Sat data is proposed in present paper for decoupling atmospheric density and drag coefficient identification. For the first step, by using a dynamic approach-based inversion, several empirical atmospheric density models are revised based on Q-Sat orbit data. Depending on the performs, one of the revised atmospheric density model would be selected for the next step in which the same inversion is employed for drag coefficient identification for a low-earth orbit operating spacecraft whose orbit needs to be predicted. Finally, orbit prediction is conducted by extrapolation with the dynamic parameters in the previous steps. Tests are carried out with the proposed method by using a GOCE satellite 15-day continuous orbit data. Compared with legacy \"correction - prediction\" method in which only GOCE data is employed, the accuracy of the 24-hour orbit prediction is improved by about 171m the highest for the proposed method. 14-day averaged 24-hour prediction precision is elevated by approximately 70m.',\n",
       "  'len': 1837},\n",
       " {'abstract': \"The solar gravitational lens (SGL) offers unique capabilities for high-resolution imaging of faint, distant objects, such as exoplanets. In the near future, a spacecraft carrying a meter-class telescope with a solar coronagraph would be placed in the focal region of the SGL. That region begins at ~547 astronomical units from the Sun and occupies the vicinity of the target-specific primary optical axis - the line that connects the center of the target and that of the Sun. This axis undergoes complex motion as the exoplanet orbits its host star, as that star moves with respect to the Sun, and even as the Sun itself moves with respect to the solar system's barycenter due to the gravitational pull of planets in our solar system. An image of an extended object is projected by the SGL into an image plane and moves within that plane, responding to the motion of the optical axis. To sample the image, a telescope must follow the projection with precise knowledge of its own position with respect to the image. We consider the dominant motions that determine the position of the focal line as a function of time. We evaluate the needed navigational capability for the telescope to conduct a multiyear exoplanet imaging mission. We show that even in a rather conservative case, when an Earth-like exoplanet is in our stellar neighborhood at $\\\\sim10$ light years, the motion of the image is characterized by a small total acceleration $\\\\sim 6.1\\\\,\\\\mu {\\\\rm m/s}^2$ that is driven primarily by the orbital motion of the exoplanet and by the reflex motion of our Sun. We discuss how the amplified light of the host star allows establishing a local reference frame thus relaxing navigational requirements. We conclude that the required navigation in the SGL's focal region, although complex, can be accurately modeled and a $\\\\sim 10$-year imaging mission is achievable with the already available propulsion technology.\",\n",
       "  'len': 1926},\n",
       " {'abstract': 'The correlation between the plasma density measured in space and the surface potential of an electrically conducting satellite body with biased electric field detectors has been recognized and used to provide density proxies. However, for Parker Solar Probe, this correlation has not produced quantitative density estimates over extended periods of time because it depends on the energy dependent exponential variation of the photoemission spectrum, the electron temperature, the ratio of the biased surface area to the conducting spacecraft surface area, the spacecraft secondary or thermal emission, the spacecraft distance from the Sun, etc. In this paper the density as a function of time and frequency to frequencies as high as the electron gyrofrequency is determined through least squares fits of a function of the spacecraft potential to the plasma density measured on the Parker Solar Probe. This function allows correction for the many effects on the spacecraft potential other than that due to the plasma density. Some examples of plasma density obtained from this procedure are presented.',\n",
       "  'len': 1111},\n",
       " {'abstract': 'This paper explores expressing the relative state in the close-proximity satellite relative motion problem in terms of fundamental solution constants. The nominal uncontrolled relative state can be expressed in terms of a weighted sum of fundamental and geometrically insightful motions. These fundamental motions are obtained using Lyapunov-Floquet theory. In the case that the dynamics are perturbed by the action of a controller or by unmodeled dynamics, the weights on each fundamental solution are allowed to vary as in a variation-of-parameters approach, and in this manner function as state variables. This methodology reveals interesting insights about satellite relative motion and also enables elegant control approaches. This approach can be applied in any dynamical environment as long as the chief orbit is periodic, and this is demonstrated with results for relative motion analysis and control in the eccentric Keplerian problem and in the circular restricted three-body problem (CR3BP). Some commentary on extension of the methodology beyond the periodic chief orbit case is also provided. This is a promising and widely applicable new approach to the close-proximity satellite relative motion problem.',\n",
       "  'len': 1229},\n",
       " {'abstract': \"Context: Despite the observed signs of large impacts on the surface of Ceres, there is no confirmed collisional family associated with this dwarf planet. After a dynamical and photometric study, a sample of 156 asteroids was proposed as candidate members of a Ceres collisional family. Aims: Our main objective is to study the connection between Ceres and a total of 14 observed asteroids among the candidate's sample to explore their genetic relationships with Ceres. Methods: We obtained visible spectra of these 14 asteroids using the OSIRIS spectrograph at the 10.4 m Gran Telescopio Canarias(GTC). We computed spectral slopes in two different wavelength ranges, from 0.49 to 0.80{\\\\mu}m and from 0.80 to 0.92{\\\\mu}m, to compare the values obtained with those on Ceres' surface previously computed using the Visible and Infrared Spectrometer (VIR) instrument onboard the NASA Dawn spacecraft. We also calculated the spectral slopes in the same range for ground-based observations of Cerescollected from the literature. Results: We present the visible spectra and the taxonomy of 14 observed asteroids. We found that only two of the asteroids are spectrally compatible with Ceres' surface. Further analysis of those two asteroids indicates that they are spectrally young and thus less likely to be members of the Ceres family. Conclusions: All in all, our results indicate that most of the 14 observed asteroids are not likely to belong to a Ceres collisional family. Despite two of them being spectrally compatible with the young surface of Ceres, further evaluation is needed to confirm or reject their origin from Ceres.\",\n",
       "  'len': 1635},\n",
       " {'abstract': 'Solar wind turbulence is anisotropic with respect to the mean magnetic field. Anisotropy leads to ambiguity when interpreting in-situ turbulence observations in the solar wind because an apparent change in the measurements could be due either to the change of intrinsic turbulence properties or to a simple change of the spacecraft sampling direction. We demonstrate the ambiguity using the spectral index and magnetic compressibility in the inertial range observed by the Parker Solar Probe during its first seven orbits ranging from 0.1 to 0.6 AU. To unravel the effects of the sampling direction, we assess whether the wavevector anisotropy is consistent with a two-dimensional (2D) plus slab turbulence transport model and determine the fraction of power in the 2D versus slab component. Our results confirm that the 2D plus slab model is consistent with the data and the power ratio between 2D and slab components depends on radial distance, with the relative power in 2D fluctuations becoming smaller closer to the Sun.',\n",
       "  'len': 1036},\n",
       " {'abstract': 'Scheduled to launch in late 2021, the Imaging X-ray Polarimetry Explorer (IXPE) is a NASA Small Explorer Mission in collaboration with the Italian Space Agency (ASI). The mission will open a new window of investigation - imaging X-ray polarimetry. The observatory features 3 identical telescopes each consisting of a mirror module assembly with a polarization-sensitive imaging X-ray detector at the focus. A coilable boom, deployed on orbit, provides the necessary 4-m focal length. The observatory utilizes a 3-axis-stabilized spacecraft which provides services such as power, attitude determination and control, commanding, and telemetry to the ground. During its 2-year baseline mission, IXPE will conduct precise polarimetry for samples of multiple categories of X-ray sources, with follow-on observations of selected targets.',\n",
       "  'len': 842},\n",
       " {'abstract': \"The Laser Interferometer Space Antenna (LISA) is a future space-based gravitational wave (GW) detector designed to be sensitive to sources radiating in the low frequency regime (0.1 mHz to 1 Hz). LISA's interferometer signals will be dominated by laser frequency noise which has to be suppressed by about 7 orders of magnitude using an algorithm called Time-Delay Interferometry (TDI). Arm locking has been proposed to reduce the laser frequency noise by a few orders of magnitude to reduce the potential risks associated with TDI. In this paper, we present an updated performance model for arm locking for the new LISA mission using 2.5 Gm arm lengths, the currently assumed clock noise, spacecraft motion, and shot noise. We also update the Doppler frequency pulling estimates during lock acquisition.\",\n",
       "  'len': 814},\n",
       " {'abstract': \"The Double Asteroid Redirection Test (DART) mission will be the first test of a kinetic impactor as a means of planetary defense. In late 2022, DART will collide with Dimorphos, the secondary in the Didymos binary asteroid system. The impact will cause a momentum transfer from the spacecraft to the binary asteroid, changing the orbit period of Dimorphos and forcing it to librate in its orbit. Owing to the coupled dynamics in binary asteroid systems, the orbit and libration state of Dimorphos are intertwined. Thus, as the secondary librates, it also experiences fluctuations in its orbit period. These variations in the orbit period are dependent on the magnitude of the impact perturbation, as well as the system's state at impact and the moments of inertia of the secondary. In general, any binary asteroid system whose secondary is librating will have a non-constant orbit period on account of the secondary's fluctuating spin rate. The orbit period variations are typically driven by two modes: a long-period and short-period, each with significant amplitudes on the order of tens of seconds to several minutes. The fluctuating orbit period offers both a challenge and an opportunity in the context of the DART mission. Orbit period oscillations will make determining the post-impact orbit period more difficult, but can also provide information about the system's libration state and the DART impact.\",\n",
       "  'len': 1421},\n",
       " {'abstract': 'The impact of interplanetary shocks on the magnetosphere can trigger magnetic substorms that intensify auroral electrojet currents. These currents enhance ground magnetic field perturbations (d$B$/d$t$), which in turn generate geomagnetically induced currents (GICs) that can be detrimental to power transmission infrastructure. We perform a comparative study of d$B$/d$t$ variations in response to two similarly strong shocks, but with one being nearly frontal, and the other, highly inclined. Multi-instrument analyses by the Time History of Events and Macroscale Interactions during Substorms (THEMIS) and Los Alamos National Laboratory spacecraft show that nightside substorm-time energetic particle injections are more intense and occur faster in the case of the nearly head-on impact. The same trend is observed in d$B$/d$t$ variations recorded by THEMIS ground magnetometers. THEMIS all-sky imager data show a fast and clear poleward auroral expansion in the first case, which does not clearly occur in the second case. Strong field-aligned currents computed with the spherical elementary current system (SECS) technique occur in both cases, but the current variations resulting from the inclined shock impact are weaker and slower compared to the nearly frontal case. SECS analyses also reveal that geographic areas with d$B$/d$t$ surpassing the thresholds 1.5 and 5 nT/s, usually linked to high-risk GICs, are larger and occur earlier due to the symmetric compression caused by the nearly head-on impact. These results, with profound space weather implications, suggest that shock impact angles affect the geospace driving conditions and the location and intensity of the subsequent d$B$/d$t$ variations during substorm activity.',\n",
       "  'len': 1749},\n",
       " {'abstract': 'This research provides an analysis of extreme events in the solar wind and in the magnetosphere due to disturbances of the solar wind. Extreme value theory has been applied to a 20 year data set from the Advanced Composition Explorer (ACE) spacecraft for the period 1998-2017. The solar proton speed, solar proton temperature, solar proton density and magnetic field have been analyzed to characterize extreme events in the solar wind. The solar wind electric field, vB$_{z}$ has been analyzed to characterize the impact from extreme disturbances in the solar wind to the magnetosphere. These extreme values were estimated for one-in-40 and one-in-80 years events, which represent two and four times the range of the original data set. The estimated values were verified by comparison with measured values of extreme events recorded in previous years. Finally, our research also suggests the presence of an upper boundary in the magnitudes under study.',\n",
       "  'len': 963},\n",
       " {'abstract': \"Water vapor geysers on Europa have been inferred from observations made by the Galileo spacecraft, the Hubble Space Telescope, and the Keck Observatory. Unlike the water-rich geysers observed on Enceladus, Europa's geysers appear to be an intermittent phenomenon, and the dynamical mechanism permitting water to sporadically erupt through a kilometers-thick ice sheet is not well understood. Here we outline and explore the hypothesis that the Europan geysers are driven by CO$_2$ gas released by dissociation and depressurization of CO$_2$ clathrate hydrates initially sourced from the subsurface ocean. We show that CO$_2$ hydrates can become buoyant to the upper ice-water interface under plausible oceanic conditions, namely, if the temperature or salinity conditions of a density-stratified two-layer water column evolve to permit the onset of convection that generates a single mixed layer. To quantitatively describe the eruptions once the CO$_2$ has been released from the hydrate state, we extend a one-dimensional hydrodynamical model that draws from the literature on volcanic magma explosions on Earth. Our results indicate that for a sufficiently high concentration of exsolved CO$_2$, these eruptions develop vertical velocities of $\\\\sim$700 m s$^{-1}$. These high velocities permit the ejecta to reach heights of $\\\\sim$200 km above the Europan surface, thereby explaining the intermittent presence of water vapor at these high altitudes. Molecules ejected via this process will persist in the Europan atmosphere for a duration of about 10 minutes, limiting the timescale over which geyser activity above the Europan surface may be observable. Our proposed mechanism requires Europa's ice shell thickness to be d$\\\\lesssim$ 10 km.\",\n",
       "  'len': 1754},\n",
       " {'abstract': 'We develop multiple Deep Learning (DL) models that advance the state-of-the-art predictions of the global auroral particle precipitation. We use observations from low Earth orbiting spacecraft of the electron energy flux to develop a model that improves global nowcasts (predictions at the time of observation) of the accelerated particles. Multiple Machine Learning (ML) modeling approaches are compared, including a novel multi-task model, models with tail- and distribution-based loss functions, and a spatio-temporally sparse 2D-convolutional model. We detail the data preparation process as well as the model development that will be illustrative for many similar time series global regression problems in space weather and across domains. Our ML improvements are three-fold: 1) loss function engineering; 2) multi-task learning; and 3) transforming the task from time series prediction to spatio-temporal prediction. Notably, the ML models improve prediction of the extreme events, historically obstinate to accurate specification and indicate that increased expressive capacity provided by ML innovation can address grand challenges in the science of space weather.',\n",
       "  'len': 1183},\n",
       " {'abstract': 'Aims: Our goal is to propagate multiple eruptions - obtained through numerical simulations performed in a previous study - to 1 AU and to analyse the effects of different background solar winds on their dynamics and structure at Earth. We also aim to improve the understanding of why some consecutive eruptions do not result in the expected geoeffectiveness, and how a secondary coronal mass ejection (CME) can affect the configuration of the preceding one. Methods: Using the 2.5D magnetohydrodynamics (MHD) package of the code MPI-AMRVAC, we numerically modeled consecutive CMEs inserted in two different solar winds by imposing shearing motions onto the inner boundary. The initial magnetic configuration depicts a triple arcade structure shifted southward, and embedded into a bimodal solar wind. We compared our simulated signatures with those of a multiple CME event in Sept 2009 using data from spacecraft around Mercury and Earth. We computed and analysed the Dst index for all the simulations performed. Results: The observed event fits well at 1 AU with two of our simulations, one with a stealth CME and the other without. This highlights the difficulty of attempting to use in situ observations to distinguish whether or not the second eruption was stealthy, because of the processes the flux ropes undergo during their propagation in the interplanetary space. We simulate the CMEs propagated in two different solar winds, one slow and another faster one. Only in the first case, plasma blobs arise in the trail of eruptions. Interestingly, the Dst computation results in a reduced geoeffectiveness in the case of consecutive CMEs when the flux ropes arrive with a leading positive Bz. When the Bz component is reversed, the geoeffectiveness increases, meaning that the magnetic reconnections with the trailing blobs and eruptions strongly affect the impact of the arriving interplanetary CME.',\n",
       "  'len': 1916},\n",
       " {'abstract': 'Whistler mode wave is a fundamental perturbation of electromagnetic fields and plasmas in various environments including planetary space, laboratory and astrophysics. The origin and evolution of the waves are a long-standing question due to the limited instrumental capability in resolving highly variable plasma and electromagnetic fields. Here, we analyse data with the high time resolution from the multi-scale magnetospheric spacecraft in the weak magnetic environment (i.e., foreshock) enabling a relatively long gyro-period of whistler mode wave. Moreover, we develop a novel approach to separate the three-dimensional fluctuating electron velocity distributions from their background, and have successfully captured the coherent resonance between electrons and electromagnetic fields at high frequency, providing the resultant growth rate of unstable whistler waves. Regarding the energy origin for the waves, the ion distributions are found to also play crucial roles in determining the eigenmode disturbances of fields and electrons. The quantification of wave growth rate can significantly advance the understandings of the wave evolution and the energy conversion with particles.',\n",
       "  'len': 1201},\n",
       " {'abstract': 'IXPE scientific payload comprises of three telescopes, each composed of a mirror and a photoelectric polarimeter based on the Gas Pixel Detector design. The three focal plane detectors, together with the unit which interfaces them to the spacecraft, are named IXPE Instrument and they will be built and calibrated in Italy; in this proceeding, we will present how IXPE Instrument will be calibrated, both on-ground and in-flight. The Instrument Calibration Equipment is being finalized at INAF-IAPS in Rome (Italy) to produce both polarized and unpolarized radiation, with a precise knowledge of direction, position, energy and polarization state of the incident beam. In flight, a set of four calibration sources based on radioactive material and mounted on a filter and calibration wheel will allow for the periodic calibration of all of the three IXPE focal plane detectors independently. A highly polarized source and an unpolarized one will be used to monitor the response to polarization; the remaining two will be used to calibrate the gain through the entire lifetime of the mission.',\n",
       "  'len': 1102},\n",
       " {'abstract': 'Europa has been spotted to have water outgassing activities by the space and ground-based telescopes as well as reanalysis of the Galileo data (Roth et al. 2014; Sparks et al. 2016, 2017; Paganini et al. 2020; Jia et al. 2018; Arnold et al. 2019). However, these observations only provided limited information about plume dynamics, which is critical in understanding the eruption mechanism and preparation of future exploration. We adopt a 3D DSMC model to investigate the plume characteristics of Europa assuming supersonic expansion originated from the undersurface vent. The main goal is to understand the physical processes and structures of Europa water vapor plumes, which can play a key role on probing its undersurface vent condition and outgassing mechanism. With a parametric study of the total gas production rate and initial gas bulk velocity, the gas number density, temperature and velocity information of the outgassing plumes from the various case studies are derived. Our results show that the plume gases experience acceleration through mutual collisions and adiabatic cooling when exiting and expanding from the surface. The central part of the plume with the relatively large gas production rates (of 1029 and 1030 H2O s-1) is found to sustain thermal equilibrium and nearly continuum condition. Column density maps integrated along two different viewing angles are presented to demonstrate the importance of the projection effect on remote sensing diagnostics. Finally, the density profiles at different altitudes are provided to prepare for observations of Europa plumes including the upcoming spacecraft missions such as JUICE and Europa Clipper.',\n",
       "  'len': 1680},\n",
       " {'abstract': 'We present the first statistical analysis of complexity changes affecting the magnetic structure of interplanetary coronal mass ejections (ICMEs), with the aim of answering the questions: How frequently do ICMEs undergo magnetic complexity changes during propagation? What are the causes of such changes? Do the in situ properties of ICMEs differ depending on whether they exhibit complexity changes? We consider multi-spacecraft observations of 31 ICMEs by MESSENGER, Venus Express, ACE, and STEREO between 2008 and 2014 while radially aligned. By analyzing their magnetic properties at the inner and outer spacecraft, we identify complexity changes which manifest as fundamental alterations or significant re-orientations of the ICME. Plasma and suprathermal electron data at 1 au, and simulations of the solar wind enable us to reconstruct the propagation scenario for each event, and to identify critical factors controlling their evolution. Results show that ~65% of ICMEs change their complexity between Mercury and 1 au and that interaction with multiple large-scale solar wind structures is the driver of these changes. Furthermore, 71% of ICMEs observed at large radial (>0.4 au) but small longitudinal (<15 degrees) separations exhibit complexity changes, indicating that propagation over large distances strongly affects ICMEs. Results also suggest ICMEs may be magnetically coherent over angular scales of at least 15 degrees, supporting earlier theoretical and observational estimates. This work presents statistical evidence that magnetic complexity changes are consequences of ICME interactions with large-scale solar wind structures, rather than intrinsic to ICME evolution, and that such changes are only partly identifiable from in situ measurements at 1 au.',\n",
       "  'len': 1787},\n",
       " {'abstract': 'Risk to human astronauts and interplanetary distance causing slow and limited communication drives scientists to pursue an autonomous approach to exploring distant planets, such as Mars. A portion of exploration of Mars has been conducted through the autonomous collection and analysis of Martian data by spacecraft such as the Mars rovers and the Mars Express Orbiter. The autonomy used on these Mars exploration spacecraft and on Earth to analyze data collected by these vehicles mainly consist of machine learning, a field of artificial intelligence where algorithms collect data and self-improve with the data. Additional applications of machine learning techniques for Mars exploration have potential to resolve communication limitations and human risks of interplanetary exploration. In addition, analyzing Mars data with machine learning has the potential to provide a greater understanding of Mars in numerous domains such as its climate, atmosphere, and potential future habitation. To explore further utilizations of machine learning techniques for Mars exploration, this paper will first summarize the general features and phenomena of Mars to provide a general overview of the planet, elaborate upon uncertainties of Mars that would be beneficial to explore and understand, summarize every current or previous usage of machine learning techniques in the exploration of Mars, explore implementations of machine learning that will be utilized in future Mars exploration missions, and explore machine learning techniques used in Earthly domains to provide solutions to the previously described uncertainties of Mars.',\n",
       "  'len': 1636},\n",
       " {'abstract': 'We use magnetic field measurements by the Juno spacecraft to catalog and investigate interplanetary coronal mass ejections (ICMEs) beyond 1 AU. During its cruise phase, Juno spent about 5 years in the solar wind between 2011 September and 2016 June, providing measurements of the interplanetary magnetic field (IMF) between 1 and 5.4 AU. Juno therefore presents the most recent opportunity for a statistical analysis of ICME properties beyond 1 AU since the Ulysses mission (1990-2009). Our catalog includes 80 such ICME events, 32 of which contain associated flux-rope-like structures. We find that the dependency of the mean magnetic field strength of the magnetic flux ropes decreases with heliocentric distance as $r^{-1.24 \\\\pm 0.43}$ between 1 and 5.4 AU, in good agreement with previous relationships calculated using ICME catalogs at Ulysses. We combine the Juno catalog with the HELCATS catalog to create a dataset of ICMEs covering 0.3-5.4 AU. Using a linear regression model to fit the combined dataset on a double-logarithmic plot, we find that there is a clear difference between global expansion rates for ICMEs observed at lesser heliocentric distances and those observed farther out beyond 1 AU. The cataloged ICMEs at Juno present a good basis for future multispacecraft studies of ICME evolution between the inner heliosphere, 1 AU, and beyond.',\n",
       "  'len': 1372},\n",
       " {'abstract': 'Onboard autonomy technologies such as planning and scheduling, identification of scientific targets, and content-based data summarization, will lead to exciting new space science missions. However, the challenge of operating missions with such onboard autonomous capabilities has not been studied to a level of detail sufficient for consideration in mission concepts. These autonomy capabilities will require changes to current operations processes, practices, and tools. We have developed a case study to assess the changes needed to enable operators and scientists to operate an autonomous spacecraft by facilitating a common model between the ground personnel and the onboard algorithms. We assess the new operations tools and workflows necessary to enable operators and scientists to convey their desired intent to the spacecraft, and to be able to reconstruct and explain the decisions made onboard and the state of the spacecraft. Mock-ups of these tools were used in a user study to understand the effectiveness of the processes and tools in enabling a shared framework of understanding, and in the ability of the operators and scientists to effectively achieve mission science objectives.',\n",
       "  'len': 1207},\n",
       " {'abstract': 'Here, we report the detection of phosphorus and fluorine in solid particles collected from the inner coma of comet 67P/Churyumov-Gerasimenko measured with the COmetary Secondary Ion Mass Analyser (COSIMA) instrument on-board the Rosetta spacecraft, only a few kilometers away from the comet nucleus. We have detected phosphorus-containing minerals from the presented COSIMA mass spectra, and can rule out e.g. apatite minerals as the source of phosphorus. This result completes the detection of life-necessary CHNOPS-elements in solid cometary matter, indicating cometary delivery as a potential source of these elements to the young Earth. Fluorine was also detected with CF$^+$ secondary ions originating from the cometary dust.',\n",
       "  'len': 741},\n",
       " {'abstract': 'Aims: Our goal is to develop methodologies to seamlessly track transient solar wind flows viewed by coronagraphs or heliospheric imagers from rapidly varying viewpoints. Methods: We constructed maps of intensity versus time and elongation (J-maps) from Parker Solar Probe (PSP) Wide-field Imager (WISPR) observations during the fourth encounter of PSP. From the J-map, we built an intensity on impact-radius-on-Thomson-surface map (R-map). Finally, we constructed a latitudinal intensity versus time map (Lat-map). Our methodology satisfactorily addresses the challenges associated with the construction of such maps from data taken from rapidly varying viewpoint observations. Results: Our WISPR J-map exhibits several tracks, corresponding to transient solar wind flows ranging from a coronal mass ejection (CME) down to streamer blobs. The latter occurrence rate is about 4-5 per day, which is similar to the occurrence rate in a J-map made from $\\\\sim1$ AU data obtained with the Heliospheric Imager-1 (HI-1) on board the Solar Terrestrial Relations Observatory Ahead spacecraft (STEREO-A). STEREO-A was radially aligned with PSP during the study period. The WISPR J-map tracks correspond to angular speeds of $2.28 \\\\pm 0.7$$^{\\\\circ}$/hour ($2.49 \\\\pm 0.95$$^{\\\\circ}$/hour), for linear (quadratic) time-elongation fittings, and radial speeds of about 150-300 km s$^{-1}$. The analysis of the Lat-map reveals a bifurcating streamer, which implies that PSP was flying through a slightly folded streamer during perihelion. Conclusions: We developed a framework to systematically capture and characterize transient solar wind flows from space platforms with rapidly varying vantage points. The methodology can be applied to PSP WISPR observations as well as to upcoming observations from instruments on board the Solar Orbiter mission.',\n",
       "  'len': 1844},\n",
       " {'abstract': 'We investigate compressive turbulence at sub-ion scales with measurements from the Magnetospheric MultiScale Mission. The tetrahedral configuration and high time resolution density data obtained by calibrating spacecraft potential allow an investigation of the turbulent density fluctuations in the solar wind and their three-dimensional structure in the sub-ion range. The wave-vector associated with the highest energy density at each spacecraft frequency is obtained by application of the Multi-point signal resonator technique to the four-point density data. The fluctuations show a strong wave-vector anisotropy $k_{\\\\perp}\\\\gg k_{\\\\parallel}$ where the parallel and perpendicular symbols are with respect to the mean magnetic field direction. The plasma frame frequencies show two populations, one below the proton cyclotron frequency $\\\\omega<\\\\Omega_{ci}$ consistent with kinetic Alfvén wave (KAW) turbulence. The second component has higher frequencies $\\\\omega > \\\\Omega_{ci}$ consistent with ion Bernstein wave (IBW) turbulence. Alternatively, these fluctuations may constitute KAWs that have undergone multiple wave-wave interactions causing a broadening in the plasma frame frequencies. The scale-dependent kurtosis in this wave-vector region shows a reduction in intermittency at the small scales which can also be explained by the presence of wave activity. Our results suggest that small-scale turbulence exhibits linear-wave properties of kinetic Alfvén and possibly ion-Bernstein/magnetosonic waves. Based on our results, we speculate that these waves may play a role in describing the observed reduction in intermittency at sub ion scales.',\n",
       "  'len': 1662},\n",
       " {'abstract': \"The Educational Irish Research Satellite, EIRSAT-1, is a project developed by students at University College Dublin that aims to design, build, and launch Ireland's first satellite. EIRSAT-1 is a 2U CubeSat incorporating three novel payloads; GMOD, a gamma-ray detector, EMOD, a thermal coating management experiment, and WBC, a novel attitude control algorithm. The EIRSAT-1 project is carried out with the support of the Education Office of the European Space Agency, under the educational Fly your Satellite! programme. The Assembly, Integration and Verification plan for EIRSAT-1 is central to the philosophy and the development of the spacecraft. The model philosophy employed for the project is known as the 'prototype' approach in which two models of the spacecraft are assembled; an Engineering Qualification Model (EQM) and a Flight Model (FM). The payloads, GMOD and EMOD, and the Antenna Deployment Module (ADM) platform element warrant a Development Model in addition to an EQM and a FM, as they have been designed and developed in-house. After successful completion of the Critical Design Review and Ambient Test Readiness Review phases of the project, the EQM of EIRSAT-1 will be assembled and integrated. After assembly and integration of the EQM, the project will begin the ambient test campaign, in which the EQM undergoes ambient functional and mission testing. This work details the preparation and execution of the assembly, integration, and verification activities of EIRSAT-1 EQM.\",\n",
       "  'len': 1513},\n",
       " {'abstract': 'One of the main discoveries from the first two orbits of Parker Solar Probe (PSP) was the presence of magnetic switchbacks, whose deflections dominated the magnetic field measurements. Determining their shape and size could provide evidence of their origin, which is still unclear. Previous work with a single solar wind stream has indicated that these are long, thin structures although the direction of their major axis could not be determined. We investigate if this long, thin nature extends to other solar wind streams, while determining the direction along which the switchbacks within a stream were aligned. We try to understand how the size and orientation of the switchbacks, along with the flow velocity and spacecraft trajectory, combine to produce the observed structure durations for past and future orbits. We searched for the alignment direction that produced a combination of a spacecraft cutting direction and switchback duration that was most consistent with long, thin structures. The expected form of a long, thin structure was fitted to the results of the best alignment direction, which determined the width and aspect ratio of the switchbacks for that stream. The switchbacks had a mean width of $50,000 \\\\, \\\\rm{km}$, with an aspect ratio of the order of $10$. We find that switchbacks are not aligned along the background flow direction, but instead aligned along the local Parker spiral, perhaps suggesting that they propagate along the magnetic field. Since the observed switchback duration depends on how the spacecraft cuts through the structure, the duration alone cannot be used to determine the size or influence of an individual event. For future PSP orbits, a larger spacecraft transverse component combined with more radially aligned switchbacks will lead to long duration switchbacks becoming less common.',\n",
       "  'len': 1850},\n",
       " {'abstract': 'The Earth Observation planning community has access to tools that can propagate orbits and compute coverage of Earth observing imagers with customizable shapes and orientation, model the expected Earth Reflectance at various bands, epochs and directions, generate simplified instrument performance metrics for imagers and radars, and schedule single and multiple spacecraft payload operations. We are working toward integrating existing tools to design a planner that allows commercial small spacecraft to assess the opportunities for cross-calibration of their sensors against current satellite to be calibrated, specifications of the reference instruments, sensor stability, allowable latency between calibration measurements, differences in viewing and solar geometry between calibration measurements, etc. The planner would output cross-calibration opportunities for every reference target pair as a function of flexible user-defined parameters. We use a preliminary version of this planner to inform the design of a constellation of transfer radiometers that can serve as stable, radiometric references for commercial sensors to cross-calibrate with. We propose such a constellation for either vicarious cross-calibration using pre-selected sites, or top of the atmosphere (TOA) cross-calibration globally. Results from the calibration planner applied to a subset of informed architecture designs show that a 4 sat constellation provides multiple calibration opportunities within half a day planning horizon, for Cubesat sensors deployed into a typical rideshare orbits. While such opportunities are available for cross calibration image pairs within 5 deg of solar or view directions, and with-in an hour (for TOA) and less than a day (vicariously), the planner allows us to identify many more by relaxing user-defined restrictions.',\n",
       "  'len': 1849},\n",
       " {'abstract': 'Small spacecraft now have precise attitude control systems available commercially, allowing them to slew in 3 degrees of freedom, and capture images within short notice. When combined with appropriate software, this agility can significantly increase response rate, revisit time and coverage. In prior work, we have demonstrated an algorithmic framework that combines orbital mechanics, attitude control and scheduling optimization to plan the time-varying, full-body orientation of agile, small spacecraft in a constellation. The proposed schedule optimization would run at the ground station autonomously, and the resultant schedules uplinked to the spacecraft for execution. The algorithm is generalizable over small steerable spacecraft, control capability, sensor specs, imaging requirements, and regions of interest. In this article, we modify the algorithm to run onboard small spacecraft, such that the constellation can make time-sensitive decisions to slew and capture images autonomously, without ground control. We have developed a communication module based on Delay/Disruption Tolerant Networking (DTN) for onboard data management and routing among the satellites, which will work in conjunction with the other modules to optimize the schedule of agile communication and steering. We then apply this preliminary framework on representative constellations to simulate targeted measurements of episodic precipitation events and subsequent urban floods. The command and control efficiency of our agile algorithm is compared to non-agile (11.3x improvement) and non-DTN (21% improvement) constellations.',\n",
       "  'len': 1624},\n",
       " {'abstract': 'Plasma turbulence can be viewed as a magnetic landscape populated by large and small scale coherent structures. In this complex network, large helical magnetic tubes might be separated by small scale magnetic reconnection events (current sheets). However, the identification of these magnetic structures in a continuous stream of data has always been a challenging task. Here we present a method that is able to characterize both the large and small scale structures of the turbulent solar wind, based on the combined use of a filtered magnetic helicity ($H_m$) and the Partial Variance of Increments (PVI). This simple, single-spacecraft technique, has been validated first via direct numerical simulations of plasma turbulence and then applied to data from the Parker Solar Probe (PSP) mission. This novel analysis, combining $H_m$&PVI methods, reveals that a large number of flux tubes populate the solar wind and continuously merge in contact regions where magnetic reconnection and particle acceleration may occur.',\n",
       "  'len': 1030},\n",
       " {'abstract': 'The future space-based gravitational wave observatory LISA will consist of a constellation of three spacecraft in a triangular constellation, connected by laser interferometers with 2.5 million-kilometer arms. Among other challenges, the success of the mission strongly depends on the quality of the cancellation of laser frequency noise, whose power lies eight orders of magnitude above the gravitational signal. The standard technique to perform noise removal is time-delay interferometry (TDI). TDI constructs linear combinations of delayed phasemeter measurements tailored to cancel laser noise terms. Previous work has demonstrated the relationship between TDI and principal component analysis (PCA). We build on this idea to develop an extension of TDI based on a model likelihood that directly depends on the phasemeter measurements. Assuming stationary Gaussian noise, we decompose the measurement covariance using PCA in the frequency domain. We obtain a comprehensive and compact framework that we call PCI for \"principal component interferometry,\" and show that it provides an optimal description of the LISA data analysis problem.',\n",
       "  'len': 1153},\n",
       " {'abstract': \"The New Horizons spacecraft's flyby of Kuiper Belt Object (KBO) (486958) Arrokoth revealed a bilobed shape with highly flattened lobes both aligned to its equatorial plane, and a rotational axis almost aligned to the orbital plane (obliquity ~99 deg). Arrokoth belongs to the Cold Classical Kuiper Belt Object population that occupies dynamically undisturbed orbits around the Sun, and as such, is a primitive object that formed in situ. Therefore, whether its shape is primordial or evolutionary carries important implications for understanding the evolution of both KBOs and potentially their dynamically derived objects, Centaurs and Jupiter Family Comets (JFC). Applying our mass loss driven shape evolution model (MONET), here we suggest that the current shape of Arrokoth could be of evolutionary origin due to volatile outgassing in a timescale of about 1 to 100 Myr, while its spin state would not significantly affected. We further argue that such a process may be ubiquitous in the evolution of the shape of KBOs shortly after their formation. This shape changing process could also be reactivated when KBOs dynamically evolve to become Centaurs and then JFCs and receive dramatically increased solar heating.\",\n",
       "  'len': 1230},\n",
       " {'abstract': 'Twin STEREO spacecraft pre-perihelion photometric and polarimetric observations of the sungrazing Kreutz comet C/2010 E6 (STEREO) in March 2010 at heliocentric distances $3-28 R_{\\\\odot}$ were investigated using a newly-created set of analysis routines. The comet fully disintegrated during its perihelion passage. Prior to that, a broadening and an increase of the intensity peak with decreasing heliocentric distance was accompanied by a drop to zero polarisation at high phase angles (~105-135°, STEREO-B) and the emergence of negative polarisation at low phase angles (~25-35°, STEREO-A). Outside the near-comet region, the tail exhibited a steep slope of increasing polarisation with increasing cometocentric distance, with the slope showing a marked decrease with decreasing heliocentric distance. The steep slope is attributed to sublimation of refractory organic matrix and the accompanied processing of the fluffy aggregate dust grains, exposing silicates. The decrease in slope is likely caused by the gradual sublimation of all refractory material closer to the Sun, with resulting gases suppressing polarisation signal of the dust. Near-zero polarisation closer to the comet head may be explained by the same mechanism, stronger there due to the large amounts of material being ejected and sublimated from the presumably disintegrating core, which correlates with intensity data. Negative polarisation at small phase angles may be explained by the presence of freshly ejected large silicate-rich aggregates. Despite both zero and negative polarisation being observed simultaneously, the two hypotheses on their causes are not easily reconciled. The need for further studies of such comets, both observational and theoretical, is highlighted.',\n",
       "  'len': 1763},\n",
       " {'abstract': \"Key questions surrounding the origin and evolution of Titan and the Saturnian system in which it resides remain following the Cassini-Huygens mission. In-situ measurements performed at key locations on the body are a highly effective way to address these questions, and the aerial-aquatic platform proposed in this report serves to deliver unprecedented access to Titan's northern surface lakes, allowing an understanding of the hydrocarbon cycle, the potential for habitability in the environment and the chemical processes that occur at the surface. The proposed heavier-than-air flight and plunge-diving aquatic landing spacecraft, ASTrAEUS, is supported by the modelling of the conditions which can be expected on Titan's surface lakes using multiphysics fluid-structure interaction (FSI) CFD simulations with a coupled meshfree smoothed-particle hydrodynamics (SPH) and finite element method (FEM) approach in LS-DYNA.\",\n",
       "  'len': 934},\n",
       " {'abstract': \"The very low frequency (VLF) regime below 30 MHz in the electromagnetic spectrum has presently drawing global attentions in radio astronomical research due to its potentially significant science outcomes exploring many unknown extragalactic sources, transients, and so on. However, the non-transparency of the Earth's ionosphere, ionospheric distortion and artificial radio frequency interference (RFI) have made it difficult to detect the VLF celestial radio emission with ground-based instruments. A straightforward solution to overcome these problems is a space based VLF radio telescope, just like the VLF radio instruments onboard the Chang'E 4 spacecraft. But building such a space telescope would be inevitably costly and technically challenging. The alternative approach would be then a ground based VLF radio telescope. Particularly, in the period of post 2020 when the solar and terrestrial ionospheric activities are expected to be in a 'calm' state, it will provide us a good chance to perform VLF ground-based radio observations. Anticipating such an opportunity, we built an agile VLF radio spectrum explorer co-located with the currently operational Mingantu Spectra Radio Heliograph (MUSER). The instrument includes four antennas operating in the VLF frequency range 1-70 MHz. Along with them, we employ an eight-channel analog and digital receivers to amplify, digitize and process the radio signals received by the antennas. We present in the paper this VLF radio spectrum explorer and the instrument will be useful for celestial studies of VLF radio emissions.\",\n",
       "  'len': 1590},\n",
       " {'abstract': 'We systematically search for magnetic flux rope structures in the solar wind to within the closest distance to the Sun of 0.13 AU, using data from the third and fourth orbits of the Parker Solar Probe. We extend our previous magnetic helicity based technique of identifying magnetic flux rope structures. The method is improved upon to incorporate the azimuthal flow, which becomes larger as the spacecraft approaches the Sun. A total of 21 and 34 magnetic flux ropes are identified during the third (21 days period) and fourth (17 days period) orbits of the Parker Solar Probe, respectively. We provide a statistical analysis of the identified structures, including their relation to the streamer belt and heliospheric current sheet crossing.',\n",
       "  'len': 754},\n",
       " {'abstract': 'Measurements made with the Voyager 1 spacecraft indicate that significant levels of compressive fluctuations exist in the inner heliosheath. Some studies have already been performed with respect to the mirror-mode instability in the downstream region close to the solar wind termination shock, and here we extend the investigation to the whole inner heliosheath. We employ quasilinear theory and results from a global magnetohydrodynamic model of the heliosphere to compute the time evolution of both the temperature anisotropy and the energy density of the corresponding magnetic fluctuations, and we demonstrate their likely presence in the inner heliosheath. Furthermore, we compute the associated, locally generated density fluctuations. The results can serve as inputs for future models of the transport of compressible turbulence in the inner heliosheath.',\n",
       "  'len': 872},\n",
       " {'abstract': 'We study the characteristics of Near-Earth-Networks (NENs) of gamma-ray burst (GRB) detectors, with the objective of defining a network with all-sky, full-time localization capability for multi-messenger astrophysics. We show that a minimum network consisting of 9 identical spacecraft in two orbits with different inclinations provides a good combination of sky coverage with several-degree localization accuracy with detector areas of 100 cm$^2$. In order to achieve this, careful attention must be paid to systematics. This includes accurate photon timing ($\\\\sim$ 0.1 ms), good energy resolution ($\\\\sim$ 10\\\\%), and reduction of Earth albedo, which are all within current capabilities. Such a network can be scaled in both the number and size of detectors to produce increased accuracy. We introduce a new method of localization which does not rely on on-board trigger systems or on the cross-correlation of time histories, but rather, in ground processing, tests positions over the entire sky and assigns probabilities to them to detect and localize events. We demonstrate its capabilities with simulations. If the NEN spacecraft can downlink at least several hundred time- and energy-tagged events per second, and the data can be ground-processed as they are received, it can in principle derive GRB positions in near-real time over the entire sky.',\n",
       "  'len': 1363},\n",
       " {'abstract': 'We study the surface of Ceres at visible wavelengths, as observed by the Visible and InfraRed mapping spectrometer (VIR) onboard the Dawn spacecraft, and analyze the variations of various spectral parameters across the whole surface. We also focus on several noteworthy areas of the surface of this dwarf planet. We made use of the newly corrected VIR visible data to build global maps of a calibrated radiance factor at 550 nm, with two color composites and three spectral slopes between 400 and 950 nm. We have made these maps available for the community via the Aladin Desktop software. Ceres surface shows diverse spectral behaviors in the visible range. The color composite and the spectral slope between 480 and 800 nm highlight fresh impact craters and young geologic formations of endogenous origin, which appear bluer than the rest of the surface. The steep slope before 465 nm displays very distinct variations and may be a proxy for the absorptions caused by the $O_2^{-}$ -> $Fe^{3+}$ or the $2Fe^{3+}$ -> $Fe^{2+} + Fe^{4+}$ charge transfer, if the latter are found to be responsible for the drop in this spectral range. We notice several similarities between the spectral slopes and the abundance of phyllosilicates detected in the infrared by the VIR, whereas no correlation can be clearly established with carbonate species. The region of the Dantu impact crater presents a peculiar spectral behavior (especially through the color and the spectral slope before 465 nm) suggesting a change in composition or in the surface physical properties that is not observed elsewhere on Ceres.',\n",
       "  'len': 1609},\n",
       " {'abstract': 'The Kepler spacecraft observed the hot subdwarf star PHL 417 during its extended K2 mission, and the high-precision photometric lightcurve reveals the presence of 17 pulsation modes with periods between 38 and 105 minutes. From follow-up ground-based spectroscopy we find that the object has a relatively high temperature of 35 600 K, a surface gravity of $\\\\log g / {\\\\rm cm\\\\,s^{-2}}\\\\,=\\\\,5.75$ and a super-solar helium abundance. Remarkably, it also shows strong zirconium lines corresponding to an apparent +3.9 dex overabundance compared with the Sun. These properties clearly identify this object as the third member of the rare group of pulsating heavy-metal stars, the V366 Aquarii pulsators. These stars are intriguing in that the pulsations are inconsistent with the standard models for pulsations in hot subdwarfs, which predicts that they should display short-period pulsations rather than the observed longer periods. We perform a stability analysis of the pulsation modes based on data from two campaigns with K2. The highest amplitude mode is found to be stable with a period drift, $\\\\dot{P}$, of less than $1.1\\\\cdot10^{-9}$ s/s. This result rules out pulsations driven during the rapid stages of helium flash ignition.',\n",
       "  'len': 1241},\n",
       " {'abstract': 'In this paper we present a statistical study of the kinematics of 28894 coronal mass ejections (CMEs) recorded by the Large Angle and Spectrometric Coronagraph (LASCO) on board the Solar and Heliospheric Observatory spacecraft from 1996 until mid-2017. The initial acceleration phase is characterized by a rapid increase in CME velocity just after eruption in the inner corona. This phase is followed by a non-significant residual acceleration (deceleration) characterized by an almost constant speed of CMEs. We demonstrate that the initial acceleration is in the range 0.24-2616 ms-2 with median (average) value of 57 ms-2 (34 ms-2 ) and it takes place up to a distance of about 28 solar radius with median (average) value of 7.8 solar radius (6 solar radius). Additionally, the initial acceleration is significant in the case of fast CMEs (V > 900 kms-1 ), where the median (average) values are about 295 ms-2 (251 ms-2 ), respectively, and much weaker in the case of slow CMEs (V < 250 kms-1 ), where the median (average) values are about 18 ms-2 (17 ms-2 ), respectively. We note that the significant driving force (Lorentz force) can operate up to a distance of 6 solar radius from the Sun during the first 2 hours of propagation. We found a significant anti-correlation between the initial acceleration magnitude and the acceleration duration, whereas the residual acceleration covers a range from -1224 to 0 ms-2 with a median (average) value of -34 ms-2 (-17 ms-2 ). One intriguing finding is that the residual acceleration is much smaller during the 24th cycle in comparison to the 23rd cycle of solar activity. Our study has also revealed that the considered parameters, initial acceleration (ACC INI ), residual acceleration (ACC RES ), maximum velocity (V MAX ), and time at maximum velocity (Time MAX ) mostly follow solar cycles and the intensities of the individual cycle.',\n",
       "  'len': 1899},\n",
       " {'abstract': \"The Deep Space Network (DSN) enables NASA to communicate with its spacecraft in deep space. By virtue of its large antennas, the DSN can also be used as a powerful instrument for radio astronomy. Specifically, Deep Space Station (DSS) 43, the 70 m antenna at the Canberra Deep Space Communications Complex (CDSCC) has a K-band radio astronomy system covering a 10 GHz bandwidth at 17 GHz to 27 GHz. This spectral range covers a number of atomic and molecular lines, produced in a rich variety of interstellar gas conditions. Lines include hydrogen radio recombination lines (RRLs), cyclopropenylidene, water masers, and ammonia. A new high-resolution spectrometer was deployed at CDSCC in November 2019 and connected to the K-band downconverter. The spectrometer has a total bandwidth of 16 GHz. Such a large total bandwidth enables, for example, the simultaneous observations of a large number of RRLs, which can be combined together to significantly improve the sensitivity of these observations. The system has two firmware modes: 1) A 65k-pt FFT to provide 32768 spectral channels at 30.5 kHz and 2) A 16k-pt polyphase filterbank (PFB) to provide 8192 spectral channels with 122 kHz resolution. The observation process is designed to maximize autonomy, from the Principle Investigator's inputs to the output data in FITS file format. We present preliminary mapping observations of hydrogen RRLs in Orion KL mapping taken using the new spectrometer.\",\n",
       "  'len': 1463},\n",
       " {'abstract': 'We propose a method to compute approximate solutions to the minimum-fuel far-field rendezvous problem for thrust-vectoring spacecraft. It is well-known that the use of linearized spacecraft rendezvous equations may not give sufficiently accurate results for far-field rendezvous. In particular, as the distance between the active and the target spacecraft becomes significantly greater than the distance between the target spacecraft and the center of gravity of the planet, the accuracy of linearization-based control design approaches may decline substantially. In this paper, we use a nonlinear state space model which corresponds to more accurate description of dynamics than linearized models but at the same time poses the known challenges of nonlinear control design. To overcome these challenges, we utilize a Koopman operator based approach with which the nonlinear spacecraft rendezvous dynamics is lifted into a higher dimensional space over which the nonlinear dynamics can be approximated by a linear system which is more suitable for control design purposes than the original nonlinear model. An Iteratively Recursive Least Squares (IRLS) algorithm from compressive sensing is then used to solve the minimum fuel control problem based on the lifted linear system. Numerical simulations are performed to show the efficacy of the proposed Koopman operator based approach.',\n",
       "  'len': 1394},\n",
       " {'abstract': \"A highly reflective sail provides a way to propel a spacecraft out of the solar system using solar radiation pressure. The closer the spacecraft is to the Sun when it starts its outward journey, the larger the radiation pressure and so the larger the final velocity. For a spacecraft starting on the Earth's orbit, closer proximity can be achieved via a retrograde impulse from a rocket engine. The sail is then deployed at the closest approach to the Sun. Employing the so-called Oberth effect, a second, prograde, impulse at closest approach will raise the final velocity further. Here I investigate how a fixed total impulse ({\\\\Delta}v) can best be distributed in this procedure to maximize the sail's velocity at infinity. Once {\\\\Delta}v exceeds a threshold that depends on the lightness number of the sail (a measure of its sun-induced acceleration), the best strategy is to use all of the {\\\\Delta}v in the retrograde impulse to dive as close as possible to the Sun. Below the threshold the best strategy is to use all of the {\\\\Delta}v in the prograde impulse and thus not to dive at all. Although larger velocities can be achieved with multi-stage impulsive transfers, this study shows some interesting and perhaps counter-intuitive consequences of combining impulses with solar sails.\",\n",
       "  'len': 1302},\n",
       " {'abstract': 'The Direct Fusion Drive (DFD) is a nuclear fusion engine that will provide thrust and electrical power for any spacecraft. It is a compact engine, based on the D -$^{3}$He aneutronic fusion reaction that uses the Princeton field reversed configuration for the plasma confinement and an odd parity rotating magnetic field as heating method to achieve nuclear fusion (Cohen et al., 2019), which will heat the deuterium, also used as propellant. \\\\par In this work we present possibilities to explore the solar system outer border using the DFD. The objective is to reach some trans-Neptunian object, such as the dwarf planets Makemake, Eris and Haumea in less than 10 years with a payload mass of at least of 1500 kg, so that it would enable all kind of missions, from scientific observation to in-situ operations. For each mission a thrust-coast-thrust profile is considered. For this reason, each mission is divided into 3 phases: i. the spiral trajectory to escape Earth gravity; ii. the interplanetary travel, from the exit of Earth sphere of influence to the end of the coasting phase; iii. maneuvers to rendezvous with the dwarf planet. Propellant mass consumption, initial and final masses, velocities and $\\\\Delta V$ for each maneuver are presented. Calculations to reach a vicinity at 125 AU for the study of Sun magnetosphere as well as Eris via flyby are also presented, with interest on the influence of different acceleration phases. Our calculations show that a spacecraft propelled by DFD will open unprecedented possibilities to explore the border of the solar system, in a limited amount of time and with a very high payload to propellant masses ratio.',\n",
       "  'len': 1676},\n",
       " {'abstract': \"The main purpose of this work is to perform an analysis of realistic new trajectories for a robotic mission to Saturn's largest moon, Titan, in order to demonstrate the great advantages related to the Direct Fusion Drive (DFD). The DFD is a D -$^3$He fuelled, aneutronic, thermonuclear fusion propulsion system. This fusion propulsion concept is based on a magnetically confined field reversed configuration plasma, where the deuterium propellant is heated by fusion products, and then expanded into a magnetic nozzle, providing both thrust and electrical energy to the spacecraft [1]. The trajectories calculations and analysis for the Titan mission are obtained based on the characteristics provided by the PPPL [1]. Two different profile missions are considered: the first one is a thrust-coast-thrust profile with constant thrust and specific impulse; the second scenario is a continuous and constant thrust profile mission. Each mission study is divided into four different phases, starting from the initial low Earth orbit departure, the interplanetary trajectory, Saturn orbit insertion and the Titan orbit insertion. For all mission phases, maneuver time and propellant consumption are calculated. The results of calculations and mission analysis offer a complete overview of the advantages in term of payload mass and travel time. It is important to emphasize that the deceleration capability is one of the DFD game changer: in fact, the DFD performance allows to rapidly reach high velocities and decelerate in even shorter time period. This capability results in a total trip duration of 2.6 years for the thrust-coast-thrust profile and less than 2 years considering the continuous thrust profile. The high payload enabling capability, combined with the huge electrical power available from the fusion reactor, leads to a tremendous advantage compared to present technology.\",\n",
       "  'len': 1897},\n",
       " {'abstract': 'We propose an abiotic geological mechanism that accounts for the abundance of phosphine detected by Greaves et al., 2020. We hypothesize that trace amounts of phosphides formed in the mantle would be brought to the surface by volcanism, and then subsequently ejected into the atmosphere, where they could react with water or sulfuric acid to form phosphine. To investigate the plausibility of this hypothesis, we carry out an order of magnitude calculation. We suggest that active volcanism today could produce a rate comparable to that required to produce the phosphide-source of the phosphine. Our hypothesis requires that Venus be currently experiencing a high rate of basaltic volcanism, one that is consistent with spacecraft observations and laboratory experiments.',\n",
       "  'len': 782},\n",
       " {'abstract': 'A curious rotation period distribution in the Color-Magnitude-Period Diagram (CMPD) of the Kepler field was recently revealed, thanks to data from Gaia and Kepler spacecraft. It was found that redder and brighter stars are spinning slower than the rest of the main sequence. On the theoretical side, it was demonstrated that metallicity should affect the rotational evolution of stars as well as their evolution in the Hertzprung-Rüssel or Color-Magnitude diagram. In this work we combine this dataset with medium and high resolution spectroscopic metallicities and carefully select main sequence single stars in a given mass range. We show that the structure seen in the CMPD also corresponds to a broad correlation between metallicity and rotation, such that stars with higher metallicity rotate on average more slowly than those with low metallicity. We compare this sample to theoretical rotational evolution models that include a range of different metallicities. They predict a correlation between rotation rate and metallicity that is in the same direction and of about the same magnitude as that observed. Therefore metallicity appears to be a key parameter to explain the observed rotation period distributions. We also discuss a few different ways in which metallicity can affect the observed distribution of rotation period, due to observational biases and age distributions, as well as the effect on stellar wind torques.',\n",
       "  'len': 1444},\n",
       " {'abstract': 'The CHaracterising ExOPlanet Satellite (CHEOPS) was selected in 2012, as the first small mission in the ESA Science Programme and successfully launched in December 2019. CHEOPS is a partnership between ESA and Switzerland with important contributions by ten additional ESA Member States. CHEOPS is the first mission dedicated to search for transits of exoplanets using ultrahigh precision photometry on bright stars already known to host planets. As a follow-up mission, CHEOPS is mainly dedicated to improving, whenever possible, existing radii measurements or provide first accurate measurements for a subset of those planets for which the mass has already been estimated from ground-based spectroscopic surveys and to following phase curves. CHEOPS will provide prime targets for future spectroscopic atmospheric characterisation. Requirements on the photometric precision and stability have been derived for stars with magnitudes ranging from 6 to 12 in the V band. In particular, CHEOPS shall be able to detect Earth-size planets transiting G5 dwarf stars in the magnitude range between 6 and 9 by achieving a photometric precision of 20 ppm in 6 hours of integration. For K stars in the magnitude range between 9 and 12, CHEOPS shall be able to detect transiting Neptune-size planets achieving a photometric precision of 85 ppm in 3 hours of integration. This is achieved by using a single, frame-transfer, back-illuminated CCD detector at the focal plane assembly of a 33.5 cm diameter telescope. The 280 kg spacecraft has a pointing accuracy of about 1 arcsec rms and orbits on a sun-synchronous dusk-dawn orbit at 700 km altitude. The nominal mission lifetime is 3.5 years. During this period, 20% of the observing time is available to the community through a yearly call and a discretionary time programme managed by ESA.',\n",
       "  'len': 1842},\n",
       " {'abstract': 'Uncertain partially observable Markov decision processes (uPOMDPs) allow the probabilistic transition and observation functions of standard POMDPs to belong to a so-called uncertainty set. Such uncertainty, referred to as epistemic uncertainty, captures uncountable sets of probability distributions caused by, for instance, a lack of data available. We develop an algorithm to compute finite-memory policies for uPOMDPs that robustly satisfy specifications against any admissible distribution. In general, computing such policies is theoretically and practically intractable. We provide an efficient solution to this problem in four steps. (1) We state the underlying problem as a nonconvex optimization problem with infinitely many constraints. (2) A dedicated dualization scheme yields a dual problem that is still nonconvex but has finitely many constraints. (3) We linearize this dual problem and (4) solve the resulting finite linear program to obtain locally optimal solutions to the original problem. The resulting problem formulation is exponentially smaller than those resulting from existing methods. We demonstrate the applicability of our algorithm using large instances of an aircraft collision-avoidance scenario and a novel spacecraft motion planning case study.',\n",
       "  'len': 1289},\n",
       " {'abstract': \"Solar Orbiter is the first space mission observing the solar plasma both in situ and remotely, from a close distance, in and out of the ecliptic. The ultimate goal is to understand how the Sun produces and controls the heliosphere, filling the Solar System and driving the planetary environments. With six remote-sensing and four in-situ instrument suites, the coordination and planning of the operations are essential to address the following four top-level science questions: (1) What drives the solar wind and where does the coronal magnetic field originate? (2) How do solar transients drive heliospheric variability? (3) How do solar eruptions produce energetic particle radiation that fills the heliosphere? (4) How does the solar dynamo work and drive connections between the Sun and the heliosphere? Maximising the mission's science return requires considering the characteristics of each orbit, including the relative position of the spacecraft to Earth (affecting downlink rates), trajectory events (such as gravitational assist manoeuvres), and the phase of the solar activity cycle. Furthermore, since each orbit's science telemetry will be downloaded over the course of the following orbit, science operations must be planned at mission level, rather than at the level of individual orbits. It is important to explore the way in which those science questions are translated into an actual plan of observations that fits into the mission, thus ensuring that no opportunities are missed. First, the overarching goals are broken down into specific, answerable questions along with the required observations and the so-called Science Activity Plan (SAP) is developed to achieve this. The SAP groups objectives that require similar observations into Solar Orbiter Observing Plans (SOOPs), resulting in a strategic, top-level view of the optimal opportunities for science observations during the mission lifetime.\",\n",
       "  'len': 1931},\n",
       " {'abstract': 'Cometary meteoroid trails exist in the vicinity of comets, forming fine structure of the interplanetary dust cloud. The trails consist predominantly of cometary particles with sizes of approximately 0.1 mm to 1 cm which are ejected at low speeds and remain very close to the comet orbit for several revolutions around the Sun. When re-analysing the Helios dust data measured in the 1970s, Altobelli et al. (2006) recognized a clustering of seven impacts, detected in a very narrow region of space at a true anomaly angle of 135 deg, which the authors considered as potential cometary trail particles. We re-analyse these candidate cometary trail particles to investigate the possibility that some or all of them indeed originate from cometary trails and we constrain their source comets. The Interplanetary Meteoroid Environment for eXploration (IMEX) dust streams in space model is a new universal model for cometary meteoroid streams in the inner solar system, developed by Soja et al. (2015). Using IMEX we study cometary trail traverses by Helios. During ten revolutions around the Sun, and in the narrow region of space where Helios detected the candidate dust particles, the spacecraft repeatedly traversed the trails of comets 45P/Honda-Mrkos-Pajduvsakova and 72P/Denning-Fujikawa. Based on the detection times and particle impact directions, four detected particles are compatible with an origin from these two comets. We find a dust spatial density in these trails of about 10^-8 to 10^-7 m^-3. The in-situ detection and analysis of meteoroid trail particles which can be traced back to their source bodies by spacecraft-based dust analysers opens a new window to remote compositional analysis of comets and asteroids without the necessity to fly a spacecraft to or even land on those celestial bodies. This provides new science opportunities for future missions like Destiny+, Europa Clipper and IMAP.',\n",
       "  'len': 1922},\n",
       " {'abstract': 'The Nuclear Spectroscopic Telescope Array (NuSTAR) mission is the first focusing X-ray telescope in the hard X-ray (3-79 keV) band. Among the phenomena that can be studied in this energy band, some require high time resolution and stability: rotation-powered and accreting millisecond pulsars, fast variability from black holes and neutron stars, X-ray bursts, and more. Moreover, a good alignment of the timestamps of X-ray photons to UTC is key for multi-instrument studies of fast astrophysical processes. In this Paper, we describe the timing calibration of the NuSTAR mission. In particular, we present a method to correct the temperature-dependent frequency response of the on-board temperature-compensated crystal oscillator. Together with measurements of the spacecraft clock offsets obtained during downlinks passes, this allows a precise characterization of the behavior of the oscillator. The calibrated NuSTAR event timestamps for a typical observation are shown to be accurate to a precision of ~65 microsec.',\n",
       "  'len': 1032},\n",
       " {'abstract': \"We present observations of the same magnetic cloud made near Earth by the Advance Composition Explorer (ACE), Wind, and the Acceleration, Reconnection, Turbulence and Electrodynamics of the Moon's Interaction with the Sun (ARTEMIS) mission comprising the Time History of Events and Macroscale Interactions during Substorms (THEMIS) B and THEMIS C spacecraft, and later by Juno at a distance of 1.2 AU. The spacecraft were close to radial alignment throughout the event, with a longitudinal separation of $3.6^{\\\\circ}$ between Juno and the spacecraft near Earth. The magnetic cloud likely originated from a filament eruption on 22 October 2011 at 00:05 UT, and caused a strong geomagnetic storm at Earth commencing on 24 October. Observations of the magnetic cloud at each spacecraft have been analysed using Minimum Variance Analysis and two flux rope fitting models, Lundquist and Gold-Hoyle, to give the orientation of the flux rope axis. We explore the effect different trailing edge boundaries have on the results of each analysis method, and find a clear difference between the orientations of the flux rope axis at the near-Earth spacecraft and Juno, independent of the analysis method. The axial magnetic field strength and the radial width of the flux rope are calculated using both observations and fitting parameters and their relationship with heliocentric distance is investigated. Differences in results between the near-Earth spacecraft and Juno are attributed not only to the radial separation, but to the small longitudinal separation which resulted in a surprisingly large difference in the in situ observations between the spacecraft. This case study demonstrates the utility of Juno cruise data as a new opportunity to study magnetic clouds beyond 1 AU, and the need for caution in future radial alignment studies.\",\n",
       "  'len': 1844},\n",
       " {'abstract': 'This paper extends the optimal covariance steering problem for linear stochastic systems subject to chance constraints to account for optimal risk allocation. Previous works have assumed a uniform risk allocation to cast the optimal control problem as a semi-definite program (SDP), which can be solved efficiently using standard SDP solvers. We adopt an Iterative Risk Allocation (IRA) formalism, which uses a two-stage approach to solve the optimal risk allocation problem for covariance steering. The upper-stage of IRA optimizes the risk, which is proved to be a convex problem, while the lower-stage optimizes the controller with the new constraints. This is done iteratively so as to find the optimal risk allocation that achieves the lowest total cost. The proposed framework results in solutions that tend to maximize the terminal covariance, while still satisfying the chance constraints, thus leading to less conservative solutions than previous methodologies. We also introduce two novel convex relaxation methods to approximate quadratic chance constraints as second-order cone constraints. We finally demonstrate the approach to a spacecraft rendezvous problem and compare the results.',\n",
       "  'len': 1209},\n",
       " {'abstract': 'Assistive free-flying robots are a promising platform for supporting and working alongside astronauts in carrying out tasks that require interaction with the environment. However, current free-flying robot platforms are limited by existing manipulation technologies in being able to grasp and manipulate surrounding objects. Instead, gecko-inspired adhesives offer many advantages for an alternate grasping and manipulation paradigm for use in assistive free-flyer applications. In this work, we present the design of a gecko-inspired adhesive gripper for performing perching and grasping maneuvers for the Astrobee robot, a free-flying robot currently operating on-board the International Space Station. We present software and hardware integration details for the gripper units that were launched to the International Space Station in 2019 for in-flight experiments with Astrobee. Finally, we present preliminary results for on-ground experiments conducted with the gripper and Astrobee on a free-floating spacecraft test bed.',\n",
       "  'len': 1039},\n",
       " {'abstract': \"The long-term evolution of the Sun's rotation period cannot be directly observed, and is instead inferred from trends in the measured rotation periods of other Sun-like stars. Assuming the Sun spins-down as it ages, following rotation rate $\\\\propto$ age$^{-1/2}$, requires the current solar angular momentum-loss rate to be around $6\\\\times 10^{30}$erg. Magnetohydrodynamic models, and previous observations of the solar wind (from the Helios and Wind spacecraft), generally predict a values closer to $1\\\\times 10^{30}$erg or $3\\\\times 10^{30}$erg, respectively. Recently, the Parker Solar Probe (PSP) observed tangential solar wind speeds as high as $\\\\sim50$km/s in a localized region of the inner heliosphere. If such rotational flows were prevalent throughout the corona, it would imply that the solar wind angular momentum-loss rate is an order of magnitude larger than all of those previous estimations. In this letter, we evaluate the angular momentum flux in the solar wind, using data from the first two orbits of PSP. The solar wind is observed to contain both large positive (as seen during perihelion), and negative angular momentum fluxes. We analyse two solar wind streams that were repeatedly traversed by PSP; the first is a slow wind stream whose average angular momentum flux fluctuates between positive to negative, and the second is an intermediate speed stream containing a positive angular momentum flux (more consistent with a constant flow of angular momentum). When the data from PSP is evaluated holistically, the average equatorial angular momentum flux implies a global angular momentum-loss rate of around $2.6-4.2\\\\times 10^{30}$ erg (which is more consistent with observations from previous spacecraft).\",\n",
       "  'len': 1741},\n",
       " {'abstract': \"Titan's ionosphere contains a plethora of hydrocarbons and nitrile cations and anions as measured by the Ion Neutral Mass Spectrometer and Cassini Plasma Spectrometer (CAPS) onboard the Cassini spacecraft. Data from the CAPS Ion Beam Spectrometer (IBS) sensor have been examined for five close encounters of Titan during 2009. The high relative velocity of Cassini with respect to the cold ions in Titan's ionosphere allows CAPS IBS to function as a mass spectrometer. Positive ion masses between 170 and 310 u/q are examined with ion mass groups identified between 170 and 275 u/q containing between 14 and 21 heavy (carbon/nitrogen/oxygen) atoms. These groups are the heaviest positive ion groups reported so far from the available in situ ion data at Titan. The ion group peaks are found to be consistent with masses associated with Polycyclic Aromatic Compounds (PAC), including Polycyclic Aromatic Hydrocarbon (PAH) and nitrogen-bearing polycyclic aromatic molecular ions. The ion group peak identifications are compared with previously proposed neutral PAHs and are found to be at similar masses, supporting a PAH interpretation. The spacing between the ion group peaks is also investigated, finding a spacing of 12 or 13 u/q indicating the addition of C or CH. Lastly, the occurrence of several ion groups is seen to vary across the five flybys studied, possibly relating to the varying solar radiation conditions observed across the flybys. These findings further the understanding between the low mass ions and the high mass negative ions, as well as with aerosol formation in Titan's atmosphere.\",\n",
       "  'len': 1616},\n",
       " {'abstract': 'Recent miniaturization of electronics in very small, low-cost and low-power configurations suitable for use in spacecraft have inspired innovative small-scale satellite concepts, such as ChipSats, centimeter-scale satellites with a mass of a few grams. These extremely small spacecraft have the potential to usher in a new age of space science accessibility. Due to their low ballistic coefficient, ChipSats can potentially be used in a swarm constellation for extended surveys of planetary atmospheres, providing large amounts of data with high reliability and redundancy. We present a preliminary feasibility analysis of a ChipSat planetary atmospheric entry mission with the purpose of searching for traces of microscopic lifeforms in the atmosphere of Venus. Indeed, the lower cloud layer of the Venusian atmosphere could be a good target for searching for microbial lifeforms, due to the favourable atmospheric conditions and the presence of micron-sized sulfuric acid aerosols. A numerical model simulating the planetary entry of a spacecraft of specified geometry, applicable to any atmosphere for which sufficient atmospheric data are available, is implemented and verified. The results are used to create a high-level design of a ChipSat mission cruising in the Venusian atmosphere at altitudes favorable for the existence of life. The paper discusses the ChipSat mission concept and considerations about the spacecraft preliminary design at system level, including the selection of a potential payload.',\n",
       "  'len': 1523},\n",
       " {'abstract': \"The detection of phosphine in the atmosphere of Venus at an abundance of $\\\\sim 20$ ppb suggests that this gas is being generated by either indeterminate abiotic pathways or biological processes. We consider the latter possibility, and explore whether the amount of biomass required to produce the observed flux of phosphine may be reasonable. We estimate that the typical biomass densities predicted by our simple model are potentially orders of magnitude lower than the biomass density of Earth's aerial biosphere in the lower atmosphere. We briefly discuss how small spacecraft could sample the Venusian cloud decks and search for biomarkers.\",\n",
       "  'len': 655},\n",
       " {'abstract': 'We aim to investigate the distribution function of $\\\\langle Q_{Fe}\\\\rangle$ at 1AU to check if it corresponds to a bimodal wind. We use data from SWICS instrument on board the ACE spacecraft along 20 years. We propose the bi-Gaussian function as the probability distribution function that fits the $\\\\langle Q_{Fe}\\\\rangle$ distribution. We study the evolution of the parameters of the bimodal distribution with the solar cycle. We compare the outliers of the sample with the existing catalogues of ICMEs and identify new ICMEs. The $\\\\langle Q_{Fe}\\\\rangle$ at 1 AU shows a bimodal distribution related to the solar cycle. Our results confirm that $\\\\langle Q_{Fe}\\\\rangle> 12 $ is a trustworthy proxy for ICME identification and a reliable signature in the ICME boundary definition.',\n",
       "  'len': 788},\n",
       " {'abstract': \"Functionally graded materials (FGM) eliminate the stress singularity in the interface between two different materials and therefore have a wide range of applications in high temperature environments such as engines, nuclear reactors, spacecrafts etc. Therefore, it is essential to study the mechanical properties of different FGM materials. This paper aims at establishing a method for modelling FGMs in molecular dynamics (MD) to get a better insight of their mechanical properties. In this study, the mechanical characteristics of Cu-Ni FGM nanowires (NW) under uniaxial loading have been investigated using the proposed method through MD simulations. In order to describe the inter-atomic forces and hence predict the properties properly, EAM (Embedded atom model) potential has been used. The nanowire is composed of an alloying constituent in the core and the other constituent graded functionally along the outward radial direction. Simple Linear and Exponential functions have been considered as the functions which defines the grading pattern. The alloying percentage on the surface has been varied from 0% to 50% for both Cu-cored and Ni-cored nanowires. All the simulations have been carried out at 300 K. The L/D ratios are 10.56 and 10.67 for Cu-cored and Ni-cored NWs, respectively. This study suggests that Ultimate Tensile Stress and Young's modulus increase with increasing surface Ni percentage in Cu-cored NWs. However, in Ni-cored NWs these values decrease with the increase of surface Cu percentage. Also, for the same surface percentage of Ni in Cu-cored NW, the values are higher in linearly graded FGMs than that in exponentially graded FGMs. While in Ni-cored NWs, exponentially graded FGM shows higher values of UTS and E than those in linearly graded FGM. Thus, grading functions and surface percentages can be used as parameters for modulating the mechanical properties of FGM nanowires.\",\n",
       "  'len': 1925},\n",
       " {'abstract': 'Asteroid Impacts pose a major threat to all life on the Earth. Deflecting the asteroid from the impact trajectory is an important way to mitigate the threat. A kinetic impactor remains to be the most feasible method to deflect the asteroid. However, due to the constraint of the launch capability, an impactor with the limited mass can only produce a very limited amount of velocity increment for the asteroid. In order to improve the deflection efficiency of the kinetic impactor strategy, this paper proposed a new concept called the Assembled Kinetic Impactor (AKI), which is combining the spacecraft with the launch vehicle final stage. By making full use of the mass of the launch vehicle final stage, the mass of the impactor will be increased, which will cause the improvement of the deflection efficiency. According to the technical data of Long March 5 (CZ-5) launch vehicle, the missions of deflecting Bennu are designed to demonstrate the power of the AKI concept. Simulation results show that, compared with the Classic Kinetic Impactor (CKI, performs spacecraft-rocket separation), the addition of the mass of the launch vehicle final stage can increase the deflection distance to more than 3 times, and reduce the launch lead-time by at least 15 years. With the requirement of the same deflection distance, the addition of the mass of the launch vehicle final stage can reduce the number of launches to 1/3 of that of the number of CKI launches. The AKI concept makes it possible to defend Bennu-like large asteroids by a no-nuclear technique within 10-year launch lead-time. At the same time, for a single CZ-5, the deflection distance of a 140 m diameter asteroid within 10-year launch lead-time, can be increased from less than 1 Earth radii to more than 1 Earth radii.',\n",
       "  'len': 1797},\n",
       " {'abstract': \"The Mars Express (MEX) mission has been successfully operated around Mars since 2004. Among many results, MEX has provided some of the most accurate astrometric data of the two Mars moons, Phobos and Deimos. In this work we present new ephemerides of Mars' moons benefitting from all previously published astrometric data to the most recent MEX SRC data. All in all, observations from 1877 until 2018 and including spacecraft measurements from Mariner 9 to MEX were included. Assuming a homogeneous interior, we fitted Phobos' forced libration amplitude simultaneously with the Martian tidal k2/Q ratio and the initial state of the moons. Our solution of the physical libration 1.09 +/- 0.01 degrees deviates notably from the homogeneous solution. But considering the very low error bar, this may essentially suggest the necessity to consider higher order harmonics, with an improved rotation model, in the future. While most data could be successfully fitted, we found a disagreement between the Mars Reconnaissance Orbiter and the Mars Express astrometric data at the kilometer level probably associated with a biased phase correction. The present solution precision is expected at the level of a few hundreds of meters for Phobos and several hundreds of meters for Deimos for the coming years. The real accuracy of our new ephemerides will have to be confirmed by confrontation with independent observational means.\",\n",
       "  'len': 1429},\n",
       " {'abstract': 'The spacecraft attitude tracking problem is addressed with actuator faults and uncertainties among inertias, external disturbances, and, in particular, state estimates. A continuous sliding mode attitude controller is designed using attitude and angular velocity estimates from an arbitrary stable stand-alone observer. Rigorous analysis shows that the controller ensures robust stability of the entire closed-loop system as long as the observer yields state estimates with uniformly ultimately bounded estimation errors. In addition, a sequential Lyapunov analysis is utilized to obtain a convergent sequence of analytical, successively tighter upper bounds on the steady-state tracking error. Therefore, our results can be used to predict steady-state performance bounds given selected gains or facilitate gain selection given steady-state performance bounds. Numerical examples demonstrate the utility of the proposed theory.',\n",
       "  'len': 939},\n",
       " {'abstract': \"We propose an optimal control method for simultaneous slewing and vibration control of flexible spacecraft. Considering dynamics on different time scales, the optimal control problem is discretized on micro and macro time grids using a multirate variational approach. The description of the system and the necessary optimality conditions are derived through the discrete Lagrange-d'Alembert principle. The discrete problem retains the conservation properties of the continuous model and achieves high fidelity simulation at a reduced computational cost. Simulation results for a single-axis rotational maneuver demonstrate vibration suppression and achieve the same accuracy as the single rate method at reduced computational cost.\",\n",
       "  'len': 742},\n",
       " {'abstract': 'Interplanetary coronal mass ejections (ICMEs) often consist of a shock wave, sheath region, and ejecta region. The ejecta regions are divided into two broad classes: magnetic clouds (MC) that exhibit the characteristics of magnetic flux ropes and non-magnetic clouds (NMC) that do not. As CMEs result from eruption of magnetic flux ropes, it is important to answer why NMCs do not have the flux rope features. One claims that NMCs lose their original flux rope features due to the interactions between ICMEs or ICMEs and other large scale structures during their transit in the heliosphere. The other attributes this phenomenon to the geometric selection effect, i.e., when an ICME has its nose (flank, including leg and non-leg flanks) pass through the observing spacecraft, the MC (NMC) features will be detected along the spacecraft trajectory within the ejecta. In this Letter, we examine which explanation is more reasonable through the geometric properties of ICMEs. If the selection effect leads to different ejecta types, MCs should have narrower sheath region compared to NMCs from the statistical point of view, which is confirmed by our statistics. Besides, we find that NMCs have the similar size in solar cycles 23 and 24, and NMCs are smaller than MCs in cycle 23 but larger than MCs in cycle 24. This suggests that most NMCs have their leg flank pass through the spacecraft. Our geometric analyses support that all ICMEs should have a magnetic flux rope structure near 1 AU.',\n",
       "  'len': 1500},\n",
       " {'abstract': 'The 2009 Lunar CRater Observation and Sensing Satellite (LCROSS) impact mission detected water ice absorption using spectroscopic observations of the impact-generated debris plume taken by the Shepherding Spacecraft, confirming an existing hypothesis regarding the existence of water ice in permanently shadowed regions within Cabeus crater. Ground-based observations in support of the mission were able to further constrain the mass of the debris plume and the concentration of the water ice ejected during the impact. In this work, we explore additional constraints on the initial conditions of the pre-impact lunar sediment required in order to produce a plume model that is consistent with the ground-based observations. We match the observed debris plume lightcurve using a layer of dirty ice with an ice concentration that increases with depth, a layer of pure regolith, and a layer of material at about 6 meters below the lunar surface that would otherwise have been visible in the plume but has a high enough tensile strength to resist excavation. Among a few possible materials, a mixture of regolith and ice with a sufficiently high ice concentration could plausibly produce such a behavior. The vertical albedo profiles used in the best fit model allows us to calculate a pre-impact mass of water ice within Cabeus crater of $5 \\\\pm 3.0 \\\\times 10^{11}$ kg and a mass concentration of water in the lunar sediment of $8.2 \\\\pm 0.001$ %wt, assuming a water ice albedo of 0.8 and a lunar regolith density of 1.5 g cm$^{-3}$, or a mass concentration of water of $4.3 \\\\pm 0.01$ %wt, assuming a lunar regolith density of 3.0. These models fit to ground-based observations result in derived masses of regolith and water ice within the debris plume that are consistent with \\\\emph{in situ} measurements, with a model debris plume ice mass of 108 kg.',\n",
       "  'len': 1859},\n",
       " {'abstract': 'Turbulence, a ubiquitous phenomenon in interplanetary space, is crucial for the energy conversion of space plasma at multiple scales. This work focuses on the propagation, polarization and wave composition properties of the solar wind turbulence within 0.3AU, and its variation with heliocentric distances at MHD scales (from 10s to 1000s in the spacecraft frame). We present the probability density function of propagation wavevectors (${\\\\rm{PDF}}(k_\\\\parallel,k_\\\\perp)$) for solar wind turbulence winthin 0.3 AU for the first time: (1) wavevectors cluster quasi-(anti-)parallel to the local background magnetic field for $kd_{\\\\rm i}<0.02$, where $d_{\\\\rm i}$ is the ion inertial length; (2) wavevectors shift to quasi-perpendicular directions for $kd_{\\\\rm i}>0.02$. Based on our wave composition diagnosis, we find that: the outward/anti-sunward Alfvén mode dominates over the whole range of scales and distances, the spectral energy density fraction of the inward/sunward fast mode decreases with distance, and the fractional energy densities of the inward and outward slow mode increase with distance. The outward fast mode and inward Alfvén mode represent minority populations throughout the explored range of distances and scales. On average, the degree of anisotropy of the magnetic fluctuations defined with respect to the minimum variation direction decreases with increasing scale, with no trend in distance at all scales. Our results provide comprehensive insight into the scenario of transport and transfer of the solar wind fluctuations/turbulence in the inner heliosphere.',\n",
       "  'len': 1595},\n",
       " {'abstract': 'Mission designers must study many dynamical models to plan a low-cost spacecraft trajectory that satisfies mission constraints. They routinely use Poincaré maps to search for a suitable path through the interconnected web of periodic orbits and invariant manifolds found in multi-body gravitational systems. This paper is concerned with the extraction and interactive visual exploration of this structural landscape to assist spacecraft trajectory planning. We propose algorithmic solutions that address the specific challenges posed by the characterization of the topology in astrodynamics problems and allow for an effective visual analysis of the resulting information. This visualization framework is applied to the circular restricted three-body problem (CR3BP), where it reveals novel periodic orbits with their relevant invariant manifolds in a suitable format for interactive transfer selection. Representative design problems illustrate how spacecraft path planners can leverage our topology visualization to fully exploit the natural dynamics pathways for energy-efficient trajectory designs.',\n",
       "  'len': 1113},\n",
       " {'abstract': 'Spacecraft potential has often been used to infer electron density with much higher time resolution than is typically possible with plasma instruments. However, recently two studies by Torkar et al. 2017 and Graham et al. 2018 have shown that external electric fields can also have an effect on the spacecraft potential by enhancing photoelectron escape from the surface. Consequently, should the electron density derived from the spacecraft potential be used during an event with a large electric field, the estimation would be contaminated and the user would see the effects of the electric field rather than density perturbations. The goal of this paper is to propose a method to remove the electric field effects to allow the density derived from spacecraft potential to be used even during large amplitude wave events such as Langmuir waves or upper hybrid waves.',\n",
       "  'len': 879},\n",
       " {'abstract': 'The zeroth law of turbulence states that, for fixed energy input into large-scale motions, the statistical steady state of a turbulent system is independent of microphysical dissipation properties. The behavior, which is fundamental to nearly all fluid-like systems from industrial processes to galaxies, occurs because nonlinear processes generate smaller and smaller scales in the flow, until the dissipation -- no matter how small -- can thermalize the energy input. Using direct numerical simulations and theoretical arguments, we show that in strongly magnetized plasma turbulence such as that recently observed by the Parker Solar Probe (PSP) spacecraft, the zeroth law is routinely violated. Namely, when such turbulence is \"imbalanced\" -- when the large-scale energy input is dominated by Alfvén waves propagating in one direction (the most common situation in space plasmas) -- nonlinear conservation laws imply the existence of a \"barrier\" at scales near the ion gyroradius. This causes energy to build up over time at large scales. The resulting magnetic-energy spectra bear a strong similarity to those observed in situ, exhibiting a sharp, steep kinetic transition range above and around the ion-Larmor scale, with flattening at yet smaller scales, thus resolving the decade-long puzzle of the position and variability of ion-kinetic spectral breaks in plasma turbulence. The \"barrier\" effect also suggests that how a plasma is forced at large scales (the imbalance) may have a crucial influence on thermodynamic properties such as the ion-to-electron heating ratio.',\n",
       "  'len': 1590},\n",
       " {'abstract': \"Reliable spatial information can be difficult to obtain in planetary remote sensing applications because of errors present in the metadata of images taken with space probes. We have designed a pipeline to address this problem on disk-resolved images of Jupiter's moon Europa taken with New Horizons' LOng Range Reconnaissance Imager, Galileo's Solid State Imager and Voyager's Imaging Science Subsystem. We correct for errors in the spacecraft position, pointing and the target's attitude by comparing them to the same reference. We also address ways to correct for distortion prior to any metadata consideration. Finally, we propose a vectorized method to efficiently project images pixels onto an elliptic target and compute the coordinates and geometry of observation at each intercept point.\",\n",
       "  'len': 806},\n",
       " {'abstract': 'We review the current understanding of the upper atmospheres of Uranus and Neptune, and explore the upcoming opportunities available to study these exciting planets. The ice giants are the least understood planets in the solar system, having been only visited by a single spacecraft, in 1986 and 1989, respectively. The upper atmosphere plays a critical role in connecting the atmosphere to the forces and processes contained within the magnetic field. For example, auroral current systems can drive charged particles into the atmosphere, heating it by way of Joule heating. Ground-based observations of H$_3^+$ provides a powerful remote diagnostic of the physical properties and processes that occur within the upper atmosphere, and a rich data set exists for Uranus. These observations span almost three decades and have revealed that the upper atmosphere has continuously cooled between 1992 and 2018 at about 8 K/year, from $\\\\sim$750 K to $\\\\sim$500 K. The reason for this trend remain unclear, but could be related to seasonally driven changes in the Joule heating rates due to the tilted and offset magnetic field, or could be related to changing vertical distributions of hydrocarbons. H$_3^+$ has not yet been detected at Neptune, but this discovery provides low-hanging fruit for upcoming facilities such as the James Webb Space Telescope (JWST) and the next generation of 30 metre telescopes. Detecting H$_3^+$ at Neptune would enable the characterisation of its upper atmosphere for the first time since 1989. To fully understand the ice giants we need dedicated orbital missions, in the same way the Cassini spacecraft explored Saturn. Only by combining in-situ observations of the magnetic field with in-orbit remote sensing can we get the complete picture of how energy moves between the atmosphere and the magnetic field.',\n",
       "  'len': 1847},\n",
       " {'abstract': \"Dawn's framing camera observed boulders on the surface of Vesta when the spacecraft was in its lowest orbit (LAMO). We identified, measured, and mapped boulders in LAMO images, which have a scale of 20 m per pixel. We estimate that our sample is virtually complete down to a boulder size of 4 pixels (80 m). The largest boulder is a 400 m-sized block on the Marcia crater floor. Relatively few boulders reside in a large area of relatively low albedo, surmised to be the carbon-rich ejecta of the Veneneia basin, either because boulders form less easily here or live shorter. By comparing the density of boulders around craters with a known age, we find that the maximum boulder lifetime is about 300 Ma. The boulder size-frequency distribution (SFD) is generally assumed to follow a power law. We fit power laws to the Vesta SFD by means of the maximum likelihood method, but they do not fit well. Our analysis of power law exponents for boulders on other small Solar System bodies suggests that the derived exponent is primarily a function of boulder size range. The Weibull distribution mimics this behavior and fits the Vesta boulder SFD well. The Weibull distribution is often encountered in rock grinding experiments, and may result from the fractal nature of cracks propagating in the rock interior. We propose that, in general, the SFD of particles (including boulders) on the surface of small bodies follows a Weibull distribution rather than a power law.\",\n",
       "  'len': 1475},\n",
       " {'abstract': \"Solar Orbiter, the first mission of ESA's Cosmic Vision 2015-2025 programme and a mission of international collaboration between ESA and NASA, will explore the Sun and heliosphere from close up and out of the ecliptic plane. It was launched on 10 February 2020 04:03 UTC from Cape Canaveral and aims to address key questions of solar and heliospheric physics pertaining to how the Sun creates and controls the Heliosphere, and why solar activity changes with time. To answer these, the mission carries six remote-sensing instruments to observe the Sun and the solar corona, and four in-situ instruments to measure the solar wind, energetic particles, and electromagnetic fields. In this paper, we describe the science objectives of the mission, and how these will be addressed by the joint observations of the instruments onboard. The paper first summarises the mission-level science objectives, followed by an overview of the spacecraft and payload. We report the observables and performance figures of each instrument, as well as the trajectory design. This is followed by a summary of the science operations concept. The paper concludes with a more detailed description of the science objectives. Solar Orbiter will combine in-situ measurements in the heliosphere with high-resolution remote-sensing observations of the Sun to address fundamental questions of solar and heliospheric physics. The performance of the Solar Orbiter payload meets the requirements derived from the mission's science objectives. Its science return will be augmented further by coordinated observations with other space missions and ground-based observatories.\",\n",
       "  'len': 1651},\n",
       " {'abstract': 'The proper classification of plasma regions in near-Earth space is crucial to perform unambiguous statistical studies of fundamental plasma processes such as shocks, magnetic reconnection, waves and turbulence, jets and their combinations. The majority of available studies have been performed by using human-driven methods, such as visual data selection or the application of predefined thresholds to different observable plasma quantities. While human-driven methods have allowed performing many statistical studies, these methods are often time-consuming and can introduce important biases. On the other hand, the recent availability of large, high-quality spacecraft databases, together with major advances in machine-learning algorithms, can now allow meaningful applications of machine learning to in-situ plasma data. In this study, we apply the fully convolutional neural network (FCN) deep machine-leaning algorithm to the recent Magnetospheric Multi Scale (MMS) mission data in order to classify ten key plasma regions in near-Earth space for the period 2016-2019. For this purpose, we use available intervals of time series for each such plasma region, which were labeled by using human-driven selective downlink applied to MMS burst data. We discuss several quantitative parameters to assess the accuracy of both methods. Our results indicate that the FCN method is reliable to accurately classify labeled time series data since it takes into account the dynamical features of the plasma data in each region. We also present good accuracy of the FCN method when applied to unlabeled MMS data. Finally, we show how this method used on MMS data can be extended to data from the Cluster mission, indicating that such method can be successfully applied to any in situ spacecraft plasma database.',\n",
       "  'len': 1814},\n",
       " {'abstract': 'We present a major update to the 3D coronal rope ejection (3DCORE) technique for modeling coronal mass ejection flux ropes in conjunction with an Approximate Bayesian Computation (ABC) algorithm that is used for fitting the model to in situ magnetic field measurements. The model assumes an empirically motivated torus-like flux rope structure that expands self-similarly within the heliosphere, is influenced by a simplified interaction with the solar wind environment, and carries along an embedded analytical magnetic field. The improved 3DCORE implementation allows us to generate extremely large ensemble simulations which we then use to find global best-fit model parameters using an ABC sequential Monte Carlo (SMC) algorithm. The usage of this algorithm, under some basic assumptions on the uncertainty of the magnetic field measurements, allows us to furthermore generate estimates on the uncertainty of model parameters using only a single in situ observation. We apply our model to synthetically generated measurements to prove the validity of our implementation for the fitting procedure. We also present a brief analysis, within the scope of our model, of an event captured by Parker Solar Probe (PSP) shortly after its first fly-by of the Sun on 2018 November 12 at 0.25 AU. The presented toolset is also easily extendable to the analysis of events captured by multiple spacecraft and will therefore facilitate future multi-point studies.',\n",
       "  'len': 1463},\n",
       " {'abstract': 'Based on in-situ measurements by Wind spacecraft from 2005 to 2015, this letter reports for the first time a clearly scale-dependent connection between proton temperatures and the turbulence in the solar wind. A statistical analysis of proton-scale turbulence shows that increasing helicity magnitudes correspond to steeper magnetic energy spectra. In particular, there exists a positive power-law correlation (with a slope $\\\\sim 0.4$) between the proton perpendicular temperature and the turbulent magnetic energy at scales $0.3 \\\\lesssim k\\\\rho_p \\\\lesssim 1$, with $k$ being the wavenumber and $\\\\rho_p$ being the proton gyroradius. These findings present evidence of solar wind heating by the proton-scale turbulence. They also provide insight and observational constraint on the physics of turbulent dissipation in the solar wind.',\n",
       "  'len': 842},\n",
       " {'abstract': 'We propose a novel physical layer secret key generation method for the inter-spacecraft communication links. By exploiting the Doppler frequency shifts of the reciprocal spacecraft links as a unique secrecy source, spacecrafts aim to obtain identical secret keys from their individual observations. We obtain theoretical expressions for the key disagreement rate (KDR). Using generalized Gauss-Laguerre quadrature, we derive closed form expressions for the KDR. Through numerical studies, the tightness of the provided approximations are shown. Both the theoretical and numerical results demonstrate the validity and the practicality of the presented physical layer key generation procedure considering the security of the communication links of spacecrafts.',\n",
       "  'len': 769},\n",
       " {'abstract': \"Among the current challenges in Space Weather, one of the main ones is to forecast the internal magnetic configuration within Interplanetary Coronal Mass Ejections (ICMEs). Currently, a monotonic and coherent magnetic configuration observed is associated with the result of a spacecraft crossing a large flux rope with helical magnetic field lines topology. The classification of such an arrangement is essential to predict geomagnetic disturbance. Thus, the classification relies on the assumption that the ICME's internal structure is a well organized magnetic flux rope. This paper applies machine learning and a current physical flux rope analytical model to identify and further understand the internal structures of ICMEs. We trained an image recognition artificial neural network with analytical flux rope data, generated from the range of many possible trajectories within a cylindrical (circular and elliptical cross-section) model. The trained network was then evaluated against the observed ICMEs from WIND during 1995-2015. The methodology developed in this paper can classify 84% of simple real cases correctly and has a 76% success rate when extended to a broader set with 5% noise applied, although it does exhibit a bias in favor of positive flux rope classification. As a first step towards a generalizable classification and parameterization tool, these results show promise. With further tuning and refinement, our model presents a strong potential to evolve into a robust tool for identifying flux rope configurations from in situ data.\",\n",
       "  'len': 1567},\n",
       " {'abstract': 'We investigate the transition of the solar wind turbulent cascade from MHD to sub-ion range by means of a detail comparison between in situ observations and hybrid numerical simulations. In particular we focus on the properties of the magnetic field and its component anisotropy in Cluster measurements and hybrid 2D simulations. First, we address the angular distribution of wave-vectors in the kinetic range between ion and electron scales by studying the variance anisotropy of the magnetic field components. When taking into account the single-direction sampling performed by spacecraft in the solar wind, the main properties of the fluctuations observed in situ are also recovered in our numerical description. This result confirms that solar wind turbulence in the sub-ion range is characterized by a quasi-2D gyrotropic distribution of k-vectors around the mean field. We then consider the magnetic compressibility associated with the turbulent cascade and its evolution from large-MHD to sub-ion scales. The ratio of field-aligned to perpendicular fluctuations, typically low in the MHD inertial range, increases significantly when crossing ion scales and its value in the sub-ion range is a function of the total plasma beta only, as expected from theoretical predictions, with higher magnetic compressibility for higher beta. Moreover, we observe that this increase has a gradual trend from low to high beta values in the in situ data; this behaviour is well captured by the numerical simulations. The level of magnetic field compressibility that is observed in situ and in the simulations is in fairly good agreement with theoretical predictions, especially at high beta, suggesting that in the kinetic range explored the turbulence is supported by low-frequency and highly-oblique fluctuations in pressure balance, like kinetic Alfvén waves or other slowly evolving coherent structures.',\n",
       "  'len': 1909},\n",
       " {'abstract': \"The Probe Of Extreme Multi-Messenger Astrophysics (POEMMA) is designed to identify the sources of Ultra-High-Energy Cosmic Rays (UHECRs) and to observe cosmic neutrinos, both with full-sky coverage. Developed as a NASA Astrophysics Probe-class mission, POEMMA consists of two spacecraft flying in a loose formation at 525 km altitude, 28.5 deg inclination orbits. Each spacecraft hosts a Schmidt telescope with a large collecting area and wide field of view. A novel focal plane is optimized to observe both the UV fluorescence signal from extensive air showers (EASs) and the beamed optical Cherenkov signals from EASs. In POEMMA-stereo fluorescence mode, POEMMA will measure the spectrum, composition, and full-sky distribution of the UHECRs above 20 EeV with high statistics along with remarkable sensitivity to UHE neutrinos. The spacecraft are designed to quickly re-orient to a POEMMA-limb mode to observe neutrino emission from Target-of-Opportunity (ToO) transient astrophysical sources viewed just below the Earth's limb. In this mode, POEMMA will have unique sensitivity to cosmic neutrino tau events above 20 PeV by measuring the upward-moving EASs induced by the decay of the emerging tau leptons following the interactions of neutrino tau inside the Earth.\",\n",
       "  'len': 1280},\n",
       " {'abstract': 'During the last two years we have received long time-series photometric observations of bright (V mag < 8) main-sequence A- and B-type stars observed by the NASA TESS spacecraft and the Austria-Poland-Canada BRITE satellites. Using TESS observations of metallic-line A (Am) stars having peculiar element abundances, our goal is to determine whether and why these stars pulsate in multiple radial and non-radial modes, as do the delta Scuti stars in the same region of the H-R diagram. The BRITE data were requested to investigate pulsations in bright (V around 6 mag) A- and B-type stars in the Cygnus-Lyra field of view that had been proposed for observations during the now-retired NASA Kepler mission. Of the 21 (out of 62 proposed) Am stars observed by TESS so far, we find one delta Sct star and two delta Sct / gamma Dor hybrid candidates. Of the remaining stars, we find three gamma Dor candidates, six stars showing photometric variations that may or may not be associated with pulsations, and eight stars without apparent significant photometric variability. For the A- and B-type stars observed by BRITE, one star (HR 7403) shows low amplitude low frequency modes that likely are associated with its B(emission) star properties; one star (HR 7179) shows SPB variability that is also found in prior Kepler data, and two stars (HR 7284 and HR 7591) show no variability in BRITE data, although very low amplitude variability was found in TESS or Kepler data. For the TESS and BRITE targets discussed here, follow-up ground- and space-based photometric and spectroscopic observations combined with stellar modeling will be needed to constrain stellar parameters and to understand the nature of the variability.',\n",
       "  'len': 1727},\n",
       " {'abstract': 'We now know that the outer solar system is host to at least six diverse planetary ring systems, each of which is a scientifically compelling target with the potential to inform us about the evolution, history and even the internal structure of the body it adorns. These diverse ring systems represent a set of distinct local laboratories for understanding the physics and dynamics of planetary disks, with applications reaching beyond our Solar System. We highlight the current status of planetary rings science and the open questions before the community to promote continued Earth-based and spacecraft-based investigations into planetary rings. As future spacecraft missions are launched and more powerful telescopes come online in the decades to come, we urge NASA for continued support of investigations that advance our understanding of planetary rings, through research and analysis of data from existing facilities, more laboratory work and specific attention to strong rings science goals during future mission selections.',\n",
       "  'len': 1041},\n",
       " {'abstract': 'In situ measurements of the heliospheric particle populations by the Voyager spacecraft can only be put in an appropriate context with remote-sensing observations of energetic and interstellar neutral atoms (ENAs and ISN, respectively) at 1 au when the time delay between the production and the observation times is taken into account. ENA times of flight from the production regions in the heliosheath are relatively easy to estimate because these atoms follow almost constant speed, force-free trajectories. For the ISN populations, dynamical and ballistic selection effects are important, and times of flight are much longer. We estimate these times for ISN He and H atoms observed by IBEX and in the future by IMAP using the WTPM model with synthesis method. We show that for the primary population atoms, the times of flight are on the order of three solar cycle periods, with a spread equivalent to one solar cycle. For the secondary populations, the times of flight are on the order of ten solar cycle periods, and during the past ten years of observations, IBEX has been collecting secondary He atoms produced in the OHS during almost entire 19th century. ISN atoms penetrating the heliopause at the time of Voyager crossing will become gradually visible about 2027, during the planned IMAP observations. Hypothetical variations in the ISN flow in the Local Interstellar Medium are currently not detectable. Nevertheless, we expect steady-state heliosphere models used with appropriately averaged solar wind parameters to be suitable for understanding the ISN observations.',\n",
       "  'len': 1592},\n",
       " {'abstract': 'A sample of isolated Earth-impacting ICMEs that occurred in the period January 2008 to August 2014 is analysed in order to study in detail the ICME in situ signatures with respect to the type of filament eruption related to the corresponding CME. For Earth-directed CMEs, a kinematical study was performed using the STEREO-A, B COR1 and COR2 coronagraphs and the Heliospheric Imagers HI1. Based on the extrapolated CME kinematics, we identified interacting CMEs, which were excluded from further analysis. Applying this approach, a set of 31 isolated Earth-impacting CMEs was unambiguously identified and related to the in situ measurements recorded by the Wind spacecraft. We classified the events into subsets with respect to the CME source location as well as with respect to the type of the associated filament eruption. Hence, the events are divided into three subsamples: active region (AR) CMEs, disappearing filament (DSF) CMEs, and stealthy CMEs. The related three groups of ICMEs were further divided into two subsets: magnetic obstacle (MO) events (out of which four were stealthy), covering ICMEs that at least partly expose characteristics of flux ropes, and ejecta (EJ) events, not showing such characteristics. In the next step, MO-events were analysed in more detail, considering the magnetic field strengths and the plasma characteristics in three different segments of the ICMEs, defined as the turbulent sheath (TS), the frontal region (FR), and the MO itself. The analysis revealed various well-defined correlations for AR, DSF, and stealthy ICMEs, which we interpreted considering basic physical concepts. Our results support the hypothesis that ICMEs show different signatures depending on the in situ spacecraft trajectory, in terms of apex versus flank hits.',\n",
       "  'len': 1793},\n",
       " {'abstract': 'Context. Coronal mass ejections (CMEs) are large eruptions of magnetised plasma from the Sun that are often accompanied by solar radio bursts produced by accelerated electrons. Aims. A powerful source for accelerating electron beams are CME-driven shocks, however, there are other mechanisms capable of accelerating electrons during a CME eruption. So far, studies have relied on the traditional classification of solar radio bursts into five groups (Type I-V) based mainly on their shapes and characteristics in dynamic spectra. Here, we aim to determine the origin of moving radio bursts associated with a CME that do not fit into the present classification of the solar radio emission. Methods. By using radio imaging from the Nançay Radioheliograph, combined with observations from the Solar Dynamics Observatory, Solar and Heliospheric Observatory, and Solar Terrestrial Relations Observatory spacecraft, we investigate the moving radio bursts accompanying two subsequent CMEs on 22 May 2013. We use three-dimensional reconstructions of the two associated CME eruptions to show the possible origin of the observed radio emission. Results. We identified three moving radio bursts at unusually high altitudes in the corona that are located at the northern CME flank and move outwards synchronously with the CME. The radio bursts correspond to fine-structured emission in dynamic spectra with durations of ~1 s, and they may show forward or reverse frequency drifts. Since the CME expands closely following an earlier CME, a low coronal CME-CME interaction is likely responsible for the observed radio emission.',\n",
       "  'len': 1624},\n",
       " {'abstract': 'Mars Exospheric Neutral Composition Analyzer (MENCA) of Mars Orbiter Mission (MOM) measures the \\\\emph{in-situ} neutral upper atmospheric constituents of Mars. Martian lower atmosphere predominated by the presence of $CO_2$ which photo-dissociates into atomic oxygen ($O$) in higher altitudes much near the exobase. Atomic $O$ plays a significant role in invoking the stronger presence of $O_2^+$ in the Martian ionosphere. Primary photo-dissociative species $CO_{2}$, crossover their neutral abundance with atomic $O$ in the collisionless heterogenous atmosphere with varying local solar conditions. Initial measurements from Neutral Gas and Ion Mass Spectrometer (NGIMS) instrument on Mars Atmosphere and Volatile Evolution (MAVEN) estimated these crossover/transition altitudes wavering between $\\\\approx$225 km to 240 km during solar maximum conditions with peak solar illuminations. MENCA sampled the neutral atmospheric species, below the exobase up to the periareion of $\\\\approx$160 km, under low solar activity conditions during June 2018. Observations of partial pressures of $CO_2$ and $O$ in subsequent orbits reveals that solar inputs are crucial in quantifying these crossing points, where $[O]/[CO_2]$ remain unity, alongside the influences from temperature. The multi-spacecraft measurements of the direct influences of solar wind charged particle fluxes and velocity on the daily variation of neutral thermospheric/exospheric compositions were observed on the local evening hours of Mars and presented. It marks the first-ever direct \\\\emph{in-situ} observation of interaction between the energetic solar particle radiations on Martian exospheric compositions, potentially contributing to the steady escape and differing population of atomic $[O]$ in the exosphere.',\n",
       "  'len': 1789},\n",
       " {'abstract': 'Context. The electrostatic potential of a spacecraft, VS, is important for the capabilities of in situ plasma measurements. Rosetta has been found to be negatively charged during most of the comet mission and even more so in denser plasmas. Aims. Our goal is to investigate how the negative VS correlates with electron density and temperature and to understand the physics of the observed correlation. Methods. We applied full mission comparative statistics of VS, electron temperature, and electron density to establish VS dependence on cold and warm plasma density and electron temperature. We also used Spacecraft-Plasma Interaction System (SPIS) simulations and an analytical vacuum model to investigate if positively biased elements covering a fraction of the solar array surface can explain the observed correlations. Results. Here, the VS was found to depend more on electron density, particularly with regard to the cold part of the electrons, and less on electron temperature than was expected for the high flux of thermal (cometary) ionospheric electrons. This behaviour was reproduced by an analytical model which is consistent with numerical simulations. Conclusions. Rosetta is negatively driven mainly by positively biased elements on the borders of the front side of the solar panels as these can efficiently collect cold plasma electrons. Biased elements distributed elsewhere on the front side of the panels are less efficient at collecting electrons apart from locally produced electrons (photoelectrons). To avoid significant charging, future spacecraft may minimise the area of exposed bias conductors or use a positive ground power system.',\n",
       "  'len': 1671},\n",
       " {'abstract': 'Near Rectilinear Halo Orbits (NRHOs), a subclass of halo orbits around the L1 and L2 Lagrange points, are promising candidates for future lunar gateways in cis-lunar space and as staging orbits for lunar missions. Closed-loop control is beneficial to compensate orbital perturbations and potential instabilities while maintaining spacecraft on NRHOs and performing relative motion maneuvers. This paper investigates the use of nonlinear model predictive control (NMPC) coupled with low-thrust actuators for station-keeping on NRHOs. It is demonstrated through numerical simulations that NMPC is able to stabilize a spacecraft to a reference orbit and handle control constraints. Further, it is shown that the computational burden of NMPC can be managed using specialized optimization routines and suboptimal approaches without jeopardizing closed-loop performance.',\n",
       "  'len': 875},\n",
       " {'abstract': 'This paper summarizes some recent results in the low-frequency radio physics of the Sun. The spatial domain covers the space from the outer corona to the orbit of Earth. The results obtained make use of radio dynamic spectra and white-light coronagraph images and involve radio bursts associated with solar eruptions and those occurring outside solar eruptions. In particular, the connection between type II radio bursts and the sustained gamma-ray emission from the Sun is highlighted. The directivity of interplanetary type IV bursts found recently is discussed to understand the physical reason behind it. A new event showing the diffuse interplanetary radio emission (DIRE) is introduced and its properties are compared with those of regular type II bursts. The DIRE is from the flanks of a CME-driven shock propagating through nearby streamer. Finally, a new noise storm observed by two spacecraft is briefly discussed to highlight its evolution over two solar rotations including the disruption and recovery by solar eruptions.',\n",
       "  'len': 1044},\n",
       " {'abstract': 'The Time-of-Arrival (ToA) of coronal mass ejections (CME) at Earth is a key parameter due to the space weather phenomena associated with the CME arrival, such as intense geomagnetic storms. Despite the incremental use of new instrumentation and the development of novel methodologies, ToA estimated errors remain above 10 hours on average. Here, we investigate the prediction of the ToA of CMEs using observations from heliospheric imagers, i.e., from heliocentric distances higher than those covered by the existent coronagraphs. In order to perform this work we analyse 14 CMEs observed by the heliospheric imagers HI-1 onboard the twin STEREO spacecraft to determine their front location and speed. The kinematic parameters are derived with a new technique based on the Elliptical Conversion (ElCon) method, which uses simultaneous observations from the two viewpoints from STEREO. Outside the field of view of the instruments, we assume that the dynamics of the CME evolution is controlled by aerodynamic drag, i.e., a force resulting from the interaction with particles from the background solar wind. To model the drag force we use a physical model that allows us to derive its parameters without the need to rely on drag coefficients derived empirically. We found a CME ToA mean error of $1.6\\\\pm8.0$ hours ToA and a mean absolute error of $6.9\\\\pm3.9$ hours for a set of 14 events. The results suggest that observations from HI-1 lead to estimates with similar errors to observations from coronagraphs.',\n",
       "  'len': 1519},\n",
       " {'abstract': \"In situ observations of small asteroids show that surfaces covered by boulders and coarse terrain are frequent on such bodies. Regolith grain sizes have distributions on approximately mm and cm scales, and the behavior of such large grains in the very low-gravity environments of small body surfaces dictates their morphology and evolution. In order to support the understanding of natural processes (e.g., the recapturing of impact ejecta) or spacecraft-induced interactions (e.g., the fate of a small lander), we aim to experimentally investigate the response of coarse-grained target surfaces to very-low-speed impacts (below 2 m/s). We present the outcome of 86 low-speed impacts of a cm-sized spherical projectile into a bed of simulated regolith, composed of irregular mm- and cm-sized grains. These impacts were performed under vacuum and microgravity conditions. Our results include measurements for the projectile coefficient of restitution and penetration depth, as well as ejecta production, speed, and mass estimation. We find that impact outcomes include the frequent occurrence of projectile bouncing and tangential rolling on the target surface upon impact. Ejecta is produced for impact speeds higher than about 12 cm/s, and ejecta speeds scale with the projectile to target the grain size ratio and the impact speed. Ejected mass estimations indicate that ejecta is increasingly difficult to produce for increasing grain sizes. Coefficients of restitution of rebounding projectiles do not display a dependency on the target grain size, unlike their maximum penetration depth, which can be scaled with the projectile to target grain size ratio. Finally, we compare our experimental measurements to spacecraft data and numerical work on Hayabusa 2's MASCOT landing on the surface of the asteroid Ryugu.\",\n",
       "  'len': 1828},\n",
       " {'abstract': 'The correlation scale and the Taylor scale are evaluated for interplanetary magnetic field fluctuations from two-point, single time correlation function using the Advanced Composition Explorer (ACE), Wind, and Cluster spacecraft data during the time period from 2001 to 2017, which covers over an entire solar cycle. The correlation scale and the Taylor scale are respectively compared with the sunspot number to investigate the effects of solar activity on the structure of the plasma turbulence. Our studies show that the Taylor scale increases with the increasing sunspot number, which indicates that the Taylor scale is positively correlated with the energy cascade rate, and the correlation coefficient between the sunspot number and the Taylor scale is 0.92. However, these results are not consistent with the traditional knowledge in hydrodynamic dissipation theories. One possible explanation is that in the solar wind, the fluid approximation fails at the spatial scales near the dissipation ranges. Therefore, the traditional hydrodynamic turbulence theory is incomplete for describing the physical nature of the solar wind turbulence, especially at the spatial scales near the kinetic dissipation scales.',\n",
       "  'len': 1226},\n",
       " {'abstract': 'This paper investigates the use of Reinforcement Learning for the robust design of low-thrust interplanetary trajectories in presence of severe disturbances, modeled alternatively as Gaussian additive process noise, observation noise, control actuation errors on thrust magnitude and direction, and possibly multiple missed thrust events. The optimal control problem is recast as a time-discrete Markov Decision Process to comply with the standard formulation of reinforcement learning. An open-source implementation of the state-of-the-art algorithm Proximal Policy Optimization is adopted to carry out the training process of a deep neural network, used to map the spacecraft (observed) states to the optimal control policy. The resulting Guidance and Control Network provides both a robust nominal trajectory and the associated closed-loop guidance law. Numerical results are presented for a typical Earth-Mars mission. First, in order to validate the proposed approach, the solution found in a (deterministic) unperturbed scenario is compared with the optimal one provided by an indirect technique. Then, the robustness and optimality of the obtained closed-loop guidance laws is assessed by means of Monte Carlo campaigns performed in the considered uncertain scenarios. These preliminary results open up new horizons for the use of reinforcement learning in the robust design of interplanetary missions.',\n",
       "  'len': 1420},\n",
       " {'abstract': 'Operating Earth observing satellites requires efficient planning methods that coordinate activities of multiple spacecraft. The satellite task planning problem entails selecting actions that best satisfy mission objectives for autonomous execution. Task scheduling is often performed by human operators assisted by heuristic or rule-based planning tools. This approach does not efficiently scale to multiple assets as heuristics frequently fail to properly coordinate actions of multiple vehicles over long horizons. Additionally, the problem becomes more difficult to solve for large constellations as the complexity of the problem scales exponentially in the number of requested observations and linearly in the number of spacecraft. It is expected that new commercial optical and radar imaging constellations will require automated planning methods to meet stated responsiveness and throughput objectives. This paper introduces a new approach for solving the satellite scheduling problem by generating an infeasibility-based graph representation of the problem and finding a maximal independent set of vertices for the graph. The approach is tested on a scenarios of up to 10,000 requested imaging locations for the Skysat constellation of optical satellites as well as simulated constellations of up to 24 satellites. Performance is compared with contemporary graph-traversal and mixed-integer linear programming approaches. Empirical results demonstrate improvements in both the solution time along with the number of scheduled collections beyond baseline methods. For large problems, the maximum independent set approach is able find a feasible schedule with 8% more collections in 75% less time.',\n",
       "  'len': 1713},\n",
       " {'abstract': 'The Rosetta spacecraft escorted Comet 67P/Churyumov-Gerasimenko for 2 years along its journey through the Solar System between 3.8 and 1.24~au. Thanks to the high resolution mass spectrometer on board Rosetta, the detailed ion composition within a coma has been accurately assessed in situ for the very first time. Previous cometary missions, such as $\\\\text{Giotto}$, did not have the instrumental capabilities to identify the exact nature of the plasma in a coma because the mass resolution of the spectrometers onboard was too low to separate ion species with similar masses. In contrast, the Double Focusing Mass Spectrometer (DFMS), part of the Rosetta Orbiter Spectrometer for Ion and Neutral Analysis on board Rosetta (ROSINA), with its high mass resolution mode, outperformed all of them, revealing the diversity of cometary ions. We calibrated and analysed the set of spectra acquired by DFMS in ion mode from October 2014 to April 2016. In particular, we focused on the range from 13-39 u$\\\\cdot$q$^{-1}$. The high mass resolution of DFMS allows for accurate identifications of ions with quasi-similar masses, separating $^{13}$C$^+$ from CH$^+$, for instance. We confirm the presence in situ of predicted cations at comets, such as CH$_m^+$ ($m=1-4$), H$_n$O$^+$ ($n=1-3$), O$^+$, Na$^+$, and several ionised and protonated molecules. Prior to Rosetta, only a fraction of them had been confirmed from Earth-based observations. In addition, we report for the first time the unambiguous presence of a molecular dication in the gas envelope of a Solar System body, namely CO$_2^{++}$.',\n",
       "  'len': 1601},\n",
       " {'abstract': \"Above lunar crustal magnetic anomalies, large fractions of solar wind electrons and ions can be scattered and stream back towards the solar wind flow, leading to a number of interesting effects such as electrostatic instabilities and waves. These electrostatic structures can also interact with the background plasma, resulting in electron heating and scattering. We study the electrostatic waves and electron heating observed over the lunar magnetic anomalies by analyzing data from the Acceleration, Reconnection, Turbulence, and Electrodynamics of Moon's Interaction with the Sun (ARTEMIS) spacecraft. Based on the analysis of two lunar flybys in 2011 and 2013, we find that the electron two-stream instability (ETSI) and electron cyclotron drift instability (ECDI) may play an important role in driving the electrostatic waves. We also find that ECDI, along with the modified two-stream instability (MTSI), may provide the mechanisms responsible for substantial isotropic electron heating over the lunar magnetic anomalies.\",\n",
       "  'len': 1038},\n",
       " {'abstract': \"The recently discovered first high velocity hyperbolic objects passing through the Solar System, 1I/'Oumuamua and 2I/Borisov, have raised the question about near term missions to Interstellar Objects. In situ spacecraft exploration of these objects will allow the direct determination of both their structure and their chemical and isotopic composition, enabling an entirely new way of studying small bodies from outside our solar system. In this paper, we map various Interstellar Object classes to mission types, demonstrating that missions to a range of Interstellar Object classes are feasible, using existing or near-term technology. We describe flyby, rendezvous and sample return missions to interstellar objects, showing various ways to explore these bodies characterizing their surface, dynamics, structure and composition. Interstellar objects likely formed very far from the solar system in both time and space; their direct exploration will constrain their formation and history, situating them within the dynamical and chemical evolution of the Galaxy. These mission types also provide the opportunity to explore solar system bodies and perform measurements in the far outer solar system.\",\n",
       "  'len': 1212},\n",
       " {'abstract': 'Planetary spatial data returned by spacecraft, including images and higher-order products such as mosaics, controlled basemaps, and digital elevation models (DEMs), are of critical importance to NASA, its commercial partners and other space agencies. Planetary spatial data are an essential component of basic scientific research and sustained planetary exploration and operations. The Planetary Data System (PDS) is performing the essential job of archiving and serving these data, mostly in raw or calibrated form, with less support for higher-order, more ready-to-use products. However, many planetary spatial data remain not readily accessible to and/or usable by the general science user because particular skills and tools are necessary to process and interpret them from the raw initial state. There is a critical need for planetary spatial data to be more accessible and usable to researchers and stakeholders. A Planetary Spatial Data Infrastructure (PSDI) is a collection of data, tools, standards, policies, and the people that use and engage with them. A PSDI comprises an overarching support system for planetary spatial data. PSDIs (1) establish effective plans for data acquisition; (2) create and make available higher-order products; and (3) consider long-term planning for correct data acquisition, processing and serving (including funding). We recommend that Planetary Spatial Data Infrastructures be created for all bodies and key regions in the Solar System. NASA, with guidance from the planetary science community, should follow established data format standards to build foundational and framework products and use those to build and apply PDSIs to all bodies. Establishment of PSDIs is critical in the coming decade for several locations under active or imminent exploration, and for all others for future planning and current scientific analysis.',\n",
       "  'len': 1884},\n",
       " {'abstract': 'Three spacecraft of LISA/TAIJI mission follow their respective geodesic trajectories, and the arm lengths formed by the pairs of spacecraft are unequal due to solar system dynamics. Time delay interferometry is proposed to suppress the laser frequency noise raised by the unequal-arm-ness. By employing a set of numerical mission orbit achieved from an ephemeris framework, we investigate the averaged sensitivity of the first-generation time-delay interferometry configurations and their corresponding optimal A, E, and T channels. We find that the sensitivity of the T channel from Michelson and Monitor are differing from the equal-arm case, and their performance are sensitive to the inequality of the arm lengths. We also evaluate the laser frequency noise due to the mismatch of laser beam paths, and derive formula for the path difference in the unequal arm scenario for the first-generation TDI configurations.',\n",
       "  'len': 929},\n",
       " {'abstract': 'We outline a flagship-class mission concept focused on studying Titan as a global system, with particular emphasis on the polar regions. Investigating Titan from the unique standpoint of a polar orbit would enable comprehensive global maps to uncover the physics and chemistry of the atmosphere, and the topography and geophysical environment of the surface and subsurface. The mission includes two key elements: (1) an orbiter spacecraft, which also acts as a data relay, and (2) one or more small probes to directly investigate Titan\\'s seas and make the first direct measurements of their liquid composition and physical environment. The orbiter would carry a sophisticated remote sensing payload, including a novel topographic lidar, a long-wavelength surface-penetrating radar, a sub-millimeter sounder for winds and for mesospheric/thermospheric composition, and a camera and near-infrared spectrometer. An instrument suite to analyze particles and fields would include a mass spectrometer to focus on the interactions between Titan\\'s escaping upper atmosphere and the solar wind and Saturnian magnetosphere. The orbiter would enter a stable polar orbit around 1500 to 1800 km, from which vantage point it would make global maps of the atmosphere and surface. One or more probes, released from the orbiter, would investigate Titan\\'s seas in situ, including possible differences in composition between higher and lower latitude seas, as well as the atmosphere during the parachute descent. The number of probes, as well as the instrument complement on the orbiter and probe, remain to be finalized during a mission study that we recommend to NASA as part of the NRC Decadal Survey for Planetary Science now underway, with the goal of an overall mission cost in the \"small flagship\" category of ~$2 bn. International partnerships, similar to Cassini-Huygens, may also be included for consideration.',\n",
       "  'len': 1912},\n",
       " {'abstract': \"The first definite interstellar object observed in our solar system was discovered in October of 2017 and was subsequently designated 1I/'Oumuamua. In addition to its extrasolar origin, observations and analysis of this object indicate some unusual features which can only be explained by in-situ exploration. For this purpose, various spacecraft intercept missions have been proposed. Their propulsion schemes have been chemical, exploiting a Jupiter and Solar Oberth Maneuver (mission duration of 22 years) and also using Earth-based lasers to propel laser sails (1-2 years), both with launch dates in 2030. For the former, mission durations are quite prolonged and for the latter, the necessary laser infrastructure may not be in place by 2030. In this study Nuclear Thermal Propulsion (NTP) is examined which has yet to materialise as far as real missions are concerned, but due to its research and development in the NASA Rover/NERVA programs, actually has a higher TRL than laser propulsion. Various solid reactor core options are studied, using either engines directly derived from the NASA programs, or more advanced options, like a proposed particle bed NTP system. With specific impulses at least twice those of chemical rockets, NTP opens the opportunity for much higher {\\\\Delta}V budgets, allowing simpler and more direct, time-saving trajectories to be exploited. For example a spacecraft with an upgraded NERVA/Pewee-class NTP travelling along an Earth-Jupiter-1I trajectory, would reach 1I/'Oumuamua within 14 years of a launch in 2031. The payload mass to 1I/'Oumuamua would be around 2.5metric tonnes, but even larger masses and shorter mission durations can be achieved with some of the more advanced NTP options studied. In all 4 different proposed NTP systems and 5 different trajectory scenarios are examined.\",\n",
       "  'len': 1841},\n",
       " {'abstract': 'The secondary electron emission process is essential for the optimal operation of a wide range of applications, including fusion reactors, high-energy accelerators, or spacecraft. The process can be influenced and controlled by the use of a magnetic field. An analytical solution is proposed to describe the secondary electron emission process in an oblique magnetic field. It was derived from Monte Carlo simulations. The analytical formula captures the influence of the magnetic field magnitude and tilt, electron emission energy, electron reflection on the surface, and electric field intensity on the secondary emission process. The last two parameters increase the effective emission while the others act the opposite. The electric field effect is equivalent to a reduction of the magnetic field tilt. A very good agreement is shown between the analytical and numerical results for a wide range of parameters. The analytical solution is a convenient tool for the theoretical study and design of magnetically assisted applications, providing realistic input for subsequent simulations.',\n",
       "  'len': 1100},\n",
       " {'abstract': 'The construction of a space elevator would be an inspiring feat of planetary engineering of immense cost and risk. But would the benefit outweigh the costs and risks? What, precisely, is the purpose for building such a structure? For example, what if the space elevator could provide propellant-free (free release) orbital transfer to every planet in the solar system and beyond on a daily basis? In our view, this benefit might outweigh the costs and risks. But can a space elevator provide such a service? In this manuscript, we examine 3 tiers of space elevator launch system design and provide a detailed mathematical analysis of the orbital mechanics of spacecraft utilizing such designs. We find the limiting factor in all designs is the problem of transition to the ecliptic plane. For Tiers 1 and 2, we find that free release transfers to all the outer planets is possible, achieving velocities far beyond the ability of current Earth-based rocket technology, but with significant gaps in coverage due to planetary alignment. For Tier 3 elevators, however, we find that fast free release transfers to all planets in the solar system is possible on a daily basis. Finally, we show that Tier 2 and 3 space elevators can potentially use counterweights to perform staged slingshot maneuvers, providing a velocity multiplier which could dramatically reduce transit times to outer planets and interstellar destinations.',\n",
       "  'len': 1432},\n",
       " {'abstract': 'A recent paper by Owens (2018) presents a statistical analysis of the properties of interplanetary coronal mass ejections (ICMEs) and variations in their compositions and ion charge-state signatures using data from the Advanced Composition Explorer (ACE) spacecraft. However, the article contains several serious flaws, which we will discuss here.',\n",
       "  'len': 358},\n",
       " {'abstract': \"Developed as NASA Astrophysics Probe-class mission, the Probe Of Extreme Multi-Messenger Astrophysics (POEMMA) is designed to identify the sources of ultra-high energy cosmic rays (UHECRs) and to observe cosmic neutrinos. POEMMA consists of two spacecraft flying in a loose formation at 525 km altitude, 28.5$^\\\\circ$ inclination orbits. Each spacecraft hosts a Schmidt telescope with a large collecting area and wide Field-of-View (FoV). A novel focal plane is employed that is optimized to observe both the UV fluorescence signal from extensive air showers (EASs) and the optical Cherenkov signals from EASs. In UHECR stereo fluorescence mode, POEMMA will measure the spectrum, composition, and full-sky distribution of the UHECRs above 20 EeV with high statistics along with remarkable sensitivity to UHE neutrinos. The POEMMA spacecraft are designed to quickly re-orient to a Target-of-Opportunity (ToO) neutrino mode to observe transient astrophysical sources with unique sensitivity. In this mode, POEMMA will be able to detect cosmic tau neutrino events above 20 PeV by measuring the upward-moving EASs for $\\\\tau$-lepton decays induced from tau neutrino interactions in the Earth. In this paper, POEMMA's science goals and instrument design are summarized with a focus on the SiPM implementation in POEMMA, along with a detailed discussion of the properties of the Cherenkov EAS signal in the context of wide wavelength sensitivity offered by SiPMs. A comparison of the fluorescence response between SiPMs and the MAPMTs currently planned for use in POEMMA will also be discussed, assessing the potential for SiPMs to perform EAS fluorescence measurements.\",\n",
       "  'len': 1673},\n",
       " {'abstract': 'We report observation of isotropic interplanetary dust (IPD) by analyzing the infrared (IR) maps of Diffuse Infrared Background Experiment (DIRBE) onboard the Cosmic Background Explorer (COBE) spacecraft. To search for the isotropic IPD, we perform new analysis in terms of solar elongation angle ($\\\\epsilon$), because we expect zodiacal light (ZL) intensity from the isotropic IPD to decrease as a function of $\\\\epsilon$. We use the DIRBE weekly-averaged maps covering $64^\\\\circ \\\\lesssim \\\\epsilon \\\\lesssim 124^\\\\circ$ and inspect the $\\\\epsilon$-dependence of residual intensity after subtracting conventional ZL components. We find the $\\\\epsilon$-dependence of the residuals, indicating the presence of the isotropic IPD. However, the mid-IR $\\\\epsilon$-dependence is different from that of the isotropic IPD model at $\\\\epsilon \\\\gtrsim 90^\\\\circ$, where the residual intensity increases as a function of $\\\\epsilon$. To explain the observed $\\\\epsilon$-dependence, we assume a spheroidal IPD cloud showing higher density further away from the sun. We estimate intensity of the near-IR extragalactic background light (EBL) by subtracting the spheroidal component, assuming the spectral energy distribution from the residual brightness at $12\\\\,{\\\\rm \\\\mu m}$. The EBL intensity is derived as $45_{-8}^{+11}$, $21_{-4}^{+3}$, and $15\\\\pm3\\\\,{\\\\rm nWm^{-2}sr^{-1}}$ at $1.25$, $2.2$, and $3.5\\\\,{\\\\rm \\\\mu m}$, respectively. The EBL is still a few times larger than integrated light of normal galaxies, suggesting existence of unaccounted extragalactic sources.',\n",
       "  'len': 1556},\n",
       " {'abstract': 'This work adds on to the on-going efforts to provide more autonomy to space robots. Here the concept of programming by demonstration or imitation learning is used for trajectory planning of manipulators mounted on small spacecraft. For greater autonomy in future space missions and minimal human intervention through ground control, a robot arm having 7-Degrees of Freedom (DoF) is envisaged for carrying out multiple tasks like debris removal, on-orbit servicing and assembly. Since actual hardware implementation of microgravity environment is extremely expensive, the demonstration data for trajectory learning is generated using a model predictive controller (MPC) in a physics based simulator. The data is then encoded compactly by Probabilistic Movement Primitives (ProMPs). This offline trajectory learning allows faster reproductions and also avoids any computationally expensive optimizations after deployment in a space environment. It is shown that the probabilistic distribution can be used to generate trajectories to previously unseen situations by conditioning the distribution. The motion of the robot (or manipulator) arm induces reaction forces on the spacecraft hub and hence its attitude changes prompting the Attitude Determination and Control System (ADCS) to take large corrective action that drains energy out of the system. By having a robot arm with redundant DoF helps in finding several possible trajectories from the same start to the same target. This allows the ProMP trajectory generator to sample out the trajectory which is obstacle free as well as having minimal attitudinal disturbances thereby reducing the load on ADCS.',\n",
       "  'len': 1668},\n",
       " {'abstract': \"Although the debate regarding the origin of the cyano (CN) radical in comets has been ongoing for many decades, it has yielded no definitive answer to date. CN could previously only be studied remotely, strongly hampering efforts to constrain its origin because of very limited spatial information. Thanks to the European Space Agency's Rosetta spacecraft, which orbited comet 67P/Churyumov-Gerasimenko for two years, we can investigate, for the first time, CN around a comet at high spatial and temporal resolution. On board Rosetta's orbiter module, the high-resolution double-focusing mass spectrometer DFMS, part of the ROSINA instrument suite, analyzed the neutral volatiles (including HCN and the CN radical) in the inner coma of the comet throughout that whole two-year phase and at variable cometocentric distances. From a thorough analysis of the full-mission data, the abundance of CN radicals in the cometary coma has been derived. Data from a close flyby event in February 2015 indicate a distributed origin for the CN radical in comet 67P/Churyumov-Gerasimenko.\",\n",
       "  'len': 1085},\n",
       " {'abstract': 'We analyze the detection capability of Coronal Mass Ejections (CMEs) for all currently operating coronagraphs in space. We define as CMEs events that propagate beyond 10 solar radii with morphologies broadly consistent with a magnetic flux rope presence. We take advantage of multi-viewpoint observations over five month-long intervals, corresponding to special orbital configurations of the coronagraphs aboard the STEREO and SOHO missions. This allows us to sort out CMEs from other outward-propagating features (e.g. waves or outflows), and thus to identify the total number of unique CMEs ejected during those periods. We determine the CME visibility functions of the STEREO COR2-A/B and LASCO C2/C3 coronagraphs directly as the ratio of observed to unique CMEs. The visibility functions range from 0.71 to 0.92 for a 95% confidence interval. By comparing detections between coronagraphs on the same spacecraft and from multiple spacecraft, we assess the influence of field of view, instrument performance, and projection effects on the CME detection ability without resorting to proxies, such as flares or radio bursts. We find that no major CMEs are missed by any of the coronagraphs, that a few slow halo-like events may be missed in synoptic cadence movies and, that narrow field of view coronagraphs have difficulties discriminating between CMEs and other ejections leading to false detection rates. We conclude that CME detection can only be validated with multi-viewpoint imaging-- two coronagraphs in quadrature offer adequate detection capability. Finally, we apply the visibility functions to observed CME rates resulting in upward corrections of 40%.',\n",
       "  'len': 1676},\n",
       " {'abstract': 'We examine the use of the interplanetary substance as a possible fuel for spacecraft plasma thrusters for interplanetary flights. The solar radiation is considered as an energy source for the ionization and the acceleration of particles of captured space environment that is used as drop weight. The method of capturing the space environment, which is dependent on its density, the velocity of the spacecraft, and the power of solar radiation, defines the technical feasibility of this scheme. Both pulsed and continuous operations of the plasma accelerator are possible under some conditions as shown in the estimates.',\n",
       "  'len': 630},\n",
       " {'abstract': \"A policy for six-degree-of-freedom docking maneuvers is developed through reinforcement learning and implemented as a feedback control law. Reinforcement learning provides a potential framework for robust, autonomous maneuvers in uncertain environments with low on-board computational cost. Specifically, proximal policy optimization is used to produce a docking policy that is valid over a portion of the six-degree-of-freedom state-space while striving to minimize performance and control costs. Experiments using the simulated Apollo transposition and docking maneuver exhibit the policy's capabilities and provide a comparison with standard optimal control techniques. Furthermore, specific challenges and work-arounds, as well as a discussion on the benefits and disadvantages of reinforcement learning for docking policies, are discussed to facilitate future research. As such, this work will serve as a foundation for further investigation of learning-based control laws for spacecraft proximity operations in uncertain environments.\",\n",
       "  'len': 1051},\n",
       " {'abstract': 'We investigate Abydos, the final landing site of the Philae lander after its eventful landing from the Rosetta spacecraft on comet 67P/Churyumov-Gerasimenko on 12 November 2014. Over 1000 OSIRIS level 3B images were analysed, which cover the August 2014 to September 2016 timeframe, with spatial resolution ranging from 7.6 m/px to approximately 0.06 m/px. We found that the Abydos site is as dark as the global 67P nucleus and spectrally red, with an average albedo of 6.5% at 649 nm and a spectral slope value of about 17%/(100 nm) at 50$^\\\\circ$ phase angle. Similar to the whole nucleus, the Abydos site also shows phase reddening but with lower coefficients than other regions of the comet which may imply a thinner cover of microscopically rough regolith compared to other areas. Seasonal variations, as already noticed for the whole nucleus, were also observed. We identified some potential morphological changes near the landing site implying a total mass loss of 4.7-7.0$\\\\times$10$^5$ kg. Small spots ranging from 0.1 m$^2$ to 27 m$^2$ were observed close to Abydos before and after perihelion. Their estimated water ice abundance reaches 30-40% locally, indicating fresh exposures of volatiles. Their lifetime ranges from a few hours up to three months for two pre-perihelion spots. The Abydos surroundings showed low level of cometary activity compared to other regions of the nucleus. Only a few jets are reported originating nearby Abydos, including a bright outburst that lasted for about one hour.',\n",
       "  'len': 1522},\n",
       " {'abstract': 'Spacecraft collision avoidance procedures have become an essential part of satellite operations. Complex and constantly updated estimates of the collision risk between orbiting objects inform the various operators who can then plan risk mitigation measures. Such measures could be aided by the development of suitable machine learning models predicting, for example, the evolution of the collision risk in time. In an attempt to study this opportunity, the European Space Agency released, in October 2019, a large curated dataset containing information about close approach events, in the form of Conjunction Data Messages (CDMs), collected from 2015 to 2019. This dataset was used in the Spacecraft Collision Avoidance Challenge, a machine learning competition where participants had to build models to predict the final collision risk between orbiting objects. This paper describes the design and results of the competition and discusses the challenges and lessons learned when applying machine learning methods to this problem domain.',\n",
       "  'len': 1048},\n",
       " {'abstract': \"Orbital geophysical investigations of Enceladus are critical to understanding its energy balance. We identified key science questions for the geophysical exploration of Enceladus, answering which would support future assessment of Enceladus' astrobiological potential. Using a Bayesian framework, we explored how science requirements map to measurement requirements. We performed mission simulations to study the sensitivity of a single spacecraft and dual spacecraft configurations to static gravity and tidal Love numbers of Enceladus. We find that mapping Enceladus' gravity field, improving the accuracy of the physical libration amplitude, and measuring Enceladus' tidal response would provide critical constraints on the internal structure, and establish a framework for assessing Enceladus' long-term habitability. This kind of investigation could be carried out as part of a life search mission at little additional resource requirements.\",\n",
       "  'len': 957},\n",
       " {'abstract': 'We present the analysis of narrowband whistler wave signatures observed in the inner heliosphere (0.3-1 au). These signatures are bumps in the spectral density in the 10-200 Hz frequency range of the AC magnetic field as measured by the search coil magnetometer on board the Helios1 spacecraft. We show that the majority of whistler signatures are observed in the slow solar wind (<500 km/s) and their occurrence increases with the radial distance (R), from ~3% at 0.3 au to ~10% at 0.9 au. In the fast solar wind (>600 km/s), whistler activity is significantly lower; whistler signatures start to appear for R > 0.6 au and their number increases from ~0.03% at 0.65 au to ~1% at 0.9 au. We have studied the variation of the electron core and halo anisotropy, as well as the electron normalized heat flux as a function of R and of the solar wind speed. We find that, in the slow wind electron core and halo anisotropy is higher than in fast one, and also that these anisotropies increase radially in both types of winds, which is in line with the occurrence of whistler signatures. We hypothesize the existence of a feedback mechanism to explain the observed radial variations of the occurrence of whistlers in relation with the halo anisotropy.',\n",
       "  'len': 1256},\n",
       " {'abstract': 'A dusty ringlet designated R/2006 S3, also known as the \"Charming Ringlet\", is located around 119,940 km from the center of Saturn within the Laplace Gap in the Cassini Division. Prior to 2010, the ringlet had a simple radial profile and a predictable eccentric shape with two components, one forced by solar radiation pressure and the other freely precessing around the planet. However, observations made by the Cassini spacecraft since late 2012 revealed a shelf of material extending inwards from the ringlet that was not present in the earlier observations. Closer inspection of images obtained after 2012 shows that sometime between 2010 and 2012 the freely-precessing component of the ringlet\\'s eccentricity increased by over 50%, and that for at least 3 years after 2012 the ringlet had longitudinal brightness variations that rotated around the planet at a range of rates corresponding to roughly 60 km in orbital semi-major axis. Some event therefore disturbed this ringlet between 2010 and late 2012.',\n",
       "  'len': 1021},\n",
       " {'abstract': 'For the past 30+ years, the magnetic expansion factor ($f_s$) has been used in empirical relationships to predict solar wind speed ($v_{obs}$) at 1 AU, based on an inverse relationship between these two quantities. Coronal unipolar streamers (i.e. pseudostreamers) undergo limited field line expansion, resulting in $f_s$-dependent relationships to predict fast wind associated with these structures. However, case studies have shown that in situ observed pseudostreamer solar wind was much slower than that derived with $f_s$. To investigate this further, we conduct a statistical analysis to determine if $f_s$ and $v_{obs}$ are inversely correlated for a large sample of periods when pseudostreamer wind was observed at multiple 1 AU spacecraft (i.e. ACE, STEREO-A/B). We use the Wang-Sheeley-Arge (WSA) model driven by Air Force Data Assimilative Photospheric Flux Transport (ADAPT) photospheric field maps to identify 38 periods when spacecraft observe pseudostreamer wind. We compare the expansion factor of the last open field lines on either side of a pseudostreamer cusp with the corresponding in situ measured solar wind speed. We find only slow wind ($v_{obs}$ $<$ 500 km/s) is associated with pseudostreamers, and that there is not a significant correlation between $f_s$ and $v_{obs}$ for these field lines. This suggests that field lines near the open-closed boundary of pseudostreamers are not subject to the steady-state acceleration along continuously open flux tubes assumed in the $f_s$-$v_{obs}$ relationship. In general, dynamics at the boundary between open and closed field lines such as interchange reconnection will invalidate the steady-state assumptions of this relationship.',\n",
       "  'len': 1713},\n",
       " {'abstract': 'The NASA Double Asteroid Redirection Test (DART) spacecraft will impact the secondary member of the [65803] Didymos binary in order to perform the first demonstration of asteroid deflection by kinetic impact. Determination of the momentum transfer to the target body from the kinetic impact is a primary planetary defense objective, using ground-based telescopic observations of the orbital period change of Didymos and imaging of the DART impact ejecta plume by the LICIACube cubesat, along with modeling and simulation of the DART impact. LICIACube, contributed by the Italian Space Agency, will perform a flyby of Didymos a few minutes after the DART impact, to resolve the ejecta plume spatial structure and to study the temporal evolution. LICIACube ejecta plume images will help determine the vector momentum transfer from the DART impact, by determining or constraining the direction and the magnitude of the momentum carried by ejecta. A model is developed for the impact ejecta plume optical depth, using a point source scaling model of the DART impact. The model is applied to expected LICIACube plume images and shows how plume images enable characterization of the ejecta mass versus velocity distribution. The ejecta plume structure, as it evolves over time, is determined by the amount of ejecta that has reached a given altitude at a given time. The evolution of the plume optical depth profiles determined from LICIACube images can distinguish between strength-controlled and gravity-controlled impacts, by distinguishing the respective mass versus velocity distributions. LICIACube plume images discriminate the differences in plume structure and evolution that result from different target physical properties, mainly strength and porosity, thereby allowing inference of these properties to improve the determination of momentum transfer.',\n",
       "  'len': 1867},\n",
       " {'abstract': 'Ground-based and spacecraft telescopic observations, combined with an intensive modeling effort, have greatly enhanced our understanding of hot giant planets and brown dwarfs over the past ten years. Although these objects are all fluid, hydrogen worlds with stratified atmospheres overlying convective interiors, they exhibit an impressive diversity of atmospheric behavior. Hot Jupiters are strongly irradiated, and a wealth of observations constrain the day-night temperature differences, circulation, and cloudiness. The intense stellar irradiation, presumed tidal locking and modest rotation leads to a novel regime of strong day-night radiative forcing. Circulation models predict large day-night temperature differences, global-scale eddies, patchy clouds, and, in most cases, a fast eastward jet at the equator-equatorial superrotation. The warm Jupiters may exhibit a wide range of rotation rates, obliquities, and orbital eccentricities, which, along with the weaker irradiation, leads to circulation patterns and observable signatures predicted to differ substantially from hot Jupiters. Brown dwarfs are typically isolated, rapidly rotating worlds; they radiate enormous energy fluxes into space and convect vigorously in their interiors. Their atmospheres exhibit patchiness in clouds and temperature on regional to global scales-the result of modulation by large-scale atmospheric circulation. Despite the lack of irradiation, such circulations can be driven by interaction of the interior convection with the overlying atmosphere, as well as self-organization of patchiness due to cloud-dynamical-radiative feedbacks. Finally, irradiated brown dwarfs help to bridge the gap between these classes of objects, experiencing intense external irradiation as well as vigorous interior convection. A hierarchy of modeling approaches have yielded major new insights into the dynamics governing these phenomena.',\n",
       "  'len': 1928},\n",
       " {'abstract': \"We review the importance of recent UV observations of solar system targets and discuss the need for further measurements, instrumentation and laboratory work in the coming decade. In the past decade, numerous important advances have been made in solar system science using ultraviolet (UV) spectroscopic techniques. Formerly used nearly exclusively for studies of giant planet atmospheres, planetary exospheres and cometary emissions, UV imaging spectroscopy has recently been more widely applied. The geyser-like plume at Saturn's moon Enceladus was discovered in part as a result of UV stellar occultation observations, and this technique was used to characterize the plume and jets during the entire Cassini mission. Evidence for a similar style of activity has been found at Jupiter's moon Europa using Hubble Space Telescope (HST) UV emission and absorption imaging. At other moons and small bodies throughout the solar system, UV spectroscopy has been utilized to search for activity, probe surface composition, and delineate space weathering effects; UV photometric studies have been used to uncover regolith structure. Insights from UV imaging spectroscopy of solar system surfaces have been gained largely in the last 1-2 decades, including studies of surface composition, space weathering effects (e.g. radiolytic products) and volatiles on asteroids (e.g. [2][39][48][76][84]), the Moon (e.g. [30][46][49]), comet nuclei (e.g. [85]) and icy satellites (e.g. [38][41-44][45][47][65]). The UV is sensitive to some species, minor contaminants and grain sizes often not detected in other spectral regimes. In the coming decade, HST observations will likely come to an end. New infrastructure to bolster future UV studies is critically needed. These needs include both developmental work to help improve future UV observations and laboratory work to help interpret spacecraft data. UV instrumentation will be a critical tool on missions to a variety of targets in the coming decade, especially for the rapidly expanding application of UV reflectance investigations of atmosphereless bodies.\",\n",
       "  'len': 2107},\n",
       " {'abstract': 'Motivated by the need to improve the ability to forecast whether a certain coronal mass ejection (CME) is to impact Earth, and by the insufficiency of statistical studies that analyze the whole erupting system with the focus on the governing conditions under CME deflections, we performed a careful analysis of 13 events along a one-year time interval showing large deflections from their source region. We used telescopes imaging the solar corona at different heights and wavelengths on board the Project for Onboard Autonomy 2 (PROBA2), Solar Dynamics Observatory (SDO), Solar TErrestrial RElations Observatory (STEREO), Solar and Heliospheric Observatory (SOHO) spacecraft and from National Solar Observatory (NSO). By taking advantage of the quadrature position of these spacecraft from October 2010 until September 2011, we inspected the 3D trajectory of CMEs and their associated prominences with respect to their solar sources by means of a tie-pointing tool and a forward model. Considering the coronal magnetic fields as computed from a potential field source surface model, we investigate the roles of magnetic energy distribution and kinematic features in the non-radial propagation of both structures. The magnetic environment present during the eruption is found to be crucial in determining the trajectory of CMEs, in agreement with previous reports.',\n",
       "  'len': 1375},\n",
       " {'abstract': \"Understanding extreme space weather events is of paramount importance in efforts to protect technological systems in space and on the ground. Particularly in the thermosphere, the subsequent extreme magnetic storms can pose serious threats to low-Earth orbit (LEO) spacecraft by intensifying errors in orbit predictions. Extreme magnetic storms (minimum Dst $\\\\leq$ --250 nT) are extremely rare: only 7 events occurred during the era of spacecraft with high-level accelerometers such as CHAMP (CHAllenge Mini-satellite Payload) and GRACE (Gravity Recovery And Climate experiment), and none with minimum Dst $\\\\leq$ --500 nT, here termed magnetic superstorms. Therefore, current knowledge of thermospheric mass density response to superstorms is very limited. Thus, in order to advance this knowledge, four known magnetic superstorms in history, i.e., events occurring before CHAMP's and GRACE's commission times, with complete datasets, are used to empirically estimate density enhancements and subsequent orbital drag. The November 2003 magnetic storm (minimum Dst = --422 nT), the most extreme event observed by both satellites, is used as the benchmark event. Results show that, as expected, orbital degradation is more severe for the most intense storms. Additionally, results clearly point out that the time duration of the storm is strongly associated with storm-time orbital drag effects, being as important as or even more important than storm intensity itself. The most extreme storm-time decays during CHAMP/GRACE-like sample satellite orbits estimated for the March 1989 magnetic superstorm show that long-lasting superstorms can have highly detrimental consequences for the orbital dynamics of satellites in LEO.\",\n",
       "  'len': 1733},\n",
       " {'abstract': \"The solar photon pressure provides a viable source of thrust for spacecraft in the solar system. Theoretically it could also enable interstellar missions, but an extremely small mass per cross section area is required to overcome the solar gravity. We identify aerographite, a synthetic carbon-based foam with a density of 0.18 kg/m^3 (15,000 times more lightweight than aluminum) as a versatile material for highly efficient propulsion with sunlight. A hollow aerographite sphere with a shell thickness eps_shl = 1 mm could go interstellar upon submission to the solar radiation in interplanetary space. Upon launch at 1 AU from the Sun, an aerographite shell with eps_shl = 0.5 mm arrives at the orbit of Mars in 60 d and at Pluto's orbit in 4.3 yr. Release of an aerographite hollow sphere, whose shell is 1 micrometer thick, at 0.04 AU (the closest approach of the Parker Solar Probe) results in an escape speed of nearly 6900 km/s and 185 yr of travel to the distance of our nearest star, Proxima Centauri. The infrared signature of a meter-sized aerographite sail could be observed with JWST up to 2 AU from the Sun, beyond the orbit of Mars. An aerographite hollow sphere with eps_shl = 100 micrometer and a radius of 1 m (5 m) weighs 230 mg (5.7 g) and has a 2.2 g (55 g) mass margin for interstellar escape. The payload margin is ten times the mass of the spacecraft, whereas the payload on chemical interstellar rockets is typically a thousandth of the weight of the rocket. Simplistic communication would enable studies of the interplanetary medium and a search for the suspected Planet Nine, and would serve as a precursor mission to alpha Centauri. We estimate prototype developments costs of 1 million USD, a price of 1000 USD per sail, and a total of <10 million USD including launch for a piggyback concept with an interplanetary mission.\",\n",
       "  'len': 1865},\n",
       " {'abstract': \"As it has already done for Earth, the sun, and the stars, seismology has the potential to radically change the way the interiors of giant planets are studied. In a sequence of events foreseen by only a few, observations of Saturn's rings by the Cassini spacecraft have rapidly broken ground on giant planet seismology. Gravity directly couples the planet's normal mode oscillations to the orbits of ring particles, generating spiral waves whose frequencies encode Saturn's internal structure and rotation. These modes have revealed a stably stratified region near Saturn's center, and provided a new constraint on Saturn's rotation.\",\n",
       "  'len': 643},\n",
       " {'abstract': \"We consider the history of New Horizons target (486958) Arrokoth in the context of its sublimative evolution. Shortly after the Sun's protoplanetary disk (PPD) cleared, the newly intense sunlight sparked a sublimative period in Arrokoth's early history that lasted for ~10-100 Myr. Although this sublimation was too weak to significantly alter Arrokoth's spin state, it could drive mass transport around the surface significant enough to erase topographic features on length scales of ~10-100 m. This includes craters up to ~50-500 m in diameter, which suggests that the majority of Arrokoth's craters may not be primordial (dating from the merger of Arrokoth's lobes), but rather could date from after the end of this sublimative period. Thereafter, Arrokoth entered a Quiescent Period (which lasts to the present day), in which volatile production rates are at least 13 orders of magnitude less than the ~10^24 molecules/s detection limit of the New Horizons spacecraft (Lisse et al. 2020). This is insufficient to drive either mass transport or sublimative torques. These results suggest that the observed surface of Arrokoth is not primordial, but rather dates from the Quiescent Period. By contrast, the inability of sublimative torques to meaningfully alter Arrokoth's rotation state suggests that its shape is indeed primordial, and its observed rotation is representative of its spin state after formation.\",\n",
       "  'len': 1425},\n",
       " {'abstract': 'The recent discovery of the first confirmed Interstellar Objects (ISOs) passing through the Solar System on clearly hyperbolic objects opens the potential for near term ISO missions, either to the two known objects, or to similar objects found in the future. Such ISOs are the only exobodies we have a chance of accessing directly in the near future. This White Paper focuses on the science possible from in situ spacecraft exploration of nearby ISOs. Such spacecraft missions are technically possible now and are suitable potential missions in the period covered by the 2023-2032 Decadal Survey. Spacecraft missions can determine the structure and the chemical and isotopic composition of ISO in a close flyby coupled with a small sub-probe impactor and either a mass spectrometer or a high resolution UV spectrometer; this technology will also be useful for fast missions to TransNeptune Objects (TNOs) and long period comets. ISO exploration holds the potential of providing considerable improvements in our knowledge of galactic evolution, of planetary formation, and of the cycling of astrobiologically important materials through the galaxy.',\n",
       "  'len': 1158},\n",
       " {'abstract': 'In this paper it is presented the concept and design of a new type of spacecraft that could be used to make the first manned interstellar travel. Solar one would integrate three near-term technologies, namely: compact nuclear fusion reactors, extremely large light sails, and high-energy laser arrays. A system of lenses or mirrors to propel the sail with sunlight is suggested as an alternative to laser propulsion. With a mile-long light sail, Solar One could reach an average of 22% the speed of light, arriving to the closest potentially habitable exoplanet in less than 19 years with the help of a Bussard scoop producing reverse electromagnetic propulsion. Key challenges are reducing the weight of continuous-wave lasers and compact fusion reactors as well as achieving cryo-sleep and artificial gravity.',\n",
       "  'len': 822},\n",
       " {'abstract': 'Selected by NASA for an Astrophysics Science SmallSat study, The Virtual Telescope for X-Ray Observations (VTXO) is a small satellite mission being developed by NASAs Goddard Space Flight Center (GSFC) and New Mexico State University (NMSU). VTXO will perform X-ray observations with an angular resolution around 50 milliarcseconds, an order of magnitude better than is achievable by current state of the art X-ray telescopes. VTXOs fine angular resolution enables measuring the environments closer to the central engines in compact X-ray sources. This resolution will be achieved by the use of Phased Fresnel Lenses (PFLs) optics which provide near diffraction-limited imaging in the X-ray band. However, PFLs require long focal lengths in order to realize their imaging performance, for VTXO this dictates that the telescopes optics and the camera will have a separation of 1 km. As it is not realistic to build a structure this large in space, the solution being adapted for VTXO is to place the camera, and the optics on two separate spacecraft and fly them in formation with the necessary spacing. This requires centimeter level control, and sub-millimeter level knowledge of the two spacecrafts relative transverse position. This paper will present VTXOs current baseline, with particular emphasis on the missions flight dynamics design.',\n",
       "  'len': 1354},\n",
       " {'abstract': 'The Virtual Telescope for X-Ray Observations (VTXO) is an Astrophysics SmallSat mission being developed to demonstrate 10-milliarcsecond X-ray imaging using a Phase Fresnel Lense (PFL) based space telescope. PFLs promise to provide several orders of magnitude improvement in angular resolution over current state of the art X-ray optics. However, PFLs for astronomical applications require a long focal length, for VTXO the focal length is estimated to be in the range of 0.5 km to 4 km. Since these focal lengths are not feasible on a single spacecraft, the proposed solution is to use two separate spacecraft, one with the lense(s), and the second with an X-ray camera. These two spacecrafts will then fly in a formation approximating a single rigid telescope. In order to achieve this configuration, the two spacecraft must maintain the formation a focal length distance apart, with centimeter level control, and sub-millimeter level knowledge requirements. Additionally, the system must keep the telescope axis pointed at a fixed target on the celestial sphere for extended durations. VTXOs system architecture calls for two CubeSats to operate in a highly eccentric Earth orbit with one of the spacecrafts traveling on a natural keplarian orbit. The second spacecraft will then fly on a pseudo orbit maintaining a fixed offset during observations. Observations with this system will occur near apogee where differential forces on the spacecrafts are minimal which in turn minimizes fuel consumption. This paper overviews VTXOs system architecture, and looks in depth at the formation flying techniques, including fuel consumption, and methods maintaining the formation. Beyond its use in X-ray astronomy, these formations flying techniques should eventually contribute to the development of distributed aperture telescopes, with imaging performance orders of magnitude better than the current state of the art.',\n",
       "  'len': 1926},\n",
       " {'abstract': 'This paper provides perspectives on recent progress in the understanding of the physics of devices where the external magnetic field is applied perpendicularly to the discharge current. This configuration generates a strong electric field, which acts to accelerates ions. The many applications of this set up include generation of thrust for spacecraft propulsion and the separation of species in plasma mass separation devices. These ExB plasmas are subject to plasma-wall interaction effects as well as various micro and macro instabilities, and in many devices, we observe the emergence of anomalous transport. This perspective presents the current understanding of the physics of these phenomena, state-of-the-art computational results, identifies critical questions, and suggests directions for future research',\n",
       "  'len': 826},\n",
       " {'abstract': \"Terrain relative navigation can improve the precision of a spacecraft's position estimate by detecting global features that act as supplementary measurements to correct for drift in the inertial navigation system. This paper presents a system that uses a convolutional neural network (CNN) and image processing methods to track the location of a simulated spacecraft with an extended Kalman filter (EKF). The CNN, called LunaNet, visually detects craters in the simulated camera frame and those detections are matched to known lunar craters in the region of the current estimated spacecraft position. These matched craters are treated as features that are tracked using the EKF. LunaNet enables more reliable position tracking over a simulated trajectory due to its greater robustness to changes in image brightness and more repeatable crater detections from frame to frame throughout a trajectory. LunaNet combined with an EKF produces a decrease of 60% in the average final position estimation error and a decrease of 25% in average final velocity estimation error compared to an EKF using an image processing-based crater detection method when tested on trajectories using images of standard brightness.\",\n",
       "  'len': 1217},\n",
       " {'abstract': 'Very low Earth orbits (VLEO), typically classified as orbits below approximately 450 km in altitude, have the potential to provide significant benefits to spacecraft over those that operate in higher altitude orbits. This paper provides a comprehensive review and analysis of these benefits to spacecraft operations in VLEO, with parametric investigation of those which apply specifically to Earth observation missions. The most significant benefit for optical imaging systems is that a reduction in orbital altitude improves spatial resolution for a similar payload specification. Alternatively mass and volume savings can be made whilst maintaining a given performance. Similarly, for radar and lidar systems, the signal-to-noise ratio can be improved. Additional benefits include improved geospatial position accuracy, improvements in communications link-budgets, and greater launch vehicle insertion capability. The collision risk with orbital debris and radiation environment can be shown to be improved in lower altitude orbits, whilst compliance with IADC guidelines for spacecraft post-mission lifetime and deorbit is also assisted. Finally, VLEO offers opportunities to exploit novel atmosphere-breathing electric propulsion systems and aerodynamic attitude and orbit control methods. However, key challenges associated with our understanding of the lower thermosphere, aerodynamic drag, the requirement to provide a meaningful orbital lifetime whilst minimising spacecraft mass and complexity, and atomic oxygen erosion still require further research. Given the scope for significant commercial, societal, and environmental impact which can be realised with higher performing Earth observation platforms, renewed research efforts to address the challenges associated with VLEO operations are required.',\n",
       "  'len': 1822},\n",
       " {'abstract': 'Compressive plasma turbulence is investigated at sub-ion scales in the solar wind using both the Fast Plasma Investigation (FPI) instrument on the Magnetospheric MultiScale mission (MMS), as well as using calibrated spacecraft potential data from the Spin Plane Double Probe (SDP) instrument. The data from FPI allow a measurement down to the sub-ion scale region ($f_{sc}\\\\gtrsim 1$ Hz) to be investigated before the instrumental noise becomes significant at a spacecraft frame frequency of $f_{sc}\\\\approx 3$Hz, whereas calibrated spacecraft potential allows a measurement up to $f_{sc}\\\\approx 40$Hz. In this work, we give a detailed description of density estimation in the solar wind using the spacecraft potential measurement from the SDP instrument on MMS. Several intervals of solar wind plasma have been processed using the methodology described which are made available. One of the intervals is investigated in more detail and the power spectral density of the compressive fluctuations is measured from the inertial range to the sub-ion range. The morphology of the density spectra can be explained by either a cascade of Alfvén waves and slow waves at large scales and kinetic Alfvén waves at sub-ion scales, or more generally by the Hall effect. Using electric field measurements the two hypotheses are discussed.',\n",
       "  'len': 1333},\n",
       " {'abstract': \"The five classical Uranian moons are possible ocean worlds that exhibit bizarre geologic landforms, hinting at recent surface-interior communication. However, Uranus' classical moons, as well as its ring moons and irregular satellites, remain poorly understood. We assert that a Flagship-class orbiter is needed to explore the Uranian satellites.\",\n",
       "  'len': 357},\n",
       " {'abstract': 'Challenging space missions include those at very low altitudes, where the atmosphere is source of aerodynamic drag on the spacecraft. To extend such missions lifetime, an efficient propulsion system is required. One solution is Atmosphere-Breathing Electric Propulsion (ABEP). It collects atmospheric particles to be used as propellant for an electric thruster. The system would minimize the requirement of limited propellant availability and can also be applied to any planet with atmosphere, enabling new mission at low altitude ranges for longer times. Challenging is also the presence of reactive chemical species, such as atomic oxygen in Earth orbit. Such species cause erosion of (not only) propulsion system components, i.e. acceleration grids, electrodes, and discharge channels of conventional EP systems. IRS is developing within the DISCOVERER project, an intake and a thruster for an ABEP system. The paper describes the design and implementation of the RF helicon-based inductive plasma thruster (IPT). This paper deals in particular with the design and implementation of a novel antenna called the birdcage antenna, a device well known in magnetic resonance imaging (MRI), and also lately employed for helicon-wave based plasma sources in fusion research. The IPT is based on RF electrodeless operation aided by an externally applied static magnetic field. The IPT is composed by an antenna, a discharge channel, a movable injector, and a solenoid. By changing the operational parameters along with the novel antenna design, the aim is to minimize losses in the RF circuit, and accelerate a quasi-neutral plasma plume. This is also to be aided by the formation of helicon waves within the plasma that are to improve the overall efficiency and achieve higher exhaust velocities. Finally, the designed IPT with a particular focus on the birdcage antenna design procedure is presented',\n",
       "  'len': 1907},\n",
       " {'abstract': 'The Rosetta spacecraft detected transient and sporadic diamagnetic regions around comet 67P/Churyumov-Gerasimenko. In this paper we present a statistical analysis of bulk and suprathermal electron dynamics, as well as a case study of suprathermal electron pitch angle distributions (PADs) near a diamagnetic region. Bulk electron densities are correlated with the local neutral density and we find a distinct enhancement in electron densities measured over the southern latitudes of the comet. Flux of suprathermal electrons with energies between tens of eV to a couple of hundred eV decreases each time the spacecraft enters a diamagnetic region. We propose a mechanism in which this reduction can be explained by solar wind electrons that are tied to the magnetic field and after having been transported adiabatically in a decaying magnetic field environment, have limited access to the diamagnetic regions. Our analysis shows that suprathermal electron PADs evolve from an almost isotropic outside the diamagnetic cavity to a field-aligned distribution near the boundary. Electron transport becomes chaotic and non-adiabatic when electron gyroradius becomes comparable to the size of the magnetic field line curvature, which determines the upper energy limit of the flux variation. This study is based on Rosetta observations at around 200 km cometocentric distance when the comet was at 1.24 AU from the Sun and during the southern summer cometary season.',\n",
       "  'len': 1470},\n",
       " {'abstract': 'We have compiled a catalog of solar flares as observed by the Extreme ultraviolet Imaging Telescope (EIT) aboard the Solar and Heliospheric Observatory (SOHO) spacecraft and the GOES spacecraft over a span from 1997 to 2010. During mid-1998, the cadence of EIT images was revised from two images per day to 12 minutes. However, the low temporal resolution causes significant data gaps in capturing much of the flaring phenomenon. Therefore, we monitor possible errors in flare detection by flare parameters such as temporal overlap, observational wavelength, and considering full field of view (FOV) images. We consider the GOES flare catalog as the primary source. We describe the technique used to enhance the GOES detected flares using the Extreme Ultraviolet (EUV) image captured by the EIT instrument. In order to detect brightenings, we subtract the images with a maximum cadence of 25 minutes. We have downloaded and analyzed the EIT data via the Virtual Solar Observatory (VSO). This flare dataset from the SOHO/EIT period proves indispensable to the process of the solar flare predictions as the instrument has covered most of Solar Cycle 23.',\n",
       "  'len': 1162},\n",
       " {'abstract': 'Solar flares are known to generate seismic waves in the Sun. We present a detailed analysis of seismic emission in sunspots accompanying M- and X-class solar flares. For this purpose, we have used high-resolution Dopplergrams and line-of-sight magnetograms at a cadence of 45 s, along with vector magnetograms at a cadence of 135 s obtained from Helioseismic and Magnetic Imager (HMI) instrument aboard the Solar Dynamic Observatory (SDO) space mission. In order to identify the location of flare ribbons and hard X-ray foot-points, we have also used H-alpha chromospheric intensity observations obtained from Global Oscillation Network Group (GONG) instruments and hard X-ray images in 12-25 KeV band from the Reuvan Ramaty High Energy Solar Spectroscopic Imager (RHESSI) spacecraft. The Fast Fourier Transform (FFT) technique is applied to construct the acoustic velocity power map in 2.5-4 mHz band for pre-flare, spanning flare, and post flare epochs for the identification of seismic emission locations in the sunspots. In the power maps, we have selected only those locations which are away from the flare ribbons and hard X-rays foot-points. These regions are believed to be free from any flare related artefacts in the observational data. We have identified concentrated locations of acoustic power enhancements in sunspots accompanying major flares. Our investigation provides evidence that abrupt changes in the magnetic fields and associated impulsive changes in the Lorentz force could be the driving source for these seismic emissions in the sunspots during solar flares.',\n",
       "  'len': 1595},\n",
       " {'abstract': 'We calculate the surface temperature and the resulting brightness of sub-relativistic objects moving through the Solar system due to collisional heating by gas and radiative heating by solar radiation. The thermal emission from objects of size $\\\\gtrsim 100$ m and speed of $\\\\gtrsim 0.1c$, can be detected by the upcoming {\\\\it James Webb Space Telescope} out to a distance of $\\\\sim 100$ au. Future surveys could therefore set interesting limits on the abundance of fast-moving interstellar objects or spacecraft.',\n",
       "  'len': 522},\n",
       " {'abstract': 'Small-scale magnetic flux ropes (SFRs) are a type of structures in the solar wind that possess helical magnetic field lines. In a recent report (Chen & Hu 2020), we presented the radial variations of the properties of SFR from 0.29 to 8 au using in situ measurements from the Helios, ACE/Wind, Ulysses, and Voyager spacecraft. With the launch of the Parker Solar Probe (PSP), we extend our previous investigation further into the inner heliosphere. We apply a Grad-Shafranov-based algorithm to identify SFRs during the first two PSP encounters. We find that the number of SFRs detected near the Sun is much less than that at larger radial distances, where magnetohydrodynamic (MHD) turbulence may act as the local source to produce these structures. The prevalence of Alfvenic structures significantly suppresses the detection of SFRs at closer distances. We compare the SFR event list with other event identification methods, yielding a dozen well-matched events. The cross-section maps of two selected events confirm the cylindrical magnetic flux rope configuration. The power-law relation between the SFR magnetic field and heliocentric distances seems to hold down to 0.16 au.',\n",
       "  'len': 1191},\n",
       " {'abstract': \"In studies of the oldest solar system bodies - comets and asteroids - it is their fragments - meteoroids - that provide the most accessible planetary material for detailed laboratory analysis in the form of dust particles or meteorites. Some asteroids and comets were visited by spacecrafts and returned interplanetary samples to Earth, while missions Hayabusa 2 and OSIRIX-REx visiting asteroids Ryugu and Bennu are ongoing. However, the lack of representative samples of comets and asteroids opens the space to gain more knowledge from direct observations of meteoroids. At collision with the Earth's atmosphere, meteoroids produce light phenomena known as meteors. Different methods can be used to observe meteors, allowing us to study small interplanetary fragments, which would otherwise remain undetected. Numerous impressive meteor showers, storms and meteorite impacts have occurred throughout the recorded history and can now be predicted and analyzed in much more detail. By understanding the dynamics, composition and physical properties of meteoroids, we are able to study the formation history and dynamical evolution of the solar system. This work presents an introduction to meteor astronomy, its fundamental processes and examples of current research topics.\",\n",
       "  'len': 1285},\n",
       " {'abstract': 'This paper addresses the global exponential attitude tracking of a spacecraft when gyro measurements are corrupted by bias. Based on contraction analysis, an exponentially convergent nonlinear observer is designed first to estimate the gyro bias. Relying on this bias estimator and the quaternion logarithm representation of the tracking error, an exponentially globally convergent controller is devised. This controller stabilizes the unique equilibrium of the closed-loop system, where the tracking error is the unit quaternion. For more energy-efficiency and enhancing the robustness in the presence of measurement noise, a hysteretically switching variable as in [1] is incorporated in the control loop and an unwinding-free globally exponentially convergent tracking controller is obtained. Numeric simulations were done to evaluate its performance in terms of tracking errors and energy-efficiency, as well as the robustness to measurement noise and time-varying bias in gyro sensors.',\n",
       "  'len': 1001},\n",
       " {'abstract': 'The Mercury Orbiter radio Science Experiment (MORE) is one of the experiments on-board the ESA/JAXA BepiColombo mission to Mercury, to be launched in October 2018. Thanks to full on-board and on-ground instrumentation performing very precise tracking from the Earth, MORE will have the chance to determine with very high accuracy the Mercury-centric orbit of the spacecraft and the heliocentric orbit of Mercury. This will allow to undertake an accurate test of relativistic theories of gravitation (relativity experiment), which consists in improving the knowledge of some post-Newtonian and related parameters, whose value is predicted by General Relativity. This paper focuses on two critical aspects of the BepiColombo relativity experiment. First of all, we address the delicate issue of determining the orbits of Mercury and the Earth-Moon barycenter at the level of accuracy required by the purposes of the experiment and we discuss a strategy to cure the rank deficiencies that appear in the problem. Secondly, we introduce and discuss the role of the solar Lense-Thirring effect in the Mercury orbit determination problem and in the relativistic parameters estimation.',\n",
       "  'len': 1188},\n",
       " {'abstract': \"The large field-of-view of the Sun Watcher using Active Pixel System detector and Image Processing (SWAP) instrument on board the PRoject for Onboard Autonomy 2 (PROBA2) spacecraft provides a unique opportunity to study extended coronal structures observed in EUV in conjunction with global coronal magnetic field simulations. A global non-potential magnetic field model is used to simulate the evolution of the global corona from 1 September 2014 to 31 March 2015, driven by newly emerging bipolar active regions determined from Helioseismic and Magnetic Imager (HMI) magnetograms. We compare the large-scale structure of the simulated magnetic field with structures seen off-limb in SWAP EUV observations. In particular, we investigate how successful the model is in reproducing regions of closed and open structures; the scale of structures; and compare the evolution of a coronal fan observed over several rotations. The model is found to accurately reproduce observed large-scale off-limb structures. When discrepancies do arise they mainly occur off the east solar limb due to active regions emerging on the far side of the Sun, which cannot be incorporated into the model until they are observed on the Earth-facing side. When such ``late'' active region emergences are incorporated into the model, we find that the simulated corona self-corrects within a few days, so that simulated structures off the west limb more closely match what is observed. Where the model is less successful, we consider how this may be addressed, through model developments or additional observational products.\",\n",
       "  'len': 1607},\n",
       " {'abstract': 'The radial expansion of coronal mass ejections (CMEs) is known to occur from remote observations; from the variation of their properties with radial distance; and from local in situ plasma measurements showing a decreasing speed profile throughout the magnetic ejecta (ME). However, little is known on how local measurements compare to global measurements of expansion. Here, we present results from the analysis of 42 CMEs measured in the inner heliosphere by two spacecraft in radial conjunction. The magnetic field decrease with distance provides a measure of their global expansion. Near 1 au, the decrease in their bulk speed provides a measure of their local expansion. We find that these two measures have little relation with each other. We also investigate the relation between characteristics of CME expansion and CME properties. We find that the expansion depends on the initial magnetic field strength inside the ME, but not significantly on the magnetic field inside the ME measured near 1 au. This is an indirect evidence that CME expansion in the innermost heliosphere is driven by the high magnetic pressure inside the ME, while by the time the MEs reach 1 au, they are expanding due to the decrease in the solar wind dynamic pressure with distance. We also determine the evolution of the ME tangential and normal magnetic field components with distance, revealing significant deviations as compared to the expectations from force-free field configurations as well as some evidence that the front half of MEs expand at a faster rate than the back half.',\n",
       "  'len': 1579},\n",
       " {'abstract': 'Since Coronal Mass Ejections (CMEs) are the major drivers of space weather, it is crucial to study their evolution starting from the inner corona. In this work we use Graduated Cylindrical Shell (GCS) model to study the 3D evolution of 59 CMEs in the inner ($<$ 3R$_{\\\\odot}$) and outer ($>$ 3R$_{\\\\odot}$) corona using observations from COR-1 and COR-2 on-board Solar TErrestrial RElations Observatory (STEREO) spacecraft. We identify the source regions of these CMEs and classify them as CMEs associated with Active Regions (ARs), Active Prominences (APs), and Prominence Eruptions (PEs). We find 27 $\\\\%$ of CMEs show true expansion and 31 $\\\\%$ show true deflections as they propagate outwards. Using 3D kinematic profiles of CMEs, we connect the evolution of true acceleration with the evolution of true width in the inner and outer corona. Thereby providing the observational evidence for the influence of the Lorentz force on the kinematics to lie in the height range of $2.5-3$ R$_{\\\\odot}$. We find a broad range in the distribution of peak 3D speeds and accelerations ranging from 396 to 2465 km~s$^{-1}$ and 176 to 10922 m~s$^{-2}$ respectively with a long tail towards high values coming mainly from CMEs originating from ARs or APs. Further, we find the magnitude of true acceleration is be inversely correlated to its duration with a power law index of -1.19. We believe that these results will provide important inputs for the planning of upcoming space missions which will observe the inner corona and the models that study CME initiation and propagation.',\n",
       "  'len': 1577},\n",
       " {'abstract': \"We present an analysis of coronal mass ejections (CMEs) observed by the Heliospheric Imagers (HIs) on board NASA's Solar Terrestrial Relations Observatory (STEREO) spacecraft. Between August 2008 and April 2014 we identify 273 CMEs that are observed simultaneously, by the HIs on both spacecraft. For each CME, we track the observed leading edge, as a function of time, from both vantage points, and apply the Stereoscopic Self-Similar Expansion (SSSE) technique to infer their propagation throughout the inner heliosphere. The technique is unable to accurately locate CMEs when their observed leading edge passes between the spacecraft, however, we are able to successfully apply the technique to 151, most of which occur once the spacecraft separation angle exceeds 180 degrees, during solar maximum. We find that using a small half-width to fit the CME can result in observed acceleration to unphysically high velocities and that using a larger half-width can fail to accurately locate the CMEs close to the Sun because the method does not account for CME over-expansion in this region. Observed velocities from SSSE are found to agree well with single-spacecraft (SSEF) analysis techniques applied to the same events. CME propagation directions derived from SSSE and SSEF analysis agree poorly because of known limitations present in the latter. This work was carried out as part of the EU FP7 HELCATS (Heliospheric Cataloguing, Analysis and Techniques Service) project (this http URL).\",\n",
       "  'len': 1501},\n",
       " {'abstract': 'Deci-hertz Interferometer Gravitational Wave Observatory (DECIGO) is the future Japanese space mission with a frequency band of 0.1 Hz to 10 Hz. DECIGO aims at the detection of primordial gravitational waves, which could be produced during the inflationary period right after the birth of the universe. There are many other scientific objectives of DECIGO, including the direct measurement of the acceleration of the expansion of the universe, and reliable and accurate predictions of the timing and locations of neutron star/black hole binary coalescences. DECIGO consists of four clusters of observatories placed in the heliocentric orbit. Each cluster consists of three spacecraft, which form three Fabry-Perot Michelson interferometers with an arm length of 1,000 km. Three clusters of DECIGO will be placed far from each other, and the fourth cluster will be placed in the same position as one of the three clusters to obtain the correlation signals for the detection of the primordial gravitational waves. We plan to launch B-DECIGO, which is a scientific pathfinder of DECIGO, before DECIGO in the 2030s to demonstrate the technologies required for DECIGO, as well as to obtain fruitful scientific results to further expand the multi-messenger astronomy.',\n",
       "  'len': 1272},\n",
       " {'abstract': 'We calculate the momentum flux and pressure of ions measured by the Ion Composition Analyzer (ICA) on the Rosetta mission at comet 67P/Churyumov-Gerasimenko. The total momentum flux stays roughly constant over the mission, but the contributions of different ion populations change depending on heliocentric distance. The magnetic pressure, calculated from Rosetta magnetometer measurements, roughly corresponds with the cometary ion momentum flux. When the spacecraft enters the solar wind ion cavity, the solar wind fluxes drop drastically, while the cometary momentum flux becomes roughly ten times the solar wind fluxes outside of the ion cavity, indicating that pickup ions behave similarly to the solar wind ions in this region. We use electron density from the Langmuir probe to calculate the electron pressure, which is particularly important close to the comet nucleus where flow changes from antisunward to radially outward.',\n",
       "  'len': 944},\n",
       " {'abstract': 'The development and deployment of machine learning systems can be executed easily with modern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and expensive consequences. Engineering systems, on the other hand, follow well-defined processes and testing standards to streamline development for high-quality, reliable results. The extreme is spacecraft systems, where mission critical measures and robustness are ingrained in the development process. Drawing on experience in both spacecraft engineering and AI/ML (from research through product), we propose a proven systems engineering approach for machine learning development and deployment. Our Technology Readiness Levels for ML (TRL4ML) framework defines a principled process to ensure robust systems while being streamlined for ML research and product, including key distinctions from traditional software engineering. Even more, TRL4ML defines a common language for people across the organization to work collaboratively on ML technologies.',\n",
       "  'len': 1138},\n",
       " {'abstract': 'Using the novel Magnetospheric Multiscale (MMS) mission data accumulated during the 2019 MMS Solar Wind Turbulence Campaign, we calculate the Taylor microscale $(\\\\lambda_{\\\\mathrm{T}})$ of the turbulent magnetic field in the solar wind. The Taylor microscale represents the onset of dissipative processes in classical turbulence theory. An accurate estimation of Taylor scale from spacecraft data is, however, usually difficult due to low time cadence, the effect of time decorrelation, and other factors. Previous reports were based either entirely on the Taylor frozen-in approximation, which conflates time dependence, or that were obtained using multiple datasets, which introduces sample-to-sample variation of plasma parameters, or where inter-spacecraft distance were larger than the present study. The unique configuration of linear formation with logarithmic spacing of the 4 MMS spacecraft, during the campaign, enables a direct evaluation of the $\\\\lambda_{\\\\mathrm{T}}$ from a single dataset, independent of the Taylor frozen-in approximation. A value of $\\\\lambda_{\\\\mathrm{T}} \\\\approx 7000 \\\\, \\\\mathrm{km}$ is obtained, which is about 3 times larger than the previous estimates.',\n",
       "  'len': 1197},\n",
       " {'abstract': \"Comparatively little is known about atmospheric chemistry on Uranus and Neptune, because remote spectral observations of these cold, distant ``Ice Giants'' are challenging, and each planet has only been visited by a single spacecraft during brief flybys in the 1980s. Thermochemical equilibrium is expected to control the composition in the deeper, hotter regions of the atmosphere on both planets, but disequilibrium chemical processes such as transport-induced quenching and photochemistry alter the composition in the upper atmospheric regions that can be probed remotely. Surprising disparities in the abundance of disequilibrium chemical products between the two planets point to significant differences in atmospheric transport. The atmospheric composition of Uranus and Neptune can provide critical clues for unravelling details of planet formation and evolution, but only if it is fully understood how and why atmospheric constituents vary in a three-dimensional sense and how material coming in from outside the planet affects observed abundances. Future mission planning should take into account the key outstanding questions that remain unanswered about atmospheric chemistry on Uranus and Neptune, particularly those questions that pertain to planet formation and evolution, and those that address the complex, coupled atmospheric processes that operate on Ice Giants within our solar system and beyond.\",\n",
       "  'len': 1426},\n",
       " {'abstract': 'To investigate the power and spectral index anisotropy in the inertial range of solar wind turbulence, we use 70 intervals of electric field data accumulated by Cluster spacecraft in the free solar wind. We compute the electric field fluctuation power spectra using wavelet analysis technique and study its spectral index variation with the change in angle between the heliocentric radial direction and the local mean magnetic field. We find clear power and spectral index anisotropy in the frequency ranging from 0.01 Hz to 0.1 Hz, with more power in parallel fluctuations than perpendicular. We also report our study of anisotropy as a function of solar activity.',\n",
       "  'len': 676},\n",
       " {'abstract': 'On 2012 August 2, two CMEs (CME-1 and CME-2) erupted from the west limb of the Sun as viewed from Earth, and were observed in images from the white light coronagraphs on the SOHO and STEREO spacecraft. These events were also observed by the Very Large Array (VLA), which was monitoring the Sun at radio wavelengths, allowing time-dependent Faraday rotation observations to be made of both events. We use the white-light imaging and radio data to model the 3-D field geometry of both CMEs, assuming a magnetic flux rope geometry. For CME-2, we also consider 1 au in situ field measurements in the analysis, as this CME hits STEREO-A on August~6, making this the first CME with observational constraints from stereoscopic coronal imaging, radio Faraday rotation, and in situ plasma measurements combined. The imaging and in situ observations of CME-2 provide two clear predictions for the radio data; namely that VLA should observe positive rotation measures (RMs) when the radio line of sight first encounters the CME, and that the sign should reverse to negative within a couple hours. The initial positive RMs are in fact observed. The expected sign reversal is not, but the VLA data unfortunately end too soon to be sure of the significance of this discrepancy. We interpret an RM increase prior to the expected occultation time of the CME as a signature of a sheath region of deflected field ahead of the CME itself.',\n",
       "  'len': 1430},\n",
       " {'abstract': 'We report on the local structure of the Martian subsolar Magnetic Pileup Boundary (MPB) from minimum variance analysis of the magnetic field measured by the Mars Atmosphere and Volatile EvolutioN (MAVEN) spacecraft for six orbits. In particular, we detect a well defined current layer within the MPB and provide a local estimate of its current density which results in a sunward Hall electric force. This force accounts for the deflection of the solar wind ions and the acceleration of electrons which carry the interplanetary magnetic field through the MPB into the Magnetic Pileup Region. We find that the thickness of the MPB current layer is of the order of both the upstream (magnetosheath) solar wind proton inertial length and convective gyroradius. This study provides a high resolution view of one of the components of the current system around Mars reported in recent works.',\n",
       "  'len': 895},\n",
       " {'abstract': 'Both kinetic instabilities and strong turbulence have potential to impact the behavior of space plasmas. To assess effects of these two processes we compare results from a 3 dimensional particle-in-cell (PIC) simulation of collisionless plasma turbulence against observations by the MMS spacecraft in the terrestrial magnetosheath and by the Wind spacecraft in the solar wind. The simulation develops coherent structures and anisotropic ion velocity distributions that can drive micro-instabilities. Temperature-anisotropy driven instability growth rates are compared with inverse nonlinear turbulence time scales. Large growth rates occur near coherent structures; nevertheless linear growth rates are, on average, substantially less than the corresponding nonlinear rates. This result casts some doubt on the usual basis for employing linear instability theory, and raises questions as to why the linear theory appears to work in limiting plasma excursions in anisotropy and plasma beta.',\n",
       "  'len': 1000},\n",
       " {'abstract': \"We measured the optical phase curve of the transiting brown dwarf KELT-1b (TOI 1476, Siverd et al. 2012) using data from the TESS spacecraft. We found that KELT-1b shows significant phase variation in the TESS bandpass, with a relatively large phase amplitude of $234^{+43}_{-44}$ ppm and a secondary eclipse depth of $371^{+47}_{-49}$ ppm. We also measured a marginal eastward offset in the dayside hotspot of $18.3^\\\\circ\\\\pm7.4^\\\\circ$ relative to the substellar point. We detected a strong phase curve signal attributed to ellipsoidal distortion of the host star, with an amplitude of $399\\\\pm19$ ppm. Our results are roughly consistent with the Spitzer phase curves of KELT-1b (Beatty et al. 2019), but the TESS eclipse depth is deeper than expected. Our cloud-free 1D models of KELT-1b's dayside emission are unable to fit the full combined eclipse spectrum. Instead, the large TESS eclipse depth suggests that KELT-1b may have a significant dayside geometric albedo of $\\\\mathrm{A}_\\\\mathrm{g}\\\\sim0.5$ in the TESS bandpass, which would agree with the tentative trend between equilibrium temperature and geometric albedo recently suggested by Wong et al. 2020. We posit that if KELT-1b has a high dayside albedo, it is likely due to silicate clouds (Gao et al. 2020) that form on KELT-1b's nightside (Beatty et al. 2019, Keating et al. 2019) and are subsequently transported onto the western side of KELT-1b's dayside hemisphere before breaking up.\",\n",
       "  'len': 1459},\n",
       " {'abstract': \"We establish the feasibility of measuring the neutron lifetime via an alternative, space-based class of methods, which use neutrons generated by galactic cosmic ray spallation of planets' surfaces and atmospheres. Free neutrons decay via the weak interaction with a mean lifetime of around 880 s. This lifetime constrains the unitarity of the CKM matrix and is a key parameter for studies of Big-Bang nucleosynthesis. However, current laboratory measurements, using two independent approaches, differ by over 4$\\\\sigma$. Using data acquired in 2007 and 2008 during flybys of Venus and Mercury by NASA's MESSENGER spacecraft, which was not designed to make this measurement, we estimate the neutron lifetime to be $780\\\\pm60_\\\\textrm{stat}\\\\pm70_\\\\textrm{syst}$ s, thereby demonstrating the viability of this new approach.\",\n",
       "  'len': 827},\n",
       " {'abstract': 'Sulfur-water chemistry plays an important role in the middle atmosphere of Venus. Ground based observations have found that simultaneously observed SO2 and H2O at ~64 km vary with time and are temporally anti-correlated. To understand these observations, we explore the sulfur-water chemical system using a one-dimensional chemistry-diffusion model. We find that SO2 and H2O mixing ratios above the clouds are highly dependent on mixing ratios of the two species at the middle cloud top (58 km). The behavior of sulfur-water chemical system can be classified into three regimes but there is no abrupt transition among these regimes. In particular, there is no bifurcation behavior as previously claimed. We also find that the SO2 self-shielding effect causes H2O above the clouds to respond to the middle cloud top in a non-monotonic fashion. Through comparison with observations, we find that mixing ratio variations at the middle cloud top can explain the observed variability of SO2 and H2O. The sulfur-water chemistry in the middle atmosphere is responsible for the H2O-SO2 anti-correlation at 64 km. Eddy transport change alone cannot explain the variations of both species. These results imply that variations of species abundance in the middle atmosphere are significantly influenced by the lower atmospheric processes. Continued ground-based measurements of the co-evolution of SO2 and H2O above the clouds and new spacecraft missions will be crucial for uncover the complicated processes underlying the interaction among the lower atmosphere, the clouds and the middle atmosphere of Venus.',\n",
       "  'len': 1609},\n",
       " {'abstract': \"Luna 3 (or Lunik 3 in Russian sources) was the first spacecraft to perform a flyby of the Moon. Launched in October 1959 on a translunar trajectory with large semi-major axis and eccentricity, it collided with the Earth in late March 1960. The short, 6-month dynamical lifetime has often been explained through an increase in eccentricity due to the Lidov-Kozai effect. However, the classical Lidov-Kozai solution is only valid in the limit of small semi-major axis ratio, a condition that is satisfied only for solar (but not for lunar) perturbations. We undertook a study of the dynamics of Luna 3 with the aim of assessing the principal mechanisms affecting its evolution. We analyze the Luna 3 trajectory by generating accurate osculating solutions, and by comparing them to integrations of singly- and doubly-averaged equations of motion in vectorial form. Lunar close encounters, which cannot be reproduced in an averaging approach, decisively affect the trajectory and break the doubly-averaged dynamics. Solar perturbations induce oscillations of intermediate period that affect the geometry of the close encounters and cause the singly-averaged and osculating inclinations to change quadrants (the orbital plane ``flips''). We find that the peculiar evolution of Luna 3 can only be explained by taking into account lunar close encounters and intermediate-period terms; such terms are averaged out in the Lidov-Kozai solution, which is not adequate to describe translunar or cislunar trajectories. Understanding the limits of the Lidov-Kozai solution is of particular significance for the motion of objects in the Earth-Moon environment and of exoplanetary systems.\",\n",
       "  'len': 1684},\n",
       " {'abstract': 'Frequency-domain expressions are found for gradiometer and satellite-to-satellite tracking measurements of a point source on the surface of the Earth. The maximum signal-to-noise ratio as a function of noise in the measurement apparatus is computed, and from that the minimum detectable point mass is inferred. A point mass of magnitude M_3=100 Gt gives a signal-to-noise ratio of 3 when a GOCE-like gradiometer passes directly over the mass. On the satellite-to-satellite tracking mission GRACE-FO M_3=1.3 Gt for the microwave instrument and M_3=0.5 Gt for the laser ranging interferometer. The sensitivity of future GRACE-like missions with different orbital parameters and improved accelerometer sensitivity is explored, and the optimum spacecraft separation for detecting point-like sources is found. The future-mission benefit of improving the accelerometer sensitivity for measurement of non-gravitational disturbances is shown by the resulting reduction of M_3 to as small as 7 Mt for 500 km orbital altitude and optimized satellite separation of 900 km.',\n",
       "  'len': 1072},\n",
       " {'abstract': \"Tidal effects in planetary systems are the main driver in the orbital migration of natural satellites. They result from physical processes occurring deep inside celestial bodies, whose effects are rarely observable from surface imaging. For giant planet systems, the tidal migration rate is determined by poorly understood dissipative processes in the planet, and standard theories suggest an orbital expansion rate inversely proportional to the power 11/2 in distance, implying little migration for outer moons such as Saturn's largest moon, Titan. Here, we use two independent measurements obtained with the Cassini spacecraft to measure Titan's orbital expansion rate. We find Titan migrates away from Saturn at 11.3 $\\\\pm$ 2.0 cm/year, corresponding to a tidal quality factor of Saturn of Q $\\\\simeq$ 100, and a migration timescale of roughly 10 Gyr. This rapid orbital expansion suggests Titan formed significantly closer to Saturn and has migrated outward to its current position. Our results for Titan and five other moons agree with the predictions of a resonance locking tidal theory, sustained by excitation of inertial waves inside the planet. The associated tidal expansion is only weakly sensitive to orbital distance, motivating a revision of the evolutionary history of Saturn's moon system. The resonance locking mechanism could operate in other systems such as stellar binaries and exoplanet systems, and it may allow for tidal dissipation to occur at larger orbital separations than previously believed.\",\n",
       "  'len': 1530},\n",
       " {'abstract': \"The gravitational dipole theory of Hadjukovic (2010) is based on the hypothesis that antimatter has a negative gravitational mass and thus falls upwards on Earth. Astrophysically, the model is similar to but more fundamental than Modified Newtonian Dynamics (MOND), with the Newtonian gravity $g_{_N}$ towards an isolated point mass boosted by the factor $\\\\nu = 1 + \\\\left( \\\\alpha/x \\\\right) \\\\tanh \\\\left( \\\\sqrt{x}/\\\\alpha \\\\right)$, where $x \\\\equiv g_{_N}/a_{_0}$ and $a_{_0} = 1.2 \\\\times 10^{-10}$ m/s$^2$ is the MOND acceleration constant. We show that $\\\\alpha$ must lie in the range ${0.4-1}$ to acceptably fit galaxy rotation curves. In the Solar System, this interpolating function implies an extra Sunwards acceleration of ${\\\\alpha a_{_0}}$. This would cause Saturn to deviate from Newtonian expectations by ${7000 \\\\left( \\\\alpha/0.4 \\\\right)}$ km over 15 years, starting from known initial position and velocity on a near-circular orbit. We demonstrate that this prediction should not be significantly altered by the postulated dipole haloes of other planets due to the rather small region in which each planet's gravity dominates over that of the Sun. The orbit of Saturn should similarly be little affected by a possible ninth planet in the outer Solar System and by the Galactic gravity causing a non-spherical distribution of gravitational dipoles several kAU from the Sun. Radio tracking of the Cassini spacecraft orbiting Saturn yields a ${5\\\\sigma}$ upper limit of 160 metres on deviations from its conventionally calculated trajectory. These measurements imply a much more stringent upper limit on $\\\\alpha$ than the minimum required for consistency with rotation curve data. Therefore, no value of $\\\\alpha$ can simultaneously match all available constraints, falsifying the gravitational dipole theory in its current form at extremely high significance.\",\n",
       "  'len': 1872},\n",
       " {'abstract': \"The nature of the plasma wave modes around the ion kinetic scales in highly Alfvénic slow solar wind turbulence is investigated using data from the NASA's Parker Solar Probe taken in the inner heliosphere, at 0.18 Astronomical Unit (AU) from the sun. The joint distribution of the normalized reduced magnetic helicity ${\\\\sigma}_m ({\\\\theta}_{RB}, {\\\\tau})$ is obtained, where ${\\\\theta}_{RB}$ is the angle between the local mean magnetic field and the radial direction and ${\\\\tau}$ is the temporal scale. Two populations around ion scales are identified: the first population has ${\\\\sigma}_m ({\\\\theta}_{RB}, {\\\\tau}) < 0$ for frequencies (in the spacecraft frame) ranging from 2.1 to 26 Hz for $60^{\\\\circ} < {\\\\theta}_{RB} < 130^{\\\\circ}$, corresponding to kinetic Alfvén waves (KAWs), and the second population has ${\\\\sigma}_m ({\\\\theta}_{RB}, {\\\\tau}) > 0$ in the frequency range [1.4, 4.9] Hz for ${\\\\theta}_{RB} > 150^{\\\\circ}$, corresponding to Alfvén ion Cyclotron Waves (ACWs). This demonstrates for the first time the co-existence of KAWs and ACWs in the slow solar wind in the inner heliosphere, which contrasts with previous observations in the slow solar wind at 1 AU. This discrepancy between 0.18 and 1 AU could be explained, either by i) a dissipation of ACWs via cyclotron resonance during their outward journey, or by ii) the high Alfvénicity of the slow solar wind at 0.18 AU that may be favorable for the excitation of ACWs.\",\n",
       "  'len': 1443},\n",
       " {'abstract': 'Space-based gravitational-wave detectors consist of a triangle of three spacecraft, which makes it possible to detect polarization modes of gravitational waves due to the motion of the detectors in space. In this paper we explore the ability of Taiji to detect the polarization modes in the parametrized post-Einsteinian framework. Assuming massive black hole binaries with the total mass of $M=4\\\\times10^5\\\\,M_{\\\\odot}$ at redshift of $z=1$, we find that Taiji can measure the dipole and quadruple emission ($\\\\Delta\\\\alpha_D/\\\\alpha_D$ and $\\\\Delta\\\\alpha_Q/\\\\alpha_Q$) with the accuracy of up to $\\\\sim 0.04\\\\%$, with the fiducial value of $\\\\alpha_D=0.001$, the scalar transverse and longitudinal modes ($\\\\Delta\\\\alpha_B$ and $\\\\Delta\\\\alpha_L$) up to $\\\\sim 0.01$, and the vector modes ($\\\\Delta\\\\alpha_V$) up to $\\\\sim 0.0005$.',\n",
       "  'len': 826},\n",
       " {'abstract': 'This paper presents a new deep learning-based framework for robust nonlinear estimation and control using the concept of a Neural Contraction Metric (NCM). The NCM uses a deep long short-term memory recurrent neural network for a global approximation of an optimal contraction metric, the existence of which is a necessary and sufficient condition for exponential stability of nonlinear systems. The optimality stems from the fact that the contraction metrics sampled offline are the solutions of a convex optimization problem to minimize an upper bound of the steady-state Euclidean distance between perturbed and unperturbed system trajectories. We demonstrate how to exploit NCMs to design an online optimal estimator and controller for nonlinear systems with bounded disturbances utilizing their duality. The performance of our framework is illustrated through Lorenz oscillator state estimation and spacecraft optimal motion planning problems.',\n",
       "  'len': 959},\n",
       " {'abstract': 'This paper presents ConVex optimization-based Stochastic steady-state Tracking Error Minimization (CV-STEM), a new state feedback control framework for a class of Ito stochastic nonlinear systems and Lagrangian systems. Its innovation lies in computing the control input by an optimal contraction metric, which greedily minimizes an upper bound of the steady-state mean squared tracking error of the system trajectories. Although the problem of minimizing the bound is non-convex, its equivalent convex formulation is proposed utilizing state-dependent coefficient parameterizations of the nonlinear system equation. It is shown using stochastic incremental contraction analysis that the CV-STEM provides a sufficient guarantee for exponential boundedness of the error for all time with L2-robustness properties. For the sake of its sampling-based implementation, we present discrete-time stochastic contraction analysis with respect to a state- and time-dependent metric along with its explicit connection to continuous-time cases. We validate the superiority of the CV-STEM to PID, H-infinity, and baseline nonlinear controllers for spacecraft attitude control and synchronization problems.',\n",
       "  'len': 1203},\n",
       " {'abstract': \"Discovered in October 2017, the interstellar object designated 1I/'Oumuamua was the first such object to be observed travelling through our solar system. 1I/'Oumuamua has other characteristics never seen before in a celestial body and in-situ observations and measurements would be of extraordinary scientific value. Previous studies have demonstrated the viability of spacecraft trajectories to 'Oumuamua using chemical propulsion with a Solar Oberth burn at a perihelion as low as a few solar radii. In addition to chemical propulsion, there is also the possibility of missions involving light sails accelerated by the radiation pressure of a laser beam from a laser located on Earth. Based on a scaled-down Breakthrough Starshot beaming infrastructure, interplanetary missions and missions to the outer solar system have been proposed using lower sailcraft speeds of 0.001c relaxing the laser power requirements (3-30 GW for 1-100 kg spacecraft) and various other mission constraints. This paper uses the OITS trajectory simulation tool, which assumes an impulsive $\\\\Delta$V increment, to analyze the trajectories which might be followed by a sailcraft to 'Oumuamua, with a launch in the year 2030 and assuming it has already been accelerated to a maximum speed of 300km/s (approx. 0.001c) by the laser. A minimum flight duration of 440 days for a launch in July 2030 is found. The intercept would take place beyond 82 AU. We conclude that the possibility of launching a large number of spacecraft and reaching 1I much faster than chemical propulsion would circumvent several disadvantages of previously proposed mission architectures.\",\n",
       "  'len': 1649},\n",
       " {'abstract': \"The proposal of increasingly complex and innovative space endeavours poses growing demands for mission designers. In order to meet the established requirements and constraints while maintaining a low fuel cost, the use of low-energy trajectories is particularly interesting. These allow spacecraft to change orbits and move with little to no fuel, but they are computed using motion models of a higher fidelity than the commonly used two-body problem (2BP). For this purpose, perturbation methods that explore the third-body effect are especially attractive, since they can accurately convey the system dynamics of a three-body configuration with a lower computational cost, by employing mapping techniques or exploring analytical approximations. The focus of this work is to broaden the knowledge of low-energy trajectories by developing new mathematical tools to assist in mission design applications. In particular, novel motion models based on the third-body effect are conceived. One application of this study focuses on the trajectory design for missions to near-Earth asteroids. Two different projects are explored: one is based on the preliminary design of separate rendezvous and capture missions to the invariant manifolds of libration point $L_2$. This is achieved by studying two recently discovered asteroids and determining dates, fuel cost and final control history for each trajectory. The other covers a larger study on asteroid capture missions, where several bodies are regarded as potential targets. The candidates are considered using a multi-fidelity design framework that filters through the trajectory options using models of motion of increasing accuracy, so that a final refined, low-thrust solution is obtained. The trajectory design hinges on harnessing Earth's gravity by exploiting encounters outside its sphere of influence, the named Earth-resonant encounters.\",\n",
       "  'len': 1903},\n",
       " {'abstract': 'The High Inclination Solar Mission (HISM) is a concept for an out-of-the-ecliptic mission for observing the Sun and the heliosphere. The mission profile is largely based on the Solar Polar Imager concept: initially spiraling in to a 0.48 AU ecliptic orbit, then increasing the orbital inclination at a rate of $\\\\sim 10$ degrees per year, ultimately reaching a heliographic inclination of $>$75 degrees. The orbital profile is achieved using solar sails derived from the technology currently being developed for the Solar Cruiser mission, currently under development. HISM remote sensing instruments comprise an imaging spectropolarimeter (Doppler imager / magnetograph) and a visible light coronagraph. The in-situ instruments include a Faraday cup, an ion composition spectrometer, and magnetometers. Plasma wave measurements are made with electrical antennas and high speed magnetometers. The $7,000\\\\,\\\\mathrm{m}^2$ sail used in the mission assessment is a direct extension of the 4-quadrant $1,666\\\\,\\\\mathrm{m}^2$ Solar Cruiser design and employs the same type of high strength composite boom, deployment mechanism, and membrane technology. The sail system modelled is spun (~1 rpm) to assure required boom characteristics with margin. The spacecraft bus features a fine-pointing 3-axis stabilized instrument platform that allows full science observations as soon as the spacecraft reaches a solar distance of 0.48 AU.',\n",
       "  'len': 1430},\n",
       " {'abstract': 'Concordant evidence points towards the existence of a ninth planet in the Solar System at more than $400\\\\,$AU from the Sun. In particular, trans-Neptunian object orbits are perturbed by the presence of a putative gravitational source. Since this planet has not yet been observationally found with conventional telescope research, it has been argued that it could be a dark compact object, namely a black hole of probably primordial origin. Within this assumption, we discuss the possibility of detecting Planet 9 via a sub-relativistic spacecraft fly-by and the measure of its Hawking radiation in the radio band and conclude that it is too faint compared to the CMB. We thus present other perspectives with rather a satellite mission and conclude that smaller black holes would give much more interesting signals. We emphasize the importance of the study of such Hawking radiation laboratories in the Solar System.',\n",
       "  'len': 926},\n",
       " {'abstract': 'There has been important understanding of the process by which a hypersonic dust impact makes an electrical signal on a spacecraft sensor, leading to a fuller understanding of the physics. Zaslavsky (2015) showed that the most important signal comes from the charging of the spacecraft, less from charging of an antenna. The present work is an extension of the work of Zaslavsky. An analytical treatment of the physics of a hypersonic dust impact and the mechanism for generating an electrical signal in a sensor, an antenna, is presented. The treatment is compared with observations from STEREO and Parker Solar Probe. A full treatment of this process by simulations seems beyond present computer capabilities, but some parts of the treatment can must depend on simulations but other features can be better understood through analytical treatment. Evidence for a somewhat larger contribution from the antenna part of the signal than in previous publications is presented. Importance of electrostatic forces in forming the exiting plasma cloud is emphasized. Electrostatic forces lead to a rapid expansion of the escaping cloud, so that it expands more rapidly than escapes, and frequently surrounds one or more antennas. This accounts for the ability of dipole antennas to detect dust impacts. Some progress toward an understanding occasional negative charging of an antenna is presented, together with direct evidence of such charging. Use of laboratory measurements of charge to estimate size of spacecraft impacts are shown to be not reliable without further calibration work.',\n",
       "  'len': 1591},\n",
       " {'abstract': 'One of the very common in situ signatures of interplanetary coronal mass ejections (ICMEs), as well as other interplanetary transients, are Forbush decreases (FDs), i.e. short-term reductions in the galactic cosmic ray (GCR) flux. A two-step FD is often regarded as a textbook example, which presumably owes its specific morphology to the fact that the measuring instrument passed through the ICME head-on, encountering first the shock front (if developed), then the sheath and finally the CME magnetic structure. The interaction of GCRs and the shock/sheath region, as well as the CME magnetic structure, occurs all the way from Sun to Earth, therefore, FDs are expected to reflect the evolutionary properties of CMEs and their sheaths. We apply modelling to different ICME regions in order to obtain a generic two-step FD profile, which qualitatively agrees with our current observation-based understanding of FDs. We next adapt the models for energy dependence to enable comparison with different GCR measurement instruments (as they measure in different particle energy ranges). We test these modelling efforts against a set of multi-spacecraft observations of the same event, using the Forbush decrease model for the expanding flux rope (ForbMod). We find a reasonable agreement of the ForbMod model for the GCR depression in the CME magnetic structure with multi-spacecraft measurements, indicating that modelled FDs reflect well the CME evolution.',\n",
       "  'len': 1465},\n",
       " {'abstract': \"Mars shares many similarities and characteristics to Earth including various geological features and planetary structure. The remarkable bimodal distribution of elevations in both planets is one of the most striking global features suggesting similar geodynamic processes of crustal differentiation on Earth and Mars. There also exist several evidences, based on geographic features resembling ancient shorelines, for existence of an ancient martian ocean in the northern hemisphere which covers nearly one third of the planet's surface. However, the interpretation of some features as ancient shorelines has been thoroughly challenged that left the existence of a primordial martian ocean controversial. Moreover, if oceans were formerly present on Mars, there is still a big ambiguity about the volume of water with the estimations ranging over $4$ orders of magnitude. Here we map the martian sea level problem onto a percolation model that provides strong evidence that the longest iso-height line on Mars that separates the northern and southern hemispheres, acts as a critical level height with divergent correlation length and plays the same role as the present mean sea level does on Earth. Our results unravel remarkable similarities between Mars and Earth, posing a testable hypothesis about the level of the ancient ocean on Mars that can be answered experimentally by the future investigations and spacecraft exploration.\",\n",
       "  'len': 1444},\n",
       " {'abstract': \"Recent improvements in data collection volume from planetary and space physics missions have allowed the application of novel data science techniques. The Cassini mission for example collected over 600 gigabytes of scientific data from 2004 to 2017. This represents a surge of data on the Saturn system. Machine learning can help scientists work with data on this larger scale. Unlike many applications of machine learning, a primary use in planetary space physics applications is to infer behavior about the system itself. This raises three concerns: first, the performance of the machine learning model, second, the need for interpretable applications to answer scientific questions, and third, how characteristics of spacecraft data change these applications. In comparison to these concerns, uses of black box or un-interpretable machine learning methods tend toward evaluations of performance only either ignoring the underlying physical process or, less often, providing misleading explanations for it. We build off a previous effort applying a semi-supervised physics-based classification of plasma instabilities in Saturn's magnetosphere. We then use this previous effort in comparison to other machine learning classifiers with varying data size access, and physical information access. We show that incorporating knowledge of these orbiting spacecraft data characteristics improves the performance and interpretability of machine learning methods, which is essential for deriving scientific meaning. Building on these findings, we present a framework on incorporating physics knowledge into machine learning problems targeting semi-supervised classification for space physics data in planetary environments. These findings present a path forward for incorporating physical knowledge into space physics and planetary mission data analyses for scientific discovery.\",\n",
       "  'len': 1884},\n",
       " {'abstract': 'The coma of comet 67P/Churyumov-Gerasimenko has been probed by the Rosetta spacecraft and shows a variety of different molecules. The ROSINA COmet Pressure Sensor and the Double Focusing Mass Spectrometer provide in-situ densities for many volatile compounds including the 14 gas species H2O, CO2, CO, H2S, O2, C2H6, CH3OH, H2CO, CH4, NH3, HCN, C2H5OH, OCS, and CS2. We fit the observed densities during the entire comet mission between August 2014 and September 2016 to an inverse coma model. We retrieve surface emissions on a cometary shape with 3996 triangular elements for 50 separated time intervals. For each gas we derive systematic error bounds and report the temporal evolution of the production, peak production, and the time-integrated total production. We discuss the production for the two lobes of the nucleus and for the northern and southern hemispheres. Moreover we provide a comparison of the gas production with the seasonal illumination.',\n",
       "  'len': 969},\n",
       " {'abstract': 'When a fast dust particle hits a spacecraft, it generates a cloud of plasma some of which escapes into space and the momentary charge imbalance perturbs the spacecraft voltage with respect to the plasma. Electrons race ahead of ions, however both respond to the DC electric field of the spacecraft. If the spacecraft potential is positive with respect to the plasma, it should attract the dust cloud electrons and repel the ions, and vice versa. Here we use measurements of impulsive voltage signals from dust impacts on the Parker Solar Probe (PSP) spacecraft to show that the peak voltage amplitude is clearly related to the spacecraft floating potential, consistent with theoretical models and laboratory measurements. In addition, we examine some timescales associated with the voltage waveforms and compare to the timescales of spacecraft charging physics.',\n",
       "  'len': 872},\n",
       " {'abstract': 'During March-April 2002, while between the orbits of Jupiter and Saturn, the Cassini spacecraft detected a significant enhancement in pickup proton flux. The most likely explanation for this enhancement was the addition of protons to the solar wind by the ionization of neutral hydrogen in the corona of comet 153P/Ikeya-Zhang. This comet passed relatively close to the Sun-Cassini line during that period, allowing pickup ions to be carried to Cassini by the solar wind. This pickup proton flux could have been further modulated by the passage of the interplanetary counterparts of coronal mass ejections past the comet and spacecraft. The radial distance of 6.5 Astronomical Units (au) traveled by the pickup protons, and the implied total tail length of >7.5 au make this cometary ion tail the longest yet measured.',\n",
       "  'len': 829},\n",
       " {'abstract': \"ASTERIA (Arcsecond Space Telescope Enabling Research In Astrophysics) is a 6U CubeSat space telescope (10 cm x 20 cm x 30 cm, 10 kg). ASTERIA's primary mission objective was demonstrating two key technologies for reducing systematic noise in photometric observations: high-precision pointing control and high-stabilty thermal control. ASTERIA demonstrated 0.5 arcsecond RMS pointing stability and $\\\\pm$10 milliKelvin thermal control of its camera payload during its primary mission, a significant improvement in pointing and thermal performance compared to other spacecraft in ASTERIA's size and mass class. ASTERIA launched in August 2017 and deployed from the International Space Station (ISS) November 2017. During the prime mission (November 2017 -- February 2018) and the first extended mission that followed (March 2018 - May 2018), ASTERIA conducted opportunistic science observations which included collection of photometric data on 55 Cancri, a nearby exoplanetary system with a super-Earth transiting planet. The 55 Cancri data were reduced using a custom pipeline to correct CMOS detector column-dependent gain variations. A Markov Chain Monte Carlo (MCMC) approach was used to simultaneously detrend the photometry using a simple baseline model and fit a transit model. ASTERIA made a marginal detection of the known transiting exoplanet 55 Cancri e ($\\\\sim2$~\\\\Rearth), measuring a transit depth of $374\\\\pm170$ ppm. This is the first detection of an exoplanet transit by a CubeSat. The successful detection of super-Earth 55 Cancri e demonstrates that small, inexpensive spacecraft can deliver high-precision photometric measurements.\",\n",
       "  'len': 1656},\n",
       " {'abstract': 'We consider General Relativity as a limit case of the Scalar-Tensor theory with Barbero-Immirzi field when the field tends to a constant. We use Shapiro time delay experimental limit of $1/w = (2.1 \\\\pm 2.3)10^{-5}$ provided by the Cassini spacecraft to find the Barbero-Immirzi parameter value.',\n",
       "  'len': 305},\n",
       " {'abstract': 'In 2015, the New Horizons spacecraft flew past Pluto and its moon Charon, providing the first clear look at the surface of Charon. New Horizons images revealed an ancient surface, a large, intricate canyon system, and many fractures, among other geologic features. Here, we assess whether tidal stresses played a significant role in the formation of tensile fractures on Charon. Although presently in a circular orbit, most scenarios for the orbital evolution of Charon include an eccentric orbit for some period of time and possibly an internal ocean. Past work has shown that these conditions could have generated stresses comparable in magnitude to other tidally fractured moons, such as Europa and Enceladus. However, we find no correlation between observed fracture orientations and those predicted to form due to eccentricity-driven tidal stress. It thus seems more likely that the orbit of Charon circularized before its ocean froze, and that either tidal stresses alone were insufficient to fracture the surface or subsequent resurfacing remove these ancient fractures.',\n",
       "  'len': 1088},\n",
       " {'abstract': \"The gravity field maps of the satellite gravimetry missions GRACE (Gravity Recovery and Climate Experiment) and GRACE Follow-On are derived by means of precise orbit determination. The key observation is the biased inter-satellite range, which is measured primarily by a K-Band Ranging system (KBR) in GRACE and GRACE Follow-On. The GRACE Follow-On satellites are additionally equipped with a Laser Ranging Interferometer (LRI), which provides measurements with lower noise compared to the KBR. The biased range of KBR and LRI needs to be converted for gravity field recovery into an instantaneous range, i.e. the biased Euclidean distance between the satellites' center-of-mass at the same time. One contributor to the difference between measured and instantaneous range arises due to the non-zero travel time of electro-magnetic waves between the spacecraft. We revisit the calculation of the light time correction (LTC) from first principles considering general relativistic effects and state-of-the-art models of Earth's potential field. The novel analytical expressions for the LTC of KBR and LRI can circumvent numerical limitations of the classical approach. The dependency of the LTC on geopotential models and on the parameterization is studied, and afterwards the results are compared against the LTC provided in the official datasets of GRACE and GRACE Follow-On. It is shown that the new approach has a significantly lower noise, well below the instrument noise of current instruments, especially relevant for the LRI, and even if used with kinematic orbit products. This allows calculating the LTC accurate enough even for the next generation of gravimetric missions.\",\n",
       "  'len': 1691},\n",
       " {'abstract': 'Although deep networks have been widely adopted, one of their shortcomings has been their blackbox nature. One particularly difficult problem in machine learning is multivariate time series (MVTS) classification. MVTS data arise in many applications and are becoming ever more pervasive due to explosive growth of sensors and IoT devices. Here, we propose a novel network (IETNet) that identifies the important channels in the classification decision for each instance of inference. This feature also enables identification and removal of non-predictive variables which would otherwise lead to overfit and/or inaccurate model. IETNet is an end-to-end network that combines temporal feature extraction, variable selection, and joint variable interaction into a single learning framework. IETNet utilizes an 1D convolutions for temporal features, a novel channel gate layer for variable-class assignment using an attention layer to perform cross channel reasoning and perform classification objective. To gain insight into the learned temporal features and channels, we extract region of interest attention map along both time and channels. The viability of this network is demonstrated through a multivariate time series data from N body simulations and spacecraft sensor data.',\n",
       "  'len': 1287},\n",
       " {'abstract': 'The recent simulations showed that the whistler heat flux instability, which presumably produces the most of quasi-parallel coherent whistler waves in the solar wind, is not efficient in regulating the electron heat conduction. In addition, recent spacecraft measurements indicated that some fraction of coherent whistler waves in the solar wind may propagate anti-parallel to the electron heat flux, being produced due to a perpendicular temperature anisotropy of suprathermal electrons. We present analysis of properties of parallel and anti-parallel whistler waves unstable at electron heat fluxes and temperature anisotropies of suprathermal electrons typical of the pristine solar wind. Assuming the electron population consisting of counter-streaming dense thermal core and tenuous suprathermal halo populations, we perform a linear stability analysis to demonstrate that anti-parallel whistler waves are expected to have smaller frequencies, wave numbers and growth rates compared to parallel whistler waves. The stability analysis is performed over a wide range of parameters of core and halo electron populations. Using the quasi-linear scaling relation we show that anti-parallel whistler waves saturate at amplitudes of one order of magnitude smaller than parallel whistler waves, which is at about $10^{-3}\\\\;B_0$ in the pristine solar wind. The analysis shows that the presence of anti-parallel whistler waves in the pristine solar wind is more likely to be obscured by turbulent magnetic field fluctuations, because of lower frequencies and smaller amplitudes compared to parallel whistler waves. The presented results will be also valuable for numerical simulations of the electron heat flux regulation in the solar wind.',\n",
       "  'len': 1746},\n",
       " {'abstract': 'The slow solar wind is typically characterized as having low Alfvénicity. However, Parker Solar Probe (PSP) observed predominately Alfvénic slow solar wind during several of its initial encounters. From its first encounter observations, about 55.3\\\\% of the slow solar wind inside 0.25 au is highly Alfvénic ($|\\\\sigma_C| > 0.7$) at current solar minimum, which is much higher than the fraction of quiet-Sun-associated highly Alfvénic slow wind observed at solar maximum at 1 au. Intervals of slow solar wind with different Alfvénicities seem to show similar plasma characteristics and temperature anisotropy distributions. Some low Alfvénicity slow wind intervals even show high temperature anisotropies, because the slow wind may experience perpendicular heating as fast wind does when close to the Sun. This signature is confirmed by Wind spacecraft measurements as we track PSP observations to 1 au. Further, with nearly 15 years of Wind measurements, we find that the distributions of plasma characteristics, temperature anisotropy and helium abundance ratio ($N_\\\\alpha/N_p$) are similar in slow winds with different Alfvénicities, but the distributions are different from those in the fast solar wind. Highly Alfvénic slow solar wind contains both helium-rich ($N_\\\\alpha/N_p\\\\sim0.045$) and helium-poor ($N_\\\\alpha/N_p\\\\sim0.015$) populations, implying it may originate from multiple source regions. These results suggest that highly Alfvénic slow solar wind shares similar temperature anisotropy and helium abundance properties with regular slow solar winds, and they thus should have multiple origins.',\n",
       "  'len': 1615},\n",
       " {'abstract': 'Two new interplanetary technologies have advanced in the past decade to the point where they may enable exciting, affordable missions that reach further and faster deep into the outer regions of our solar system: (i) small and capable interplanetary spacecraft and (ii) light-driven sails. Combination of these two technologies could drastically reduce travel times within the solar system. We discuss a new paradigm that involves small and fast moving sailcraft that could enable exploration of distant regions of the solar system much sooner and faster than previously considered. We present some of the exciting science objectives for these miniaturized intelligent space systems that could lead to transformational advancements in the space sciences in the coming decade.',\n",
       "  'len': 786},\n",
       " {'abstract': 'Self-replicating probes are spacecraft with the capacity to create copies of themselves. Self-replication would potentially allow for an exponential increase in the number of probes and thereby drastically improve the efficiency of space exploration. Despite this potential, an integrated assessment of self-replicating space probes has not been presented since the 1980s, and it is still unclear how far they are feasible. In this paper, we propose a concept for a partially self-replicating probe for space exploration based on current and near-term technologies, with a focus on small spacecraft. The purpose is to chart a path towards self-replication with near-term benefits, rather than attempting full self-replication. For this reason, components such as microchips and other microelectronic components are brought with the initial probe and are not replicated. We estimate that such a probe would be capable of replicating 70% of its mass. To further increase this percentage, we identify technology gaps that are promising to address. We conclude that small-scale, partially self-replicating probes are feasible near-term. Their benefits would play out in exploration missions requiring roughly more than a dozen of probes.',\n",
       "  'len': 1244},\n",
       " {'abstract': 'First results from the Parker Solar Probe (PSP) mission have revealed ubiquitous coherent ion-scale waves in the inner heliosphere, which are signatures of kinetic wave-particle interactions and fluid-scale instabilities. However, initial studies of the circularly polarized ion-scale waves observed by PSP have only thoroughly analyzed magnetic field signatures, precluding a determination of solar-wind frame propagation direction and intrinsic wave-polarization. A comprehensive determination of wave-properties requires measurements of both electric and magnetic fields. Here, we use full capabilities of the PSP/FIELDS instrument suite to measure both the electric and magnetic components of circularly polarized waves. Comparing spacecraft frame magnetic field measurements with the Doppler-shifted cold-plasma dispersion relation for parallel transverse waves constrains allowable plasma frame polarizations and wave-vectors. We demonstrate that the Doppler-shifted cold-plasma dispersion has a maximum spacecraft frequency $f_{sc}^{*}$ for which intrinsically right-handed fast-magnetosonic waves (FMWs) propagating sunwards can appear left-handed in the spacecraft frame. Observations of left-handed waves with $|f|>f_{sc}^{*}$ are uniquely explained by intrinsically left-handed, ion-cyclotron, waves (ICWs). We demonstrate that electric field measurements for waves with $|f|>f_{sc}^{*}$ are consistent with ICWs propagating away from the sun, verifying the measured electric field. Applying the verified electric field measurements to the full distribution of waves suggests that, in the solar wind frame, the vast majority of waves propagate away from the sun, indicating that the observed population of coherent ion-scale waves contains both intrinsically left and right hand polarized modes.',\n",
       "  'len': 1817},\n",
       " {'abstract': \"In 1931/32, Schroedinger studied a hot gas Gedankenexperiment, an instance of large deviations of the empirical distribution and an early example of the so-called maximum entropy inference method. This so-called Schroedinger bridge problem (SBP) was recently recognized as a regularization of the Monge-Kantorovich Optimal Mass Transport (OMT), leading to effective computation of the latter. Specifically, OMT with quadratic cost may be viewed as a zero-temperature limit of SBP, which amounts to minimization of the Helmholtz's free energy over probability distributions constrained to possess given marginals. The problem features a delicate compromise, mediated by a temperature parameter, between minimizing the internal energy and maximizing the entropy. These concepts are central to a rapidly expanding area of modern science dealing with the so-called {\\\\em Sinkhorn algorithm} which appears as a special case of an algorithm first studied by the French analyst Robert Fortet in 1938/40 specifically for Schroedinger bridges. Due to the constraint on end-point distributions, dynamic programming is not a suitable tool to attack these problems. Instead, Fortet's iterative algorithm and its discrete counterpart, the Sinkhorn iteration, permit computation by iteratively solving the so-called {\\\\em Schroedinger system}. In both the continuous as well as the discrete-time and space settings, {\\\\em stochastic control} provides a reformulation and dynamic versions of these problems. The formalism behind these control problems have attracted attention as they lead to a variety of new applications in spacecraft guidance, control of robot or biological swarms, sensing, active cooling, network routing as well as in computer and data science. This multifacet and versatile framework, intertwining SBP and OMT, provides the substrate for a historical and technical overview of the field taken up in this paper.\",\n",
       "  'len': 1927},\n",
       " {'abstract': 'A polytropic process describes the transition of a fluid from one state to another through a specific relationship between the fluid density and temperature. The value of the polytropic index that governs this relationship determines the heat transfer and the effective degrees of freedom during the process. In this study, we analyze solar wind proton plasma measurements, obtained by the Faraday cup instrument on-board Parker Solar Probe. We examine the large-scale variations of the proton plasma density and temperature within the inner heliosphere explored by the spacecraft. We also address a polytropic behavior in the density and temperature fluctuations in short-time intervals, which we analyze in order to derive the effective polytropic index of small time-scale processes. The large-scale variations of the solar wind proton density and temperature which are associated with the plasma expansion through the heliosphere, follow a polytropic model with a polytropic index ~5/3. On the other hand, the short time-scale fluctuations which may be associated with turbulence, follow a model with a larger polytropic index. We investigate possible correlations between the polytropic index of short time-scale fluctuations and the plasma speed, plasma beta, and the magnetic field direction. We discuss the scenario of mechanisms including energy transfer or mechanisms that restrict the particle effective degrees of freedom.',\n",
       "  'len': 1445},\n",
       " {'abstract': 'The large constellations of spacecraft planned for use in cislunar space (on the Lunar surface, in Lunar orbit, and in the vicinity of the Lunar Gateway) require new solutions for positioning, navigation and timing (PNT). Here, I describe COMPASS (Combined Observational Methods for Positional Awareness in the Solar System), a spacecraft navigation system to provide cost-effective techniques for the positioning of large numbers of spacecraft in cislunar space. COMPASS will use beacons that emit coherent ultra-wideband signals designed to be interoperable with existing and future Very Long Baseline Interferometry (VLBI) networks. Using differential VLBI, COMPASS will provide rapid determination of the interferometric phase delay with picosecond level accuracy during routine VLBI observing sessions. Multi-baseline phase-referenced COMPASS-VLBI observations with simultaneous calibrator observations should thus enable sub-meter accuracy transverse positioning and meter level lunar orbit determination using with small femtospacecraft beacons and a few seconds of observation per position determination.',\n",
       "  'len': 1123},\n",
       " {'abstract': 'The analysis of the wave content inside a perpendicular bow shock indicates that heating of ions is related to the Lower-Hybrid-Drift (LHD) instability, and heating of electrons to the Electron-Cyclotron-Drift (ECD) instability. Both processes represent stochastic acceleration caused by the electric field gradients on the electron gyroradius scales, produced by the two instabilities. Stochastic heating is a single particle mechanism where large gradients break adiabatic invariants and expose particles to direct acceleration by the DC- and wave-fields. The acceleration is controlled by function $\\\\chi = m_iq_i^{-1} B^{-2}$div($\\\\mathbf{E}$), which represents a general diagnostic tool for processes of energy transfer between electromagnetic fields and particles, and the measure of the local charge non-neutrality. The identification was made with multipoint measurements obtained from the Magnetospheric Multiscale spacecraft (MMS). The source for the LHD instability is the diamagnetic drift of ions, and for the ECD instability the source is ExB drift of electrons. The conclusions are supported by laboratory diagnostics of the ECD instability in Hall ion thrusters.',\n",
       "  'len': 1187},\n",
       " {'abstract': 'Optical cameras are gaining popularity as the suitable sensor for relative navigation in space due to their attractive sizing, power and cost properties when compared to conventional flight hardware or costly laser-based systems. However, a camera cannot infer depth information on its own, which is often solved by introducing complementary sensors or a second camera. In this paper, an innovative model-based approach is instead demonstrated to estimate the six-dimensional pose of a target object relative to the chaser spacecraft using solely a monocular setup. The observed facet of the target is tackled as a classification problem, where the three-dimensional shape is learned offline using Gaussian mixture modeling. The estimate is refined by minimizing two different robust loss functions based on local feature correspondences. The resulting pseudo-measurements are then processed and fused with an extended Kalman filter. The entire optimization framework is designed to operate directly on the $SE\\\\text{(3)}$ manifold, uncoupling the process and measurement models from the global attitude state representation. It is validated on realistic synthetic and laboratory datasets of a rendezvous trajectory with the complex spacecraft Envisat. It is demonstrated how it achieves an estimate of the relative pose with high accuracy over its full tumbling motion.',\n",
       "  'len': 1380},\n",
       " {'abstract': 'This paper is aimed at finding the best separation angle between spacecraft for the three-dimensional reconstruction of solar-wind inhomogeneous structures by the CORrelation-Aided Reconstruction(CORAR) method. The analysis is based on the dual-point heliospheric observations from the STEREO HI-1 cameras. We produced synthetic HI-1 white-light images containing artificial blob-like structures in different positions in the common field of view of the two HI-1 cameras and reconstruct the structures with CORAR method. The distributions of performance levels of the reconstruction for spacecraft separation of $60^{\\\\circ}$, $90^{\\\\circ}$, $120^{\\\\circ}$ and $150^{\\\\circ}$ are obtained. It is found that when the separation angle is $120^{\\\\circ}$, the performance of the reconstruction is the best and the separation angle of $90^{\\\\circ}$ is the next. A brief discussion of the results are given as well. Based on this study, we suggest the optimal layout scheme of the recently proposed Solar Ring mission, which is designed to routinely observe the Sun and the inner heliosphere from multiple perspectives in the ecliptic plane.',\n",
       "  'len': 1140},\n",
       " {'abstract': \"The Cassini spacecraft's last orbits directly sampled Saturn's thermosphere and revealed a much more chemically complex environment than previously believed. Observations from the Ion and Neutral Mass Spectrometer (INMS) aboard Cassini provided compositional measurements of this region and found an influx of material raining into Saturn's upper atmosphere from the rings. We present here an in-depth analysis of the CH$_4$, H$_2$O, and NH$_3$ signal from INMS and provide further evidence of external material entering Saturn's atmosphere from the rings. We use a new mass spectral deconvolution algorithm to determine the amount of each species observed in the spectrum and use these values to determine the influx and mass deposition rate for these species.\",\n",
       "  'len': 772},\n",
       " {'abstract': 'Large magnetic structures are launched away from the Sun during solar eruptions. They are observed as (interplanetary) coronal mass ejections (ICMEs or CMEs) with coronal and heliospheric imagers. A fraction of them are observed insitu as magnetic clouds (MCs). Fitting these structures properly with a model requires a better understanding of their evolution. In situ measurements are done locally when the spacecraft trajectory crosses the magnetic configuration. These observations are taken for different elements of plasma and at different times, and are therefore biased by the expansion of the magnetic configuration. This aging effect leads to stronger magnetic fields measured at the front than at the rear of MCs, an asymmetry often present in MC data. However, can the observed asymmetry be explained quantitatively only from the expansion? Based on self-similar expansion, we derive a method to estimate the expansion rate from observed plasma velocity. We next correct for the aging effect both the observed magnetic field and the spatial coordinate along the spacecraft trajectory. This provides corrected data as if the MC internal structure was observed at the same time. We apply the method to 90 best observed MCs near Earth (1995-2012). The aging effect is the main source of the observed magnetic asymmetry only for 28\\\\% of MCs. After correcting the aging effect, the asymmetry is almost symmetrically distributed between MCs with a stronger magnetic field at the front and those at the rear of MCs. The proposed method can efficiently remove the aging bias within insitu data of MCs, and more generally of ICMEs. This allows one to analyse the data with a spatial coordinate, such as in models or remote sensing observations.',\n",
       "  'len': 1757},\n",
       " {'abstract': 'World ships are hypothetical, large, self-contained spacecraft for crewed interstellar travel, taking centuries to reach other stars. Due to their crewed nature, size, and long trip times, the feasibility of world ships faces an additional set of challenges compared to interstellar probes. Despite their emergence in the 1980s, most of these topics remain unexplored. This article revisits some of the key feasibility issues of world ships. First, definitions of world ships from the literature are revisited and the notion of world ship positioned with respect to similar concepts such as generation ships. Second, the key question of population size is revisited in light of recent results from the literature. Third, socio-technical and economic feasibility issues are evaluated. Finally, world ships are compared to potential alternative modes of crewed interstellar travel. Key roadblocks for world ships are the considerable resources required, shifting its economic feasibility beyond the year 2300, and the development of a maintenance system capable of detecting, replacing, and repairing several components per second. The emergence of alternative, less costly modes of crewed interstellar travel at an earlier point in time might render world ships obsolete.',\n",
       "  'len': 1281},\n",
       " {'abstract': \"The European Space Agency's Solar Orbiter spacecraft will pass approximately downstream of the position of comet C/2019 Y4 (ATLAS) in late May and early June 2020. We predict that the spacecraft may encounter the comet's ion tail around 2020 May 31-June 1, and that the comet's dust tail may be crossed on 2020 June 6. We outline the solar wind features and dust grain collisions that the spacecraft's instruments may detect when crossing the comet's two tails. Solar Orbiter will also pass close to the orbital path of C/2020 F8 (SWAN) on 2020 May 22, but we believe that it is unlikely to detect any material associated with that comet.\",\n",
       "  'len': 649},\n",
       " {'abstract': 'We address the effect of particle scattering on the energy spectra of solar energetic electron events using i) an observational and ii) a modeling approach. i) We statistically study observations of the STEREO spacecraft making use of directional electron measurements made with the SEPT instrument in the range of 45 -- 425 keV. We compare the energy spectra of the anti-sunward propagating beam with that one of the backward scattered population and find that, on average, the backward scattered population shows a harder spectrum with the effect being stronger at higher energies. ii) We use a numerical SEP transport model to simulate the effect of particle scattering (both in terms of pitch-angle and perpendicular to the mean field) on the spectrum. We find that pitch-angle scattering can lead to spectral changes at higher energies (E $>100$ keV) and further away from the Sun (r $> 1$ au) which are also often observed. At lower energies, and closer to the Sun the effect of pitch-angle scattering is much reduced so that the simulated energy spectra still resemble the injected power-law functions. When examining pitch-angle dependent spectra, we find, in agreement with the observational results, that the spectra of the backward propagating electrons are harder than that of the forward (from the Sun) propagating population. {We conclude that {\\\\it Solar Orbiter} and {\\\\it Parker Solar Probe} will be able to observe the unmodulated omni-directional SEP electron spectrum close to the Sun at higher energies, giving a direct indication of the accelerated spectrum. }',\n",
       "  'len': 1591},\n",
       " {'abstract': 'LISA Pathfinder (LPF) has been a space-based mission designed to test new technologies that will be required for a gravitational wave observatory in space. Magnetically driven forces play a key role in the instrument sensitivity in the low-frequency regime (mHz and below), the measurement band of interest for a space-based observatory. The magnetic field can couple to the magnetic susceptibility and remanent magnetic moment from the test masses and disturb them from their geodesic movement. LISA Pathfinder carried on-board a dedicated magnetic measurement subsystem with noise levels of 10 $ \\\\rm nT \\\\ Hz^{-1/2}$ from 1 Hz down to 1 mHz. In this paper we report on the magnetic measurements throughout LISA Pathfinder operations. We characterise the magnetic environment within the spacecraft, study the time evolution of the magnetic field and its stability down to 20 $\\\\mu$Hz, where we measure values around 200 $ \\\\rm nT \\\\ Hz^{-1/2}$ and identify two different frequency regimes, one related to the interplanetary magnetic field and the other to the magnetic field originating inside the spacecraft. Finally, we characterise the non-stationary component of the fluctuations of the magnetic field below the mHz and relate them to the dynamics of the solar wind.',\n",
       "  'len': 1278},\n",
       " {'abstract': 'Recent studies of optimization methods and GNC of spacecraft near small bodies focusing on descent, landing, rendezvous, etc., with key safety constraints such as line-of-sight conic zones and soft landings have shown promising results; this paper considers descent missions to an asteroid surface with a constraint that consists of an onboard camera and asteroid surface markers while using a stochastic convex MPC law. An undermodeled asteroid gravity and spacecraft technology inspired measurement model is established to develop the constraint. Then a computationally light stochastic Linear Quadratic MPC strategy is presented to keep the spacecraft in satisfactory field of view of the surface markers while trajectory tracking, employing chance based constraints and up-to-date estimation uncertainty from navigation. The estimation uncertainty giving rise to the tightened constraints is particularly addressed. Results suggest robust tracking performance across a variety of trajectories.',\n",
       "  'len': 1008},\n",
       " {'abstract': 'In this paper, we introduce a laboratory prototype of a solar energetic particle (SEP) detector which will operate along with other space-based instruments to give us more insight into the SEP physics. The instrument is designed to detect protons and electrons with kinetic energies from 10 to 100 MeV and from 1 to 10 MeV respectively. The detector is based on a scintillation cylinder divided into separated disks to get more information about detected particles. Scintillation light from isolated segments is collected by optical fibers and registered with silicon photo-multipliers (SiPM). The work contains the result of laboratory testing of the detector prototype. The detector channels were calibrated, energy resolution for every channel was obtained. Moreover, we present an advanced integral data acquisition and analysis technique based on Bayesian statistics, which will allow operation even during SEP events with very large fluxes. The work is motivated by the need for better measurement tools to study acceleration and transport of SEP in the heliosphere as well as by the need for the monitoring tool to mitigate radiation hazard for equipment and people in space.',\n",
       "  'len': 1193},\n",
       " {'abstract': 'The Laser Interferometer Space Antenna (LISA) is a European Space Agency mission that aims to measure gravitational waves in the millihertz range. The three-spacecraft constellation forms a nearly-equilateral triangle, which experiences flexing along its orbit around the Sun. These time-varying and unequal armlengths require to process measurements with time-delay interferometry (TDI) to synthesize virtual equal-arm interferometers, and reduce the otherwise overwhelming laser frequency noise. Algorithms compatible with such TDI combinations have recently been proposed in order to suppress the phase fluctuations of the onboard ultra-stable oscillators (USO) used as reference clocks. In this paper, we propose a new method to cancel USO noise in TDI combinations. This method has comparable performance to existing algorithms, but is more general as it can be applied to most TDI combinations found in the literature. We compute analytical expressions for the residual clock noise before and after correction, accounting for the effect of time-varying beatnote frequencies, previously neglected. We present results of numerical simulations that are in agreement with our models, and show that clock noise can be suppressed below required levels. The suppression algorithm introduces a new modulation noise, for which we propose a partial mitigation. This modulation noise remains the limiting effect for clock-noise suppression, setting strict timing requirements on the sideband generation.',\n",
       "  'len': 1509},\n",
       " {'abstract': \"The Alice UV spectrograph aboard NASA's New Horizons mission is sensitive to MeV electrons that penetrate the instrument's thin aluminum housing and interact with its microchannel plate detector. We have searched for penetrating electrons at heliocentric distance of 2-45 AU, finding no evidence of discrete events outside of the Jovian magnetosphere. However, we do find a gradual long-term increase in the Alice instrument's global dark count rate at a rate of ~1.5% per year, which may be caused by a heightened gamma-ray background from aging of the spacecraft's radioisotope thermoelectric generator fuel. If this hypothesis is correct, then the Alice instrument's global dark count rate should flatten and then decrease over the next 5-10 years.\",\n",
       "  'len': 762},\n",
       " {'abstract': 'On September 10 2017, a fast coronal mass ejection (CME) erupted from the active region (AR) 12673, leading to a ground level enhancement (GLE) event at Earth. Using the 2D improved Particle Acceleration and Transport in the Heliosphere (iPATH) model, we model the large solar energetic particle (SEP) event of 10 September 2017 observed at Earth, Mars and STEREO-A. Based on observational evidence,we assume that the CME-driven shock experienced a large lateral expansion shortly after the eruption, which is modelled by a double Gaussian velocity profile in this simulation. We use the in-situ shock arrival times and the observed CME speeds at multiple spacecraft near Earth and Mars as constraints to adjust the input model parameters. The modelled time intensity profiles and fluence for energetic protons are then compared with observations. Reasonable agreements with observations at Mars and STEREO-A are found. The simulated results at Earth differ from observations of GOES-15. Instead, the simulated results at a heliocentric longitude 20 degree west to Earth fit reasonably well with the GOES observation. This can be explained if the pre-event solar wind magnetic field at Earth is not described by a nominal Parker field. Our results suggest that a large lateral expansion of the CME-driven shock and a distorted interplanetary magnetic field due to previous events can be important in understanding this GLE event.',\n",
       "  'len': 1440},\n",
       " {'abstract': 'In this paper, we consider the controlled rigid spacecraft with an internal rotor as a regular point reducible regular controlled Hamiltonian (RCH) system. In the cases of coincident and non-coincident centers of buoyancy and gravity, we first give the regular point reduction and the dynamical vector field of the reduced controlled rigid spacecraft-rotor system, respectively. Then, we derive precisely the geometric constraint conditions of the reduced symplectic form for the dynamical vector field of the regular point reducible controlled spacecraft-rotor system, that is, the two types of Hamilton-Jacobi equation for the reduced controlled spacecraft-rotor system by calculation in detail. These researches reveal the deeply internal relationships of the geometrical structures of phase spaces, the dynamical vector fields and controls of the system.',\n",
       "  'len': 869},\n",
       " {'abstract': 'We introduce a new capability of the Neil Gehrels Swift Observatory, to provide event data from the Burst Alert Telescope (BAT) on demand in response to transients detected by other instruments. These event data are not continuously available due to the large telemetry load, but are critical to recovering weak or sub-threshold GRBs that are not triggered onboard, such as the likely counterparts to GW-detected off-axis binary neutron star mergers. We show that the availability of event data can effectively increase the rate of detections, and arcminute localizations, of GRB 170817-like bursts by >400%. We describe a spacecraft commanding pipeline purpose built to enable this science; to our knowledge the first fully autonomous extremely-low-latency commanding of a space telescope for scientific purposes. This pipeline has been successfully run in its complete form since early 2020, and has resulted in the recovery of BAT event data for >700 externally triggered events to date (GWs, neutrinos, GRBs triggered by other facilities, FRBs), now running with a success rate of ~90%. We exemplify the utility of this new capability by using the resultant data to (1) set the most sensitive (8 sigma) upper limits of $8.1\\\\times10^{-8}$ erg cm$^{-2}$ s$^{-1}$ (14-195 keV) on prompt 1s duration short GRB-like emission within $\\\\pm$ 15s of the unmodelled GW burst candidate S200114f, and (2) provide arcminute localizations for short GRB 200325A and other bursts. We show that using data from GUANO to localize GRBs discovered elsewhere, we can increase the net rate of arcminute localized GRBs by 10-20% per year. Along with the scientific yield of more sensitive searches for sub-threshold GRBs, the new capabilities designed for this project will enable further rapid response Target of Opportunity capabilities for Swift, and have implications for the design of future rapid-response space telescopes.',\n",
       "  'len': 1920},\n",
       " {'abstract': \"Magnetic activity of the Sun and other stars causes their brightness to vary. We investigate how typical the Sun's variability is compared to other solar-like stars, i.e. those with near-solar effective temperatures and rotation periods. By combining four years of photometric observations from the Kepler space telescope with astrometric data from the Gaia spacecraft, we measure photometric variabilities of 369 solar-like stars. Most of the solar-like stars with well-determined rotation periods show higher variability than the Sun and are therefore considerably more active. These stars appear nearly identical to the Sun, except for their higher variability. Their existence raises the question of whether the Sun can also experience epochs of such high variability.\",\n",
       "  'len': 783},\n",
       " {'abstract': \"The asymmetric gravity field measured by the Juno spacecraft allowed estimation of the depth of Jupiter's zonal jets, showing that the winds extend approximately $3000$ km beneath the cloud-level. This estimate was based on an analysis using a combination of all measured odd gravity harmonics $J_{3}$, $J_{5}$, $J_{7}$, and $J_{9}$, but the wind profile dependence on each of them separately has not been investigated. Furthermore, these calculations assumed the meridional profile of the cloud-level wind extends to depth. However, it is possible that the interior jet profile varies from that of the cloud-level as hinted by the Juno microwave measurement that find a smoother nadir brightness temperature profile at depth compared to the cloud-level. Here we analyze in detail the possible meridional and vertical structure of Jupiter's deep jet-streams. We find that each odd gravity harmonic constrains the flow at a different depth, with $J_{3}$ being the most dominant at depths below $3000$ km, $J_{5}$ being the most restrictive overall, and $J_{9}$ not constraining the flow at all if the other odd harmonics are within the measurement range. Interior flow profiles constructed from perturbations to the cloud-level winds allow a more extensive range of vertical wind profiles, yet when the profiles differ substantially from the cloud-level, the ability to match the gravity data reduces significantly. Overall, we find that while interior wind profiles that do not resemble the cloud-level are possible, they are statistically unlikely. However, slightly smoother profiles, which resemble the Juno's microwave radiometer temperature profile at depth, are still compatible with the gravity measurements.\",\n",
       "  'len': 1726},\n",
       " {'abstract': \"White-light images from Heliospheric Imager-1 (HI1) onboard the Solar Terrestrial Relations Observatory (STEREO) provide 2-dimensional (2D) global views of solar wind transients traveling in the inner heliosphere from two perspectives. How to retrieve the hidden three-dimensional (3D) features of the transients from these 2D images is intriguing but challenging. In our previous work (Li et al., 2018), a 'correlation-aided' method is developed to recognize the solar wind transients propagating along the Sun-Earth line based on simultaneous HI1 images from two STEREO spacecraft. Here the method is extended from the Sun-Earth line to the whole 3D space to reconstruct the solar wind transients in the common field of view of STEREO HI1 cameras. We demonstrate the capability of the method by showing the 3D shapes and propagation directions of a coronal mass ejection (CME) and three small-scale blobs during 3-4 April 2010. Comparing with some forward modeling methods, we found our method reliable in terms of the position, angular width and propagation direction. Based on our 3D reconstruction result, an angular distorted, nearly North-South oriented CME on 3 April 2010 is revealed, manifesting the complexity of a CME's 3D structure.\",\n",
       "  'len': 1256},\n",
       " {'abstract': \"Planet 9 was proposed as an explanation for the clustering of orbits for some trans-Neuptunian objects. Recently, the use of a sub-relativistic spacecraft was proposed to indirectly probe Planet 9's gravitational influence. Here we study the effects of the drag and electromagnetic forces exerted on a sub-relativistic spacecraft by the interstellar medium (ISM) and compare these forces with the gravitational force induced by Planet 9. We find that the resulting noise due to density and magnetic fluctuations would dominate over Planet 9's gravitational signal at sub-relativistic speeds, $v\\\\gtrsim 0.001~c$. We then identify the parameter space required to overcome the drag and magnetic noise from the ISM turbulence and enable the detection of Planet 9's gravity. Finally, we discuss practical strategies to mitigate the effect of the drag and electromagnetic forces.\",\n",
       "  'len': 884},\n",
       " {'abstract': 'As the solar wind propagates through the heliosphere, dynamical processes irreversibly erase the signatures of the near-Sun heating and acceleration processes. The elemental fractionation of the solar wind should not change during transit however, making it an ideal tracer of these processes. We aimed to verify directly if the solar wind elemental fractionation is reflective of the coronal source region fractionation, both within and across different solar wind source regions. A backmapping scheme was used to predict where solar wind measured by the Advanced Composition Explorer (ACE) originated in the corona. The coronal composition measured by the Hinode Extreme ultraviolet Imaging Spectrometer (EIS) at the source regions was then compared with the in-situ solar wind composition. On hourly timescales there was no apparent correlation between coronal and solar wind composition. In contrast, the distribution of fractionation values within individual source regions was similar in both the corona and solar wind, but distributions between different sources have significant overlap. The matching distributions directly verifies that elemental composition is conserved as the plasma travels from the corona to the solar wind, further validating it as a tracer of heating and acceleration processes. The overlap of fractionation values between sources means it is not possible to identify solar wind source regions solely by comparing solar wind and coronal composition measurements, but a comparison can be used to verify consistency with predicted spacecraft-corona connections.',\n",
       "  'len': 1602},\n",
       " {'abstract': 'A recent proposal for the detection of a hypothetical gravitating body 500 AU from the Sun (termed Planet 9) calls for a fleet of near-relativistic spacecraft, equipped with high-precision clocks, to be sent to a region where the object is suspected to be. We show that the technological constraints of such a mission can be relaxed somewhat, while improving the sensitivity: high-precision clocks can be avoided when the transverse displacement induced by Planet 9 is measurable with Earth-based, or near-Earth, telescopes. Furthermore, we note that in the absence of Planet 9, these spacecraft still yield useful data by mapping gravitational perturbations in the outer parts of the solar system.',\n",
       "  'len': 709},\n",
       " {'abstract': 'There are hints of a novel object (\"Planet 9\") with a mass $5-10$ $M_\\\\oplus$ in the outer Solar System, at a distance of order 500 AU. If it is a relatively conventional planet, it can be found in telescopic searches. Alternatively, it has been suggested that this body might be a primordial black hole (PBH). In that case, conventional searches will fail. A possible alternative is to probe the gravitational field of this object using small, laser-launched spacecraft, like the ones envisioned in the Breakthrough Starshot project. With a velocity of order $.001~c$, such spacecraft can reach Planet 9 roughly a decade after launch and can discover it if they can report timing measurements accurate to $10^{-5}$ seconds back to Earth.',\n",
       "  'len': 748},\n",
       " {'abstract': 'This paper proposes a solution for multiple-impulse orbital maneuvers near circular orbits for special cases where orbital observations are not globally available and the spacecraft is being observed through a limited window from a ground or a space-based station. The current study is particularly useful for small private launching companies with limited access to global observations around the Earth and/or for orbital maneuvers around other planets for which the orbital observations are limited to the in situ equipment. An appropriate cost function is introduced for the sake of minimizing the total control/impulse effort as well as the orbital uncertainty. It is subsequently proved that for a circle-to-circle maneuver, the optimization problem is quasi-convex with respect to the design variables. For near circular trajectories the same cost function is minimized via a gradient based optimization algorithm in order to provide a sub-optimal solution that is efficient both with respect to energy effort and orbital uncertainty. As a relevant case study, a four-impulse orbital maneuver between circular orbits under Mars gravitation is simulated and analyzed to demonstrate the effectiveness of the proposed algorithm.',\n",
       "  'len': 1242},\n",
       " {'abstract': 'Pathfinding in Euclidean space is a common problem faced in robotics and computer games. For long-distance navigation on the surface of the earth or in outer space however, approximating the geometry as Euclidean can be insufficient for real-world applications such as the navigation of spacecraft, aeroplanes, drones and ships. This article describes an any-angle pathfinding algorithm for calculating the shortest path between point pairs over the surface of a sphere. Introducing several novel adaptations, it is shown that Anya as described by (Harabor & Grastien, 2013) for Euclidean space can be extended to Spherical geometry. There, where the shortest-distance line between coordinates is defined instead by a great-circle path, the optimal solution is typically a curved line in Euclidean space. In addition the turning points for optimal paths in Spherical geometry are not necessarily corner points as they are in Euclidean space, as will be shown, making further substantial adaptations to Anya necessary. Spherical Anya returns the optimal path on the sphere, given these different properties of world maps defined in Spherical geometry. It preserves all primary benefits of Anya in Euclidean geometry, namely the Spherical Anya algorithm always returns an optimal path on a sphere and does so entirely on-line, without any preprocessing or large memory overheads. Performance benchmarks are provided for several game maps including Starcraft and Warcraft III as well as for sea navigation on Earth using the NOAA bathymetric dataset. Always returning the shorter path compared with the Euclidean approximation yielded by Anya, Spherical Anya is shown to be faster than Anya for the majority of sea routes and slower for Game Maps and Random Maps.',\n",
       "  'len': 1771},\n",
       " {'abstract': \"During the New Horizons spacecraft's encounter with Pluto, the Alice ultraviolet spectrograph conducted a series of observations that detected emissions from both the interplanetary medium (IPM) and Pluto. In the direction of Pluto, the IPM was found to be 133.4$\\\\pm$0.6R at Lyman $\\\\alpha$, 0.24$\\\\pm$0.02R at Lyman $\\\\beta$, and <0.10R at He I 584Å. We analyzed 3,900s of data obtained shortly before closest approach to Pluto and detect airglow emissions from H I, N I, N II, N$_2$, and CO above the disk of Pluto. We find Pluto's brightness at Lyman $\\\\alpha$ to be $29.3\\\\pm1.9$R, in good agreement with pre-encounter estimates. The detection of the N II multiplet at 1085Å marks the first direct detection of ions in Pluto's atmosphere. We do not detect any emissions from noble gasses and place a 3$\\\\sigma$ upper limit of 0.14 R on the brightness of the Ar I 1048Å line. We compare pre-encounter model predictions and predictions from our own airglow model, based on atmospheric profiles derived from the solar occultation observed by New Horizons, to the observed brightness of Pluto's airglow. Although completely opaque at Lyman $\\\\alpha$, Pluto's atmosphere is optically thin at wavelengths longer than 1425Å. Consequently, a significant amount of solar FUV light reaches the surface, where it can participate in space weathering processes. From the brightness of sunlight reflected from Pluto, we find the surface has a reflectance factor (I/F) of 17% between 1400-1850Å. We also report the first detection of an C$_3$ hydrocarbon molecule, methylacetylene, in absorption, at a column density of ~5$\\\\times10^{15}$ cm$^{-2}$, corresponding to a column-integrated mixing ratio of $1.6\\\\times10^{-6}$.\",\n",
       "  'len': 1714},\n",
       " {'abstract': 'We revisit a multi-spacecraft study of the element abundances of solar energetic particles (SEPs) in the 23 January 2012 event, where the power-law pattern of enhancements versus the mass-to-charge ratio A/Q for the elements C through Fe was partly disrupted by a break near Mg, which turned out to be an unfortunate distraction. In the current article we find that extending that least-squares fits for C - Fe down to H at A/Q = 1 lends much more credence to the power laws, even though H itself was not included in the fits. We also investigate the extent of an adiabatically invariant \"reservoir\" of magnetically-trapped particles behind the shock wave in this event.',\n",
       "  'len': 681},\n",
       " {'abstract': 'The Eulerian space-time correlation of strong Magnetohydrodynamic (MHD) turbulence in strongly magnetized plasmas is investigated by means of direct numerical simulations of Reduced MHD turbulence and phenomenological modeling. Two new important results follow from the simulations: 1) counter-propagating Alfvénic fluctuations at a each scale decorrelate in time at the same rate in both balanced and imbalanced turbulence; and 2) the scaling with wavenumber of the decorrelation rate is consistent with pure hydrodynamic sweeping of small-scale structures by the fluctuating velocity of the energy-containing scales. An explanation of the simulation results is proposed in the context of a recent phenomenological MHD model introduced by Bourouaine and Perez 2019 (BP19) when restricted to the strong turbulence regime. The model predicts that the two-time power spectrum exhibits an universal, self-similar behavior that is solely determined by the probability distribution function of random velocities in the energy-containing range. Understanding the scale-dependent temporal evolution of the space-time turbulence correlation as well as its associated universal properties is essential in the analysis and interpretation of spacecraft observations, such as the recently launched Parker Solar Probe (PSP).',\n",
       "  'len': 1322},\n",
       " {'abstract': \"We use Reinforcement Meta-Learning to optimize an adaptive integrated guidance, navigation, and control system suitable for exoatmospheric interception of a maneuvering target. The system maps observations consisting of strapdown seeker angles and rate gyro measurements directly to thruster on-off commands. Using a high fidelity six degree-of-freedom simulator, we demonstrate that the optimized policy can adapt to parasitic effects including seeker angle measurement lag, thruster control lag, the parasitic attitude loop resulting from scale factor errors and Gaussian noise on angle and rotational velocity measurements, and a time varying center of mass caused by fuel consumption and slosh. Importantly, the optimized policy gives good performance over a wide range of challenging target maneuvers. Unlike previous work that enhances range observability by inducing line of sight oscillations, our system is optimized to use only measurements available from the seeker and rate gyros. Through extensive Monte Carlo simulation of randomized exoatmospheric interception scenarios, we demonstrate that the optimized policy gives performance close to that of augmented proportional navigation with perfect knowledge of the full engagement state. The optimized system is computationally efficient and requires minimal memory, and should be compatible with today's flight processors.\",\n",
       "  'len': 1396},\n",
       " {'abstract': 'Simultaneous observation of characteristic 3-dimensional (3D) signatures in the electron velocity distribution function (VDF) and intense quasi-monochromatic waves by the Magnetospheric Multiscale (MMS) spacecraft in the terrestrial magnetosheath are investigated. The intense wave packets are characterised and modeled analytically as quasi-parallel circularly-polarized whistler waves and applied to a test-particle simulation in view of gaining insight into the signature of the wave-particle resonances in velocity space. Both the Landau and the cyclotron resonances were evidenced in the test-particle simulations. The location and general shape of the test-particle signatures do account for the observations, but the finer details, such as the symmetry of the observed signatures are not matched, indicating either the limits of the test-particle approach, or a more fundamental physical mechanism not yet grasped. Finally, it is shown that the energisation of the electrons in this precise resonance case cannot be diagnosed using the moments of the distribution function, as done with the classical ${\\\\bf E}.{\\\\bf J}$ \"dissipation\" estimate.',\n",
       "  'len': 1160},\n",
       " {'abstract': \"The Kepler spacecraft observed a total of only four AM Herculis cataclysmic variable stars during its lifetime. We analyze the short-cadence K2 light curve of one of those systems, Tau 4 (RX J0502.8+1624), which underwent a serendipitous jump from a low-accretion state into a high state during the final days of the observation. Apart from one brief flare, there was no evidence of accretion during the 70 d of observations of the low state. As Tau 4 transitioned into a high state, the resumption of accretion was very gradual, taking approximately six days (~90 binary orbits). We supplement Tau 4's K2 light curve with time-resolved spectroscopy obtained in both high and low states of accretion. High-excitation lines, such as He II 468.6 nm, were extraordinarily weak, even when the system was actively accreting. This strongly suggests the absence of an accretion shock, placing Tau 4 in the bombardment regime predicted for AM Herculis systems with low accretion rates. In both the high-state and low-state spectra, Zeeman absorption features from the white dwarf's photosphere are present and reveal a surface-averaged field strength of $15\\\\pm2$ MG. Remarkably, the high-state spectra also show Zeeman-split emission lines produced in a region with a field strength of $12\\\\pm1$ MG. Zeeman emission has not been previously reported in an AM Herculis system, and we propose that the phenomenon is caused by a temperature inversion in the WD's atmosphere near the accretion region.\",\n",
       "  'len': 1498},\n",
       " {'abstract': 'In this paper, attitude maneuver control without unwinding phenomenon is investigated for rigid spacecraft. First, a novel switching function is constructed by a hyperbolic sine function. It is shown that the spacecraft system possesses the unwinding-free performance when the system states are on the sliding surface. Based on the designed switching function, a sliding mode controller is developed to ensure the robustness of the attitude maneuver control system. Another essential feature of the presented attitude control law is that a dynamic parameter is introduced to guarantee the unwinding-free performance when the system states are outside the sliding surface. The simulation results demonstrate that the unwinding phenomenon is avoided during the attitude maneuver of a rigid spacecraft by adopting the constructed switching function and the proposed attitude control scheme.',\n",
       "  'len': 898},\n",
       " {'abstract': 'We anticipate noise from the Laser Interferometer Space Antenna (LISA) will exhibit nonstationarities throughout the duration of its mission due to factors such as antenna repointing, cyclostationarities from spacecraft motion, and glitches as highlighted by LISA Pathfinder. In this paper, we use a surrogate data approach to test the stationarity of a time series which does not rely on the Gaussianity assumption. The main goal is to identify noise nonstationarities in the future LISA mission. This will be necessary for determining how often the LISA noise power spectral density (PSD) will need to be updated for parameter estimation routines. We conduct a thorough simulation study illustrating the power/size of various versions of the hypothesis tests, and then apply these approaches to differential acceleration measurements from LISA Pathfinder. We also develop a data analysis strategy for addressing nonstationarities in the LISA PSD, where we update the noise PSD over time, while simultaneously conducting parameter estimation, with a focus on planned data gaps.',\n",
       "  'len': 1089},\n",
       " {'abstract': \"Global-scale energy flow throughout Earth's magnetosphere (MSP) is catalyzed by processes that occur at Earth's magnetopause (MP). Magnetic reconnection is one process responsible for solar wind entry into and global convection within the MSP, and the MP location, orientation, and motion have an impact on the dynamics. Statistical studies that focus on these and other MP phenomena and characteristics inherently require MP identification in their event search criteria, a task that can be automated using machine learning. We introduce a Long-Short Term Memory (LSTM) Recurrent Neural Network model to detect MP crossings and assist studies of energy transfer into the MSP. As its first application, the LSTM has been implemented into the operational data stream of the Magnetospheric Multiscale (MMS) mission. MMS focuses on the electron diffusion region of reconnection, where electron dynamics break magnetic field lines and plasma is energized. MMS employs automated burst triggers onboard the spacecraft and a Scientist-in-the-Loop (SITL) on the ground to select intervals likely to contain diffusion regions. Only low-resolution data is available to the SITL, which is insufficient to resolve electron dynamics. A strategy for the SITL, then, is to select all MP crossings. Of all 219 SITL selections classified as MP crossings during the first five months of model operations, the model predicted 166 (76%) of them, and of all 360 model predictions, 257 (71%) were selected by the SITL. Most predictions that were not classified as MP crossings by the SITL were still MP-like; the intervals contained mixed magnetosheath and magnetospheric plasmas. The LSTM model and its predictions are public to ease the burden of arduous event searches involving the MP, including those for EDRs. For MMS, this helps free up mission operation costs by consolidating manual classification processes into automated routines.\",\n",
       "  'len': 1930},\n",
       " {'abstract': 'In this paper, anti-unwinding attitude maneuver control for rigid spacecraft is considered. First, in order to avoid the unwinding phenomenon when the system states are restricted to the switching surface, a novel switching function is constructed by hyperbolic sine functions such that the switching surface contains two equilibriums. Then, a sliding mode attitude maneuver controller is designed based on the constructed switching function to ensure the robustness of the closed-loop attitude maneuver control system to disturbance. Another important feature of the developed attitude control law is that a dynamic parameter is introduced to guarantee the anti-unwinding performance before the system states reach the switching surface. The simulation results demonstrate that the unwinding problem is settled during attitude maneuver for rigid spacecraft by adopting the newly constructed switching function and proposed attitude control scheme.',\n",
       "  'len': 959},\n",
       " {'abstract': 'Weakly collisional space plasmas are rarely in local thermal equilibrium and often exhibit non-Maxwellian electron and ion velocity distributions that lead to the growth of microinstabilities, that is, enhanced electric and magnetic fields at relatively short wavelengths. These instabilities play an active role in the evolution of space plasmas, as does ubiquitous broadband turbulence induced by turbulent structures. This study compares certain properties of a 2.5 dimensional Particle-In-Cell (PIC) simulation for the forward cascade of Alfvenic turbulence in a collisionless plasma against the same properties of turbulence observed by the Magnetospheric Multiscale Mission spacecraft in the terrestrial magnetosheath. The PIC simulation is of decaying turbulence which develops both coherent structures and anisotropic ion velocity distributions with the potential to drive kinetic scale instabilities. The uniform background magnetic field points perpendicular to the plane of the simulation. Growth rates are computed from linear theory using the ion temperature anisotropies and ion beta values for both the simulation and the observations. Both the simulation and the observations show that strong anisotropies and growth rates occur highly intermittently in the plasma, and the simulation further shows that such anisotropies preferentially occur near current sheets. This suggests that, though microinstabilities may affect the plasma globally , they act locally and develop in response to extreme temperature anisotropies generated by turbulent structures. Further studies will be necessary to understand why there is an apparent correlation between linear instability theory and strongly intermittent turbulence.',\n",
       "  'len': 1738},\n",
       " {'abstract': 'The presence of liquid water at the base of the Martian polar caps has long been suspected but not observed. We surveyed the Planum Australe region using the Mars Advanced Radar for Subsurface and Ionosphere Sounding, a low-frequency radar on the Mars Express spacecraft. Radar profiles collected between May 2012 and December 2015, contain evidence of liquid water trapped below the ice of the South Polar Layered Deposits. Anomalously bright subsurface reflections were found within a well-defined, 20km wide zone centered at 193°E, 81°S, surrounded by much less reflective areas. Quantitative analysis of the radar signals shows that this bright feature has high dielectric permittivity >15, matching water-bearing materials. We interpret this feature as a stable body of liquid water on Mars.',\n",
       "  'len': 807},\n",
       " {'abstract': 'The aim of the Laser Interferometer Space Antenna (LISA) is to detect gravitational waves through a phase modulation in long (2.5 Mkm) laser light links between spacecraft. Among other noise sources to be addressed are the phase fluctuations caused by a possible angular jitter of the emitted beam. The present paper follows our preceding one (Vinet et al 2019 Class. Quant. Grav. 36, 205 003) based on an analytical study of the far field phase. We address here a numerical treatment of the phase, to first order in the emitted wavefront aberrations, but without any assumptions on the static bias term. We verify that, in the phase change, the higher order terms in the static mispointing are consistent with the results found in our preceding paper.',\n",
       "  'len': 763},\n",
       " {'abstract': 'The Bent Crystal Spectrometer (BCS) on the NASA Solar Maximum Mission spacecraft observed the X-ray spectra of numerous solar flares during the periods 1980 February to November and 1984~--~1989. The instrument, the first of its kind to use curved crystal technology, observed the resonance lines of He-like Ca (\\\\caxix) and Fe (\\\\fexxv) and neighboring satellite lines, allowing the study of the rapid evolution of flare plasma temperature, turbulence, mass motions etc. To date there has not been a solar X-ray spectrometer with comparable spectral and time resolution, while subsequent solar cycles have delivered far fewer and less intense flares. The BCS data archive thus offers an unparalleled resource for flare studies. A recent re-assessment of the BCS calibration and its operations is extended here by using data during a spacecraft scan in the course of a flare on 1980 November~6 that highlights small deformations in the crystal curvature of the important channel~1 (viewing lines of \\\\caxix\\\\ and satellites). The results explain long-standing anomalies in spectral line ratios which have been widely discussed in the past. We also provide an in-flight estimation of the BCS collimator field of view which improves the absolute intensity calibration of the BCS. The BCS channel~1 background is shown to be entirely due to solar continuum radiation, confirming earlier analyses implying a time-variable flare abundance of Ca. We suggest that BCS high-resolution \\\\caxix\\\\ and \\\\fexxv\\\\ line spectra be used as templates for the analysis of X-ray spectra of non-solar sources.',\n",
       "  'len': 1593},\n",
       " {'abstract': 'The Cassini Ultraviolet Imaging Spectrograph (UVIS) observed a plume of water vapor spewing out from the south polar regions of Enceladus in occultation geometry 7 times during the Cassini mission. Five of them yielded data resolved spatially that allowed fits to a set of individually modeled jets. We created a direct simulation Monte Carlo (DSMC) model to simulate individual water vapor jets with the aim of fitting them to water vapor abundance along the UVIS line of sight during occultation observations. Accurate location and attitude of spacecraft together with positions of Enceladus and Saturn at each observation determine the relationship between the three-dimensional water vapor number density in the plume and the two-dimensional profiles of water vapor abundances along the line of sight recorded by UVIS. By individually fitting observed and modeled jets, every occultation observation of UVIS presented a unique perspective to the physical properties and distribution of the jets. The minimum velocity of water vapor in the jets is determined from the narrowest observed individual jet profile: it ranges from 800 m/s to 1.8 km/s for the UVIS occultation observations. 41 individual jets were required to fit the highest resolution UVIS dataset taken during the Solar occultation however, an alternative larger set of linearly-dependent jets can not be excluded without invoking additional preferably unrelated data from other instruments. A smaller number of jets is required to fit the stellar occultation data because of their spatial resolution and geometry. We identify a set of 37 jets that were repeatedly present in best fits to several UVIS occultation observations. These jets were probably active through the whole Cassini mission.',\n",
       "  'len': 1772},\n",
       " {'abstract': 'With the development of the space missions, there are extensive missions in the demand of the prescribed time convergence. However, it is still a difficult work to combine the prescribed time method with the sliding mode control due to the infinite gain of the prescribed time method while approaching the prescribed time and two periods of sliding mode control. In this paper, a new prescribed time sliding mode control method is proposed for general systems with matched disturbances, from the second-order system to the high-order system. A novel sliding mode variable with explicit time term is designed for achieving the prescribed time convergence. More importantly, as time approaches the prescribed time, the singularity of control input can be avoided. Finally, this paper presents a disturbance observer based prescribed time sliding mode control method for attitude tracking of spacecraft and the efficiency of this method has been verified through the numerical simulations.',\n",
       "  'len': 997},\n",
       " {'abstract': 'The Kuiper Belt is a distant region of the Solar System. On 1 January 2019, the New Horizons spacecraft flew close to (486958) 2014 MU69, a Cold Classical Kuiper Belt Object, a class of objects that have never been heated by the Sun and are therefore well preserved since their formation. Here we describe initial results from these encounter observations. MU69 is a bi-lobed contact binary with a flattened shape, discrete geological units, and noticeable albedo heterogeneity. However, there is little surface color and compositional heterogeneity. No evidence for satellites, ring or dust structures, gas coma, or solar wind interactions was detected. By origin MU69 appears consistent with pebble cloud collapse followed by a low velocity merger of its two lobes.',\n",
       "  'len': 778},\n",
       " {'abstract': 'The Cold Classical Kuiper Belt, a class of small bodies in undisturbed orbits beyond Neptune, are primitive objects preserving information about Solar System formation. The New Horizons spacecraft flew past one of these objects, the 36 km long contact binary (486958) Arrokoth (2014 MU69), in January 2019. Images from the flyby show that Arrokoth has no detectable rings, and no satellites (larger than 180 meters diameter) within a radius of 8000 km, and has a lightly-cratered smooth surface with complex geological features, unlike those on previously visited Solar System bodies. The density of impact craters indicates the surface dates from the formation of the Solar System. The two lobes of the contact binary have closely aligned poles and equators, constraining their accretion mechanism.',\n",
       "  'len': 810},\n",
       " {'abstract': \"TianQin is a proposed space-based gravitational wave observatory. It is designed to detect the gravitational wave signals in the frequency range of 0.1 mHz -- 1 Hz. At a geocentric distance of $10^5$ km, the plasma in the earth magnetosphere will contribute as the main source of environmental noises. Here, we analyze the acceleration noises that are caused by the magnetic field of space plasma for the test mass of TianQin. The real solar wind data observed by the Advanced Composition Explorer are taken as the input of the magnetohydrodynamic simulation. The Space Weather Modeling Framework is used to simulate the global magnetosphere of the earth, from which we obtain the plasma and magnetic field parameters on the detector's orbits. We calculate the time series of the residual accelerations and the corresponding amplitude spectral densities on these orbit configurations. We find that the residual acceleration produced by the interaction between the TM's magnetic moment induced by the space magnetic field and the spacecraft magnetic field ($\\\\bm{a}_{\\\\rm M1}$) is the dominant term, which can approach $10^{-15}$ m/s$^2$/Hz$^{1/2}$ at $f \\\\approx$ 0.2 mHz for the nominal values of the magnetic susceptibility ($\\\\chi_{\\\\rm m} = 10^{-5}$) and the magnetic shielding factor ($\\\\xi_{\\\\rm m} = 10$) of the test mass. The ratios between the amplitude spectral density of the acceleration noise caused by the space magnetic field and the preliminary goal of the inertial sensor are 0.38 and 0.08 at 1 mHz and 10 mHz, respectively. We discuss the further reduction of this acceleration noise by decreasing $\\\\chi_{\\\\rm m}$ and/or increasing $\\\\xi_{\\\\rm m}$ in the future instrumentation development for TianQin.\",\n",
       "  'len': 1721},\n",
       " {'abstract': 'In this paper, the spacecraft attitude estimation problem has been investigated making use of the concept of matrix Lie group. Through formulation of the attitude and gyroscope bias as elements of SE(3), the corresponding extended Kalman filter, termed as SE(3)-EKF, has been derived. It is shown that the resulting SE(3)-EKF is just the newly-derived geometric extended Kalman filter (GEKF) for spacecraft attitude estimation. This provides a new perspective on the GEKF besides the common frame errors definition. Moreover, the SE(3)-EKF with reference frame attitude error is also derived and the resulting algorithm bears much resemblance to the right invariant EKF.',\n",
       "  'len': 681},\n",
       " {'abstract': 'The concept of the Solar Ring mission was gradually formed from L5/L4 mission concept, and the proposal of its pre-phase study was funded by the National Natural Science Foundation of China in November 2018 and then by the Strategic Priority Program of Chinese Academy of Sciences in space sciences in May 2019. Solar Ring mission will be the first attempt to routinely monitor and study the Sun and inner heliosphere from a full 360-degree perspective in the ecliptic plane. The current preliminary design of the Solar Ring mission is to deploy six spacecraft, grouped in three pairs, on a sub-AU orbit around the Sun. The two spacecraft in each group are separated by about 30 degrees and every two groups by about 120 degrees. This configuration with necessary science payloads will allow us to establish three unprecedented capabilities: (1) determine the photospheric vector magnetic field with unambiguity, (2) provide 360-degree maps of the Sun and the inner heliosphere routinely, and (3) resolve the solar wind structures at multiple scales and multiple longitudes. With these capabilities, the Solar Ring mission aims to address the origin of solar cycle, the origin of solar eruptions, the origin of solar wind structures and the origin of severe space weather events. The successful accomplishment of the mission will advance our understanding of the star and the space environment that hold our life and enhance our capability of expanding the next new territory of human.',\n",
       "  'len': 1496},\n",
       " {'abstract': 'Transverse waves are sometimes observed in solar helmet streamers, typically after the passage of a coronal mass ejection (CME). The CME-driven shock wave moves the streamer sideways, and a decaying oscillation of the streamer is observed after the CME passage. Previous works generally reported observations of streamer oscillations taken from a single vantage point (typically the SOHO spacecraft). We conduct a data survey searching for streamer wave events observed by the COR2 coronagraphs onboard the STEREO spacecraft. For the first time, we report observations of streamer wave events from multiple vantage points, by using the COR2 instrument on both STEREO A and B, as well as the SOHO/LASCO C2+C3 coronagraphs. We investigate the properties of streamer waves by comparing the different events and performing a statistical analysis. Common observational features give us additional insight on the physical nature of streamer wave events. The most important conclusion is that there appears to be no relation between the speed of the CME and the phase speed of the resulting streamer wave, indicating that the streamer wave speed is determined by the physical properties of the streamer rather than the properties of the CME. This result makes streamer waves events excellent candidates for coronal seismology studies. From a comparison between the measured phase speeds and the phase speeds calculated from the measured periods and wavelengths, we could determine that the speed of the post-shock solar wind flow in our streamers is around 300 $\\\\mathrm{km \\\\ s}^{-1}$.',\n",
       "  'len': 1588},\n",
       " {'abstract': 'Traditionally, the strongest magnetic fields on the Sun have been measured in sunspot umbrae. More recently, however, much stronger fields have been measured at the ends of penumbral filaments carrying the Evershed and counter-Evershed flows. Superstrong fields have also been reported within a light bridge separating two umbrae of opposite polarities. We aim to accurately determine the strengths of the strongest fields in a light bridge using an advanced inversion technique and to investigate their detailed structure. We analyze observations from the spectropolarimeter on board the Hinode spacecraft of the active region AR 11967. The thermodynamic and magnetic configurations are obtained by inverting the Stokes profiles using an inversion scheme that allows multiple height nodes. Both the traditional 1D inversion technique and the so-called 2D coupled inversions, which take into account the point spread function of the Hinode telescope, are used. We find a compact structure with an area of 32.7 arcsec$^2$ within a bipolar light bridge with field strengths exceeding 5 kG, confirming the strong fields in this light bridge reported in the literature. Two regions associated with downflows of $\\\\sim$5 km s$^{-1}$ harbor field strengths larger than 6.5 kG, covering a total area of 2.97 arcsec$^2$. The maximum field strength found is 8.2 kG, which is the largest ever observed field in a bipolar light bridge up to now.',\n",
       "  'len': 1444},\n",
       " {'abstract': 'This paper describes an algorithm obtained by merging a recursive star identification algorithm with a recently developed adaptive SVD-based estimator of the angular velocity vector (QuateRA). In a recursive algorithm, the more accurate the angular velocity estimate, the quicker and more robust to noise the resultant recursive algorithm is. Hence, combining these two techniques produces an algorithm capable of handling a variety of dynamics scenarios. The speed and robustness of the algorithm are highlighted in a selection of simulated scenarios. First, a speed comparison is made with the state-of-the-art lost-in-space star identification algorithm, Pyramid. This test shows that in the best case the algorithm is on average an order of magnitude faster than Pyramid. Next, the recursive algorithm is validated for a variety of dynamic cases including a ground-based \"Stellar Compass\" scenario, a satellite in geosynchronous orbit, a satellite during a re-orientation maneuver, and a satellite undergoing non-pure-spin dynamics.',\n",
       "  'len': 1047},\n",
       " {'abstract': \"The composition of an atmosphere has integrated the geological history of the entire planetary body. However, the long-term evolutions of the atmospheres of the terrestrial planets are not well documented. For Earth, there were until recently only few direct records of atmosphere's composition in the distant past, and insights came mainly from geochemical or physical proxies and/or from atmospheric models pushed back in time. Here we review innovative approaches on new terrestrial samples that led to the determination of the elemental and isotopic compositions of key geochemical tracers, namely noble gases and nitrogen. Such approaches allowed one to investigate the atmosphere's evolution through geological period of time, and to set stringent constraints on the past atmospheric pressure and on the salinity of the Archean oceans. For Mars, we review the current state of knowledge obtained from analyses of Martian meteorites, and from the direct measurements of the composition of the present-day atmosphere by rovers and spacecrafts. Based on these measurements, we explore divergent models of the Martian and Terrestrial atmospheric evolutions. For Venus, only little is known, evidencing the critical need for dedicated missions.\",\n",
       "  'len': 1256},\n",
       " {'abstract': 'No robust detection of prompt electromagnetic counterparts to fast radio bursts (FRBs) has yet been obtained, in spite of several multi-wavelength searches carried out so far. Specifically, X/gamma-ray counterparts are predicted by some models. We planned on searching for prompt gamma-ray counterparts in the Insight-Hard X-ray Modulation Telescope (Insight-HXMT) data, taking advantage of the unique combination of large effective area in the keV-MeV energy range and of sub-ms time resolution. We selected 39 FRBs that were promptly visible from the High-Energy (HE) instrument aboard Insight-HXMT. After calculating the expected arrival times at the location of the spacecraft, we searched for a significant excess in both individual and cumulative time profiles over a wide range of time resolutions, from several seconds down to sub-ms scales. Using the dispersion measures in excess of the Galactic terms, we estimated the upper limits on the redshifts. No convincing signal was found and for each FRB we constrained the gamma-ray isotropic-equivalent luminosity and the released energy as a function of emission timescale. For the nearest FRB source, the periodic repeater FRB180916.J0158+65, we find $L_{\\\\gamma,iso}<5.5\\\\times 10^{47}$ erg/s over 1 s, whereas $L_{\\\\gamma,iso}<10^{49}-10^{51}$ erg/s for the bulk of FRBs. The same values scale up by a factor of ~100 for a ms-long emission. Even on a timescale comparable with that of the radio pulse itself no keV-MeV emission is observed. A systematic association with either long or short GRBs is ruled out with high confidence, except for subluminous events, as is the case for core-collapse of massive stars (long) or binary neutron star mergers (short) viewed off axis. Only giant flares from extra-galactic magnetars at least ten times more energetic than Galactic siblings are ruled out for the nearest FRB.',\n",
       "  'len': 1883},\n",
       " {'abstract': 'Small-scale magnetic flux ropes (SFRs), in the solar wind, have been studied for decades. Statistical analysis utilizing various in situ spacecraft measurements is the main observational approach which helps investigate the generation and evolution of these small-scale structures. Based on the Grad-Shafranov (GS) reconstruction technique, we use the automated detection algorithm to build the databases of these small-scale structures via various spacecraft measurements at different heliocentric distances. We present the SFR properties including the magnetic field and plasma parameters at different radial distances from the sun near the ecliptic plane. It is found that the event occurrence rate is still in the order of a few hundreds per month, the duration and scale size distributions follow power laws, and the flux rope axis orientations are approximately centered around the local Parker spiral directions. In general, most SFR properties exhibit radial decays. In addition, with various databases established, we derive scaling laws for the changes of average field magnitude, event counts, and SFR scale sizes, with respect to the radial distances, ranging from $\\\\sim$ 0.3 au for Helios to $\\\\sim$ 7 au for the Voyager spacecraft. The implications of our results for comparisons with the relevant theoretical works and for the application to the Parker Solar Probe (PSP) mission are discussed.',\n",
       "  'len': 1418},\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07ebe0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\\\abstract.json\", 'w') as fout:\n",
    "    json.dump(abstracts , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345bcb56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
